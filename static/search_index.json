[{"title":"AWS Lambda: Cold Starts, Infrastructure, and Enterprise Applications","description":null,"tags":["aws","lambda","serverless","infrastructure","cold-starts"],"slug":"understanding-lambdas","html":"NOTE: This post is still under construction come back later please Introduction When AWS introduced Lambda in 2014, it fundamentally changed how developers think about building and deploying applications. The serverless paradigm promised freedom from infrastructure management, automatic scaling, and pay-per-use pricing. But beneath this simple abstraction lies a complex, fascinating infrastructure that impacts application performance, reliability, and cost-effectiveness. In this post, I'll take you deep into the world of AWS Lambda, focusing on three critical aspects that aren't widely understood: Cold starts and AWS's hidden warming mechanisms The underlying infrastructure that powers Lambda functions Real-world enterprise use cases and why infrastructure knowledge matters Understanding these elements will help you make better architectural decisions, optimize performance, and build more reliable serverless applications. Let's dive in. Cold Starts: AWS's Secret Warming Mechanism Cold starts remain one of the most discussed challenges of serverless computing. When a Lambda function is invoked after being idle or for the first time, AWS must initialize an execution environment, load your code, and start the runtime. This initialization process adds latency—anywhere from 100ms to several seconds—before your function can begin processing the request. The Traditional Cold Start Model The standard understanding of Lambda cold starts looks like this: This diagram from AWS documentation shows the initialization phase, which includes: Downloading your function code Starting the execution environment Running any initialization code outside the handler Finally executing your handler code According to AWS, &quot;cold starts typically occur in under 1% of invocations&quot; in production workloads. However, this seemingly small percentage can significantly impact real user experience, especially in latency-sensitive applications. Just imagine being Netflix with 220+ million subscribers - that 1% would mean +/- 2 million people experiencing delays... I'm sure your shareholders would be totally fine with that .-. Proactive Initialization: The Hidden Mechanism In 2023, AWS updated their documentation with the following statement: &quot;For functions using unreserved (on-demand) concurrency, Lambda may proactively initialize a function instance, even if there's no invocation.&quot; This statement, documented by AWS in response to research by AWS Serverless Hero AJ Stuyvenberg (Stuyvenberg website I really recommend his lectures btw), revealed that Lambda performs proactive warming of functions—without developers having to implement workarounds or pay for Provisioned Concurrency. Stuyvenberg's research showed that between 50-85% of Lambda initializations can be proactive rather than true cold starts. Here's what happens during proactive initialization: { if (functionDidColdStart) { const handlerWrappedTime = new Date() const proactiveInitialization = handlerWrappedTime - coldStartSystemTime > 10000 ? 1 : 0 console.log({proactiveInitialization}) functionDidColdStart = false } // Function logic here } ``` ### Why Does This Matter? Understanding proactive initialization changes how we think about Lambda performance and cold start mitigation: 1. **Cost optimization**: You might not need to pay for Provisioned Concurrency in all scenarios 2. **Performance expectations**: Your functions might perform better than expected if AWS is proactively warming them 3. **Architecture decisions**: Certain patterns become more viable when cold starts are less frequent Research published by Ran Isenberg in [\"AWS Lambda Cold Starts Explained\"](https://www.ranthebuilder.cloud/post/is-aws-lambda-cold-start-still-an-issue-in-2024) highlights the situations where cold starts truly matter: - **Critical customer-facing flows**: When performance is essential, even for 1% of customers - **Erratic traffic patterns**: Unpredictable traffic can lead to more cold starts than average - **Chained cold starts**: When microservices call each other, cold starts can compound (shown below) ![Chained Cold Starts](https://static.wixstatic.com/media/9d29a8_eade38a3b5c04172bcb70e5f122d4aa1~mv2.png/v1/fill/w_740,h_496,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/9d29a8_eade38a3b5c04172bcb70e5f122d4aa1~mv2.png) This diagram from Isenberg's research shows how cold starts can cascade across services, creating a compound delay for end users. If three microservices each experience a 1-second cold start, the user might experience a 3-second delay. --- ## The Infrastructure Behind Lambda: Firecracker, MicroVMs, and Worker Hosts To truly understand Lambda's behavior, we need to examine its underlying infrastructure. Lambda functions don't run in isolation—they're part of a sophisticated distributed system that balances performance, security, and resource efficiency. ### Firecracker MicroVMs: The Foundation At the core of Lambda is Firecracker, a virtualization technology developed by AWS and open-sourced in 2018. Written in Rust for security and performance, Firecracker creates lightweight micro virtual machines (microVMs) that: 1. Start in as little as 125ms 2. Consume only about 5 MiB of memory per microVM 3. Provide strong security isolation through multiple protection mechanisms ![Firecracker Architecture](https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/11/26/firecracker-architecture.png) Firecracker's minimalist approach is key to Lambda's performance. It implements a stripped-down device model with only essential virtualized devices: a network device, block I/O, timer, serial console, and partial keyboard—minimizing the attack surface. ### Worker Hosts and Execution Environments Lambda functions run on a fleet of EC2 instances called \"worker hosts\"—most commonly using Spot instances for cost efficiency. Each worker host runs multiple microVMs, with each microVM hosting a single execution environment. As outlined in Joud Awad's [\"AWS Lambda Architecture Deep Dive\"](https://medium.com/@joudwawad/aws-lambda-architecture-deep-dive-bef856b9b2c4), the structure looks like this: ![Worker and MicroVM Structure](https://miro.medium.com/v2/resize:fit:828/format:webp/1*Kz2fQAXdBUrgVw-33hB3Lg.png) Each execution environment is unique to a specific function version and can only process one invocation at a time. After processing, the environment remains \"warm\" for reuse, typically for hours but with a maximum lifetime of 14 hours. ### The Invoke Data Plane When you trigger a Lambda function, your request goes through Lambda's Invoke Data Plane—a series of web services that: 1. Select or create an execution environment for your function 2. Route your request to that environment 3. Enforce throttle limits to protect the service This architecture explains why Lambda has multiple throttle limits that we'll explore later. ### Function Placement and Scaling Behind the scenes, Lambda uses a component called the Placement Service to determine where to run your function: ![Lambda Placement Service](https://miro.medium.com/v2/resize:fit:828/format:webp/1*m2K7KWQXAVDt0XYNLvF05A.png) The Placement Service is responsible for efficiently distributing Lambda functions across worker hosts, balancing factors like: - Proximity to data sources and consumers - Current resource utilization - Function resource requirements - Availability zone health This service is also what enables proactive initialization—it can predict demand and pre-warm execution environments before actual invocations arrive. ### Why Does This Matter? Understanding Lambda's infrastructure helps explain many of its behaviors and constraints: 1. **Cold start patterns**: The microVM architecture explains the latency profile of cold starts 2. **Concurrency and burst limits**: The worker host model explains why Lambda throttles rapid scaling 3. **Memory/performance correlation**: Since Lambda allocates CPU proportional to memory, the underlying VM architecture explains why more memory equals better performance 4. **Execution time limits**: The 15-minute maximum execution time relates to the efficient utilization of the worker fleet By understanding the infrastructure, we can make more informed decisions about function configuration, deployment practices, and architectural patterns. --- ## Real-World Enterprise Applications: Why Infrastructure Knowledge Matters Understanding Lambda's cold starts and infrastructure isn't just academic—it has direct implications for real-world applications. Let's examine several enterprise use cases and why deep infrastructure knowledge makes a difference. ### Financial Services: Real-Time Decision Making **Case Study: Financial Engines** Financial Engines, the largest independent investment advisor in the US by assets under management, moved their core platform to Lambda. According to AWS case studies, they achieved: - Request rates of up to 60,000 per minute - Zero downtime - Significant cost savings **Why Infrastructure Knowledge Matters:** For financial services companies, understanding cold starts is critical for real-time decision making. When milliseconds can impact trading decisions or customer experiences, knowing how to minimize and predict cold starts becomes essential. Financial services companies can use this knowledge to: 1. **Strategically deploy Provisioned Concurrency**: Only paying for it on critical customer-facing flows 2. **Monitor proactive initialization patterns**: Using metrics to understand when AWS is warming functions 3. **Architect for minimal cold start impact**: Placing latency-sensitive operations in separate functions ### Media Streaming: High-Volume Processing **Case Study: Netflix** Netflix, one of the world's largest media streaming providers, uses Lambda extensively. According to their engineering blog, they process: - Billions of daily events - Tens of thousands of concurrent executions - Petabytes of data **Why Infrastructure Knowledge Matters:** For high-volume media processing, understanding Lambda's infrastructure helps optimize for cost and performance: 1. **MicroVM architecture**: Allows Netflix to optimize memory allocation per function 2. **Worker placement understanding**: Helps them design for multi-region resilience 3. **Throttle limits**: Guides their implementation of backpressure mechanisms ### Real-Time Gaming: Latency-Critical Operations **Case Study: Square Enix** Square Enix uses AWS Lambda for image processing in its Massively Multiplayer Online Role-Playing Games (MMORPGs). They achieved: - Handling traffic spikes of up to 30x normal levels - Reducing image processing time from hours to seconds - Lower infrastructure and operational costs **Why Infrastructure Knowledge Matters:** For gaming companies, understanding Lambda's infrastructure enables: 1. **Predictable player experiences**: Minimizing cold starts for user-facing operations 2. **Cost-effective scaling**: Handling unpredictable player counts without over-provisioning 3. **Region-specific optimizations**: Understanding how worker placement affects global players ### Telecommunications: Mission-Critical Systems **Case Study: T-Mobile** T-Mobile, serving more than 70 million customers, has a \"serverless first\" policy for new services. They use Lambda for mission-critical applications that serve millions of users. **Why Infrastructure Knowledge Matters:** For telecommunications companies, infrastructure knowledge directly impacts reliability: 1. **Execution environment lifecycle**: Helps predict and mitigate potential service degradations 2. **Cross-region design**: Understanding worker placement guides multi-region architectures 3. **Throttle limit handling**: Critical for managing high-volume messaging and notification systems ### E-Commerce: Handling Traffic Spikes **Case Study: MatHem** MatHem, Sweden's largest online-only grocery retailer, uses a serverless architecture that allows them to: - Bring new features to customers up to 10x faster - Handle seasonal shopping spikes - Reduce operational overhead **Why Infrastructure Knowledge Matters:** For e-commerce platforms, especially those with seasonal traffic patterns, understanding Lambda's infrastructure enables: 1. **Optimized cold start handling**: Essential during flash sales or holiday shopping 2. **Cost-effective scaling**: Understanding when to use Provisioned Concurrency vs. on-demand 3. **Efficient resource allocation**: Matching memory allocation to workload characteristics --- ## Optimizing Lambda for Your Use Case Armed with understanding of cold starts, infrastructure, and real-world applications, let's explore how to optimize Lambda for specific scenarios. ### For Latency-Sensitive Applications If your application requires consistent, low-latency responses (like financial services or gaming): 1. **Runtime selection matters**: According to AWS research, cold start times vary significantly by runtime. Consider using Rust, Go, or the Lambda Low Latency Runtime (LLRT) for the shortest cold starts 2. **Memory configuration**: Higher memory allocations not only provide more computing power but also reduce cold start times 3. **Code optimization**: Minimize imports and dependencies to reduce initialization time 4. **Strategic Provisioned Concurrency**: Use it for critical flows based on traffic patterns ### For High-Throughput Systems If your system processes large volumes of data or requests (like media processing or analytics): 1. **Concurrency limits**: Understand your account's limits and request increases if needed 2. **Burst handling**: Implement client-side throttling or queuing to handle burst limits 3. **Batch processing**: Use SQS with batching to optimize throughput and costs 4. **Infrastructure awareness**: Design with Lambda's scaling behavior in mind ### For Cost-Sensitive Workloads If cost optimization is a primary concern: 1. **Right-size memory**: Use tools like AWS Lambda Power Tuning to find the optimal balance of cost and performance 2. **Exploit proactive initialization**: Design your system to benefit from AWS's warming without paying for Provisioned Concurrency 3. **Function consolidation**: Balance the microservices approach with function consolidation to reduce cold starts 4. **Execution time optimization**: Reduce billable duration by optimizing code and dependencies --- ## Conclusion: The Future of Lambda Infrastructure Understanding Lambda's cold starts, infrastructure, and real-world applications puts you ahead of most developers. As AWS continues to evolve the service, several trends are emerging: 1. **Improved cold start handling**: Mechanisms like Proactive Initialization demonstrate AWS's commitment to addressing this challenge 2. **More efficient infrastructure**: Firecracker continues to evolve, becoming faster and more resource-efficient 3. **Specialized runtimes**: The introduction of LLRT shows AWS's focus on latency-sensitive workloads 4. **Enterprise-focused features**: As more critical workloads move to Lambda, expect more features addressing enterprise needs By staying informed about Lambda's inner workings, you'll be better positioned to leverage these advancements and build more efficient, reliable, and cost-effective serverless applications. ## References 1. Stuyvenberg, A. (2023). \"Understanding AWS Lambda Proactive Initialization.\" Retrieved from https://aaronstuyvenberg.com/posts/understanding-proactive-initialization 2. Isenberg, R. (2024). \"AWS Lambda Cold Starts Explained: What They Are & How to Reduce Them.\" Retrieved from https://www.ranthebuilder.cloud/post/is-aws-lambda-cold-start-still-an-issue-in-2024 3. AWS Documentation. (2024). \"Understanding the Lambda execution environment lifecycle.\" Retrieved from https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html 4. Barr, J. (2018). \"Firecracker – Lightweight Virtualization for Serverless Computing.\" AWS News Blog. Retrieved from https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/ 5. Awad, J. W. (2024). \"AWS Lambda Architecture Deep Dive.\" Medium. Retrieved from https://medium.com/@joudwawad/aws-lambda-architecture-deep-dive-bef856b9b2c4 6. AWS. (2024). \"AWS Lambda Customer Case Studies.\" Retrieved from https://aws.amazon.com/lambda/resources/customer-case-studies/ What aspects of Lambda's infrastructure have you found most impactful in your applications? Have you experienced the benefits of proactive initialization? I'd love to hear about your experiences in the comments. -->"},{"title":"Common multithreading issues in Java","description":null,"tags":["java","spring","multithreading","concurrency","synchronization","best-practices"],"slug":"java-spring-multithreading-pitfalls","html":"Introduction Multithreading in Java enables concurrent execution of multiple threads within a single application, potentially improving performance and resource utilization. However, improper thread management can cause a lot of bugs that only manifest under specific conditions. In this post, I'll walk through some of the multithreading pitfalls I've encountered, along with some solutions and usecases/anti-patterns to help you avoid the same mistakes I've mapped here, in the following order: Race Conditions Memory Visibility Issues Deadlocks Thread Pool Configuration NOTE: Maybe in a future post, I'll cover Spring-specific multithreading issues like @Async limitations, transaction boundaries, and event processing.* Race Conditions What Are Race Conditions? A race condition occurs when multiple threads access and modify shared data concurrently, leading to unpredictable behavior. The outcome depends on thread execution timing, making it difficult to reproduce and debug. According to a study published in the IEEE Transactions on Software Engineering, race conditions account for approximately 29% of all concurrency bugs in production systems [1]. In my experience, they account for about 99% of my hair lost. Example Consider a simple counter implementation: public class Counter { private int count = 0; public void increment() { count++; } public int getCount() { return count; } } When multiple threads call increment() concurrently, the expected value might differ from actual results. This happens because count++ is not atomic—it involves three operations: Read the current value of count Increment the value Write the new value back to count So basically if Trhead A and Thread B both read the initial value before either updates it, one increment will be lost. Detecting Race Conditions Race conditions can be detected using: Java Thread Dumps: Analyze thread dumps when application behavior is inconsistent Code Reviews: Look for shared mutable state accessed by multiple threads Testing Tools: Tools like Java PathFinder, FindBugs, and JCStress can detect potential race conditions Solving Race Conditions in Java 1. Using Synchronized Keyword public class Counter { private int count = 0; public synchronized void increment() { count++; } public synchronized int getCount() { return count; } } The synchronized keyword ensures that only one thread can execute the method at a time. It achieves this by acquiring an intrinsic lock (monitor) on the object instance. However, synchronization introduces overhead as threads must wait for the lock to be released (think of it as putting a bouncer at the door who only lets one person in at a time — secure, but creates a line). 2. Using AtomicInteger Java's java.util.concurrent.atomic package provides thread-safe primitive types: import java.util.concurrent.atomic.AtomicInteger; public class Counter { private AtomicInteger count = new AtomicInteger(0); public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } AtomicInteger uses Compare-And-Swap (CAS) operations, which are typically more efficient than synchronization for simple operations. 3. Using Lock Interface For more complex scenarios, the java.util.concurrent.locks package offers more flexible locking mechanisms (when you need a more sophisticated bouncer): import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; public class Counter { private int count = 0; private final Lock lock = new ReentrantLock(); public void increment() { lock.lock(); try { count++; } finally { lock.unlock(); } } public int getCount() { lock.lock(); try { return count; } finally { lock.unlock(); } } } Real-World Example Consider a payment processing gateway that handles high-volume financial transactions. In this scenario, a transaction processing system occasionally showed incorrect account balances. Analysis of production logs revealed a classic race condition where multiple threads were updating the same account simultaneously(I took this example from an interview I did couple of years ago): public void processPayment(Long accountId, BigDecimal amount) { Account account = accountRepository.findById(accountId).orElseThrow(); BigDecimal newBalance = account.getBalance().add(amount); account.setBalance(newBalance); accountRepository.save(account); } This implementation has a critical race condition: Thread A reads account balance (1000) Thread B reads the same account balance (1000) Thread A calculates new balance (1000 + 200 = 1200) Thread B calculates new balance (1000 + 500 = 1500) Thread A saves the new balance (1200) Thread B saves the new balance (1500) In this scenario, the 200 payment processed by Thread A is effectively lost because Thread B overwrote it with its calculation based on the original balance. The system has lost 200 units of currency - a serious financial discrepancy. The solution requires implementing proper transaction isolation. In database terms, this is precisely what the SERIALIZABLE isolation level is designed to prevent: @Transactional(isolation = Isolation.SERIALIZABLE) public void processPayment(Long accountId, BigDecimal amount) { Account account = accountRepository.findById(accountId).orElseThrow(); BigDecimal newBalance = account.getBalance().add(amount); account.setBalance(newBalance); accountRepository.save(account); } With SERIALIZABLE isolation, the database ensures that concurrent transactions behave as if they were executed sequentially. This prevents the race condition by ensuring that: Thread A begins transaction and reads account (1000) Thread B begins transaction and tries to read the same account Thread B is blocked (or gets a version error, depending on implementation) until Thread A completes Thread A updates balance to 1200 and commits Thread B now reads the updated account (1200) Thread B updates balance to 1700 (1200 + 500) and commits When implementing this solution, be aware that SERIALIZABLE has performance implications: Concurrency reduction: It reduces the number of concurrent operations the system can perform Deadlock risk: Higher isolation levels increase the risk of deadlocks Performance cost: There's a tradeoff between consistency and throughput For some financial systems, a more scalable approach is optimistic locking with version control. Think of it like a Google Doc's version history - if two people edit the same document simultaneously, the system detects the conflict and handles it gracefully. How Optimistic Locking Works Version Tracking: Each record keeps track of its version number Read Phase: When reading a record, we store both its data AND version number Update Phase: When saving, we check if the version is still the same Conflict Detection: If someone else changed the record (version mismatch), we handle the conflict Here's a practical implementation using Spring: @Entity public class Account { @Id private Long id; private BigDecimal balance; @Version // this annotation tells JPA to handle versioning private Long version; // automatically incremented on each update } @Service @Transactional(isolation = Isolation.READ_COMMITTED) public class PaymentService { private final AccountRepository accountRepository; public void processPayment(Long accountId, BigDecimal amount) { // the first thread reads version = 1 Account account = accountRepository.findById(accountId).orElseThrow(); BigDecimal newBalance = account.getBalance().add(amount); account.setBalance(newBalance); try { // if another thread updated the account (now version = 2), // this save will fail with OptimisticLockException accountRepository.save(account); } catch (OptimisticLockException e) { // handle the conflict(like a retry or something) throw new PaymentConflictException(&quot;Payment failed - please retry&quot;); } } // retry example @Retryable(maxAttempts = 3, backoff = @Backoff(delay = 100)) public void processPaymentWithRetry(Long accountId, BigDecimal amount) { processPayment(accountId, amount); } } Let's see how this prevents the race condition: Thread A reads account (balance = 1000, version = 1) Thread B reads account (balance = 1000, version = 1) Thread A calculates new balance (1000 + 200 = 1200) Thread B calculates new balance (1000 + 500 = 1500) Thread A saves changes → success (balance = 1200, version = 2) Thread B tries to save → FAILS because version changed Thread B retries with fresh data (balance = 1200, version = 2) Thread B calculates new balance (1200 + 500 = 1700) Thread B saves changes → success (balance = 1700, version = 3) The key advantages of optimistic locking over SERIALIZABLE isolation: Better Performance: No need to lock records - we only check versions when saving Higher Throughput: Multiple transactions can read the same data simultaneously Deadlock Prevention: No locks means no deadlocks Automatic Conflict Detection: The database handles version checking automatically The main trade-off is that you need to handle the retry logic when conflicts occur. This is usually acceptable because conflicts are typically rare in real-world scenarios - they only happen when two users try to modify the exact same record at the exact same time. Memory Visibility Issues What Are Memory Visibility Issues? In Java's memory model, threads may cache variables locally instead of reading them from main memory. This can lead to a situation where updates made by one thread aren't visible to others. In the Java Memory Model (JMM) specification, variable updates without proper synchronization aren't guaranteed to be visible across threads. Nerds alert: If you want to go for a deep dive into memory visibility, you'll need to dive into the Java Memory Model (JMM). For a comprehensive explanation, you might want to check &quot;Java Concurrency in Practice&quot; by Brian Goetz [4]. For the brave souls who want the formal specification(which I can't even explain explicitly), you'll be able to find it in Chapter 17.4 of the oracle doc(link). Example of Memory Visibility Issues Consider a flag to control thread execution: public class TaskManager { private boolean stopped = false; public void stop() { stopped = true; } public void runTask() { while (!stopped) { //do sometihng } } } Thread A could call stop() but Thread B might never see the update, resulting in an infinite loop. This is a classic memory visibility problem – the thread executing runTask() may maintain a local copy of stopped in its cache and never see the update made by another thread. Solving Memory Visibility Issues There are three main approaches to solving memory visibility issues, each with specific use cases: 1. Using volatile Keyword The volatile keyword ensures that reads and writes go directly to main memory(like a &quot;no-cache&quot; flag): public class TaskManager { private volatile boolean stopped = false; public void stop() { stopped = true; } public void runTask() { while (!stopped) { //do sometihng } } } When to use volatile: For simple variables that function as flags/signals When you only need visibility guarantees (without atomicity) When performance is critical (it's the lightest solution) Important: Doesn't guarantee atomicity for compound operations like i++ (read + increment + write) 2. Using Synchronized Access synchronized blocks provide both visibility guarantees and mutual exclusion (only one thread can execute the code at a time): public class TaskManager { private boolean stopped = false; public synchronized void stop() { stopped = true; } public void runTask() { while (!isStopped()) { //do sometihng } } private synchronized boolean isStopped() { // &lt; -- thread safe access return stopped; } } When to use synchronized: When you need mutual exclusion in addition to visibility To protect compound operations that must be atomic When the same thread needs to check and update multiple related variables Disadvantage: Less peformartic when compared to volatile 3. Using Atomic Variables Classes from the java.util.concurrent.atomic package combine visibility with atomic operations(the CAS): import java.util.concurrent.atomic.AtomicBoolean; public class TaskManager { private AtomicBoolean stopped = new AtomicBoolean(false); public void stop() { stopped.set(true); } public void runTask() { while (!stopped.get()) { //do sometihng } } } When to use Atomic: When you need atomic operations without the overhead of full locking For counters, accumulators, and flags that need atomic operations To implement high-performance lock-free algorithms When you need atomic compound operations like compareAndSet() Advantage: Better scalability under high contention compared to synchronized Comparison between the approaches Approach Visibility Atomicity Locking Performance Use Cases volatile ✅ ❌ ❌ Excellent Simple flags, visibility only synchronized ✅ ✅ ✅ Good/Medium Protecting complex shared state Atomic ✅ ✅ ❌ Very good Counters, CAS operations, high concurrency Deadlocks What Are Deadlocks? In technical terms, a deadlock occurs when two or more threads each hold a resource that the other needs to continue execution. This creates a circular dependency where each thread is blocked indefinitely, waiting for resources that will never be released. The four necessary conditions for a deadlock (all must be present): Mutual Exclusion: Resources cannot be shared simultaneously Hold and Wait: Threads hold resources while waiting for others No Preemption: Resources cannot be forcibly taken from threads Circular Wait: A circular chain of threads, each waiting for a resource held by the next I think of deadlocks like an episode of Foster's Home For Imaginary Friends(which I watched as a kid), where Wilt (or Minguado in Portuguese) is that super polite imaginary friend who gets stuck at a doorway: &quot;After you!&quot; &quot;No, after you!&quot; &quot;I insist, after you!&quot; &quot;No, after you!&quot; So he remains stuck forever. In real life, social awkwardness would eventually break this standoff as someone gives in. But in your application, there's no episode ending or social pressure to resolve the situation—just an unresponsive system that needs rebooting. Wilt - or Minguado for brazilians. Example of Deadlock Here's a classic deadlock scenario with two resources and two threads(This one I also took from an interview I did some time ago): public class BankTransferDeadlock { private final Object accountALock = new Object(); private final Object accountBLock = new Object(); private double accountABalance = 1000; private double accountBBalance = 1000; // Thread 1 IS EXECUTING THIS public void transferAtoB(double amount) { synchronized(accountALock) { // acquire lock on account A System.out.println(&quot;Thread 1: Locked account A&quot;); // simulate some work before trying to acquire second lock try { Thread.sleep(100); } catch (InterruptedException e) {} accountABalance -= amount; synchronized(accountBLock) { // try to acquire lock on account B System.out.println(&quot;Thread 1: Locked account B&quot;); accountBBalance += amount; System.out.println(&quot;Transfer from A to B complete&quot;); } } } // Thread 2 EXECUITING THIS public void transferBtoA(double amount) { synchronized(accountBLock) { // acquire lock on account B System.out.println(&quot;Thread 2: Locked account B&quot;); // simulate some work before trying to acquire second lock try { Thread.sleep(100); } catch (InterruptedException e) {} accountBBalance -= amount; synchronized(accountALock) { // try to acquire lock on account A System.out.println(&quot;Thread 2: Locked account A&quot;); accountABalance += amount; System.out.println(&quot;Transfer from B to A complete&quot;); } } } } Here's what happens when this deadlock occurs: Thread 1 calls transferAtoB() and acquires the lock on account A Thread 2 calls transferBtoA() and acquires the lock on account B Thread 1 tries to lock account B, but it's already locked by Thread 2 → blocks Thread 2 tries to lock account A, but it's already locked by Thread 1 → blocks Both threads are now waiting for resources held by the other. The application appears to freeze with no error message - one of the most frustrating bugs to troubleshoot. In production systems, this often manifests as an application that suddenly stops responding and requires a restart. Unless saying that your algorithm runs on O(∞) in the worst case is acceptable, you should avoid it. Preventing Deadlocks 1. Lock Ordering Always acquire locks in the same order: public class ResourceManager { private final Object resourceA = new Object(); private final Object resourceB = new Object(); public void process1() { synchronized(resourceA) { synchronized(resourceB) { // do something with both resources } } } public void process2() { synchronized(resourceA) { // now acquiring in same order as process1 synchronized(resourceB) { // do something with both resources } } } } 2. Using tryLock with Timeout The Lock interface provides tryLock() with timeout to avoid indefinite waiting: import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; import java.util.concurrent.TimeUnit; public class ResourceManager { private final Lock lockA = new ReentrantLock(); private final Lock lockB = new ReentrantLock(); public void process() throws InterruptedException { boolean gotBothLocks = false; try { boolean gotLockA = lockA.tryLock(1, TimeUnit.SECONDS); if (gotLockA) { try { boolean gotLockB = lockB.tryLock(1, TimeUnit.SECONDS); gotBothLocks = gotLockB; } finally { if (!gotBothLocks) { lockA.unlock(); // release first lock if couldn't get second } } } if (gotBothLocks) { // do something with both resources } else { log.warn(&quot;Failed to acquire locks, will retry later&quot;); } } finally { if (gotBothLocks) { lockB.unlock(); lockA.unlock(); } } } } 3. Using java.util.concurrent Classes Higher-level concurrency utilities often handle lock management internally (remember the inversion of control): import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.ConcurrentHashMap; public class ResourceManager { private final ExecutorService executor = Executors.newFixedThreadPool(10); private final ConcurrentHashMap&lt;String, Resource&gt; resources = new ConcurrentHashMap&lt;&gt;(); // no explicit locking needed for many operations } Thread Pool Configuration Issues Common Thread Pool Problems Java applications often use thread pools via ExecutorService. Incorrect configuration can lead to: Thread starvation: When all threads are busy and tasks queue up Resource exhaustion: When too many threads consume too much memory Thread Pool Best Practices 1. Size Thread Pools Appropriately For CPU-bound tasks: int cpuCores = Runtime.getRuntime().availableProcessors(); ExecutorService executor = Executors.newFixedThreadPool(cpuCores); For I/O-bound tasks (things that wait a lot): // I/O bound tasks can benefit from more threads // (because threads spend most of their time waiting) int threadPoolSize = Runtime.getRuntime().availableProcessors() * 2; ExecutorService executor = Executors.newFixedThreadPool(threadPoolSize); 2. Use Different Thread Pools for Different Types of Tasks // image the scenario you make your ferrari wait behind a garbage truck ExecutorService cpuBoundTasks = Executors.newFixedThreadPool(cpuCores); ExecutorService ioBoundTasks = Executors.newFixedThreadPool(cpuCores * 2); 3. Use Bounded Queues Choosing the right queue type and size is crucial for thread pool performance. The queue acts as a buffer between task submission and execution, but an unbounded queue can lead to OutOfMemoryError if tasks are submitted faster than they can be processed. There are three main queuing strategies: Direct handoff (SynchronousQueue): Tasks are handed directly to threads. If no thread is available, the task submission is rejected. Best for CPU-intensive tasks where queuing would just add overhead. Bounded queue (ArrayBlockingQueue): Provides a buffer but with a limit, preventing resource exhaustion. Best for mixed workloads where some queuing helps smooth out bursts of requests. Unbounded queue (LinkedBlockingQueue): Can grow indefinitely. Only appropriate when task submission rate is naturally limited or when you have infinite memory (spoiler: you don't). Here's how to implement a bounded queue with appropriate rejection handling: int corePoolSize = 5; int maxPoolSize = 10; long keepAliveTime = 60L; BlockingQueue&lt;Runnable&gt; workQueue = new ArrayBlockingQueue&lt;&gt;(100); ThreadPoolExecutor executor = new ThreadPoolExecutor( corePoolSize, maxPoolSize, keepAliveTime, TimeUnit.SECONDS, workQueue, new ThreadPoolExecutor.CallerRunsPolicy()); // saturation policy The rejection/saturation policy determines what happens when both the queue and the thread pool are full: CallerRunsPolicy: Executes the task in the caller's thread (as shown above) AbortPolicy: Throws RejectedExecutionException (default) DiscardPolicy: Silently drops the task DiscardOldestPolicy: Drops the oldest queued task to make room Goetz recommends the CallerRunsPolicy as it provides a form of throttling - when the system is overloaded, the submitting threads start executing tasks themselves, naturally slowing down the submission rate. A real-world sizing example: Scenario 1: Order Processor with Unbounded Queue @Service public class OrderProcessor { private final ExecutorService executor = new ThreadPoolExecutor( 10, 10, 60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;() // unbounded queue - DANGER! ); public void processOrder(Order order) { executor.submit(() -&gt; { // process order (validate payment, reserve inventory, etc) // takes ~500ms per order }); } } Problem: During high-load events like Black Friday: Thousands of orders arrive per second All orders get accepted into memory Java heap grows indefinitely Eventually: OutOfMemoryError Scenario 2: Order Processor with Bounded Queue @Service public class OrderProcessor { private static final int CORE_THREADS = 10; private static final int MAX_THREADS = 20; // based on available memory //(maybe in the future I can write how to calculate and monitor it based on the container you're using) private static final int QUEUE_CAPACITY = 500; private final ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_THREADS, MAX_THREADS, 60L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy() ); public void processOrder(Order order) { executor.submit(() -&gt; { // do processing, but now with backpressure }); } } With this configuration: First 500 excess tasks go to the queue When queue fills, CallerRunsPolicy makes the caller thread execute the task This naturally slows down upstream systems (API Gateway, Load Balancer) System remains stable even under extreme load Conclusion Of course that compared to Brian Goetz and Doug Lea (who literally wrote the concurrency library in Java), I'm just a protozoan on their shoe. What I've shared here is merely a light breath on the most important concepts. To truly master these topics, I strongly recommend reading Goetz's &quot;Java Concurrency in Practice&quot; - it's the definitive resource that helped me understand these complex concepts. But to sum everything up, when designing java multithreaded applications always favor simplicity and proven patterns over complex custom solutions. Java's concurrency utilities in the java.util.concurrent package, combined with a solid understanding of the principles I've covered in this post, will help you build robust, thread-safe applications. As Goetz states: &quot;Write thread-safe code, but don't use more synchronization than necessary.&quot; This balance between safety and performance is the key to effective concurrent programming - a lesson I'm still learning every day :) References Lu, S., Park, S., Seo, E., &amp; Zhou, Y. &quot;Learning from mistakes: a comprehensive study on real world concurrency bug characteristics.&quot; ACM SIGARCH Computer Architecture News, 36(1), 2008. https://dl.acm.org/doi/10.1145/1346281.1346323. Manson, J., Pugh, W., &amp; Adve, S. V. &quot;The Java memory model.&quot; ACM SIGPLAN Notices, 40(1), 2005. https://dl.acm.org/doi/10.1145/1040305.1040336. Goetz, B., Peierls, T., Bloch, J., Bowbeer, J., Holmes, D., &amp; Lea, D. Java Concurrency in Practice. Addison-Wesley Professional, 2006. https://github.com/AngelSanchezT/books-1/blob/master/concurrency/Java%20Concurrency%20in%20Practice.pdf."},{"title":"How Monads and Functional Programming Can Improve Your Exception Handling in Kotlin","description":null,"tags":["kotlin","exception","functional-programming","haskell"],"slug":"kotlin-result-functional","html":"Introduction If you've been programming in Kotlin or Java, you're probably used to handling errors with try-catch blocks. While this approach is standard, it can become really messy in complex scenarios, leading to verbose and difficult-to-maintain code. More critically, traditional exception handling forces developers to constantly anticipate where exceptions might be thrown, increasing cognitive load and potential oversight. Functional programming offers a powerful set of tools for improving code quality, especially when it comes to error handling. The goal here is to focus on the core concepts that can help you write cleaner, more robust, and more predictable error-handling code. I’m not here to dive into Haskell or push its philosophical depths, being really honest I’m very scared of the Haskell nerds in general. Instead, I’ll explore how these functional programming ideas can be practically applied in Kotlin, with real-world use cases in mind. One such concept is the monad, which provides a structured approach to handling side effects, errors, and sequencing operations. In Kotlin, while not purely functional, it can achieve some of these principles through libraries like kotlin-result. Basically, in this post I'll try to cover: Why traditional exception handling can be problematic. Core functional programming concepts relevant to error handling. How kotlin-result addresses these concerns. Designing APIs with Error Handling in Mind Addressing the errors listed in 1 using kotlin-result and functional programming concepts. The Problem: Traditional Exception Handling Traditional exception handling introduces several issues that can compromise code clarity, reliability, and make it a complete mess. Below I've listed some common problematic uses: Hidden Control Flow: Exception-based error handling introduces invisible jumps in your code execution, making it hard to follow: fun processUser(user: User) { validateUser(user) updateProfile(user) notifyUser(user) } Any of these functions could throw an exception, turning the flow unpredictable. This way, Raymond Chen from Microsoft describes exceptions as &quot;Exceptions are like non-local goto statements&quot; which results in: Hard-to-trace execution paths. Unintended disruptions. Increased difficulty in ensuring consistent error handling. A better approach would make errors explicit and handle them systematically. The Checked vs. Unchecked Dilemma: Checked exceptions in Java force developers to catch and handle exceptions at every step, cluttering codebases: try { fileOperation() // throws IOException networkCall() // throws NetworkException dbOperation() // throws SQLException } catch (IOException e) { // Handle file error } catch (NetworkException e) { // Handle network error } catch (SQLException e) { // Handle DB error } Unchecked exceptions, while more flexible, introduce uncertainty, as function signatures do not explicitly indicate possible failure cases. Resource Management Complexity: Managing resources manually often leads to nested try-finally blocks that makes code difficult to maintain and understand later on: fun processOrders() { val connection = dataSource.connection try { val statement = connection.createStatement() try { val result = statement.executeQuery(&quot;SELECT * FROM orders&quot;) try { // Process result } finally { result.close() } } finally { statement.close() } } finally { connection.close() } } This structure is: Verbose and difficult to read. Vulnerable to resource leaks if exceptions are not handled correctly. Hard to maintain. This just looks ugly overall, let's be honest. Loss of Type Safety: Traditional exceptions break type safety because failure conditions are not represented in function signatures: fun getUserProfile(id: String): UserProfile { // This function might throw exceptions that aren’t apparent from the signature. That's one of the most important points of the post throw new UserNotFoundException() } This approach: Leads to unexpected runtime failures. Reduces predictability in API contracts. Makes error handling an afterthought instead of a first-class concern. Core Functional Programming Concepts Relevant to Error Handling What is Functional Programming? Functional programming is a paradigm where functions are treated as first-class citizens, and computation is done through the evaluation of EXPRESSIONS rather than the execution of statements(like mostly done in imperative languages C, java, cpp...). It emphasizes immutability, no side effects, and the use of pure functions. This paradigm, while vast with numerous research areas like lambda calculus, category theory, and type systems, provides a lot of nerdy tools for managing complexity in software. I'd recommend this lecture if you want to dive deeper in some o these topics or if you really hate yourself -&gt; &quot;Learn You a Haskell for Great Good!&quot; by Miran Lipovaca (link) and Haskell.org for deeper dives into functional concepts. Key Points: Immutability: Data cannot be changed once created, promoting safer parallel processing. Pure Functions: Functions always produce the same output for the same input, without affecting or being affected by the external behaviors. Higher-Order Functions: Functions can take other functions as arguments or return them. This post will focus only on a few of these concepts, particularly functors and monads. Functors: Safe Value Transformations Functor is like a box or container. You can apply a transformation (or function) to what's inside the box without opening it. In programming terms, this means you can alter or map over the contents of a data structure without changing its structure: List as a Functor: If you have a list of numbers, you can double each number without altering the list itself: val numbers = listOf(1, 2, 3) val doubled = numbers.map { it * 2 } // [2, 4, 6] Conceptually: Functors allow you to work with data in a way that's safe and predictable because you're not directly manipulating the data but rather transforming it through a mapping operation. Monads: Chaining Operations with Context Monads extend the concept of functors by allowing operations to be chained together while preserving some form of 'context' or 'state': Monad: Monads help to ensure that each step in the sequence is checked before proceeding. It sounds complex(and it possibly is a little bit) but it's a way to wrapping things and provide methods to do operations on the wrapped stuff without unwrapping it. Let's take an example to make it more practical, consider monads as the operation of baking a cake. You need flour before you can add eggs. If there's no flour, you don't add the eggs. In Code: Monads provide a way to wrap values in context, manage that context through operations, and decide on the next step based on the outcome of the previous one: val maybeFlour = checkForFlour() val maybeCake = maybeFlour.flatMap { flour -&gt; if (flour) { addEggs() } else { Result.failure(NoFlourException()) } } Practical Use: In error handling, monads can encapsulate whether an operation was successful or not, allowing you to sequence operations where one depends on the success of another without explicit exception checks. How kotlin-result Addresses These Concerns kotlin-result is a Kotlin library that encapsulates the monadic approach to error handling, offering a Result type which is a monadic type that holds either a successful value or an error. Here's how it adress some issues: Explicit Success or Failure: Rather than using exceptions, Result explicitly represents outcomes as either success (Result.Success) or failure (Result.Failure). This makes error paths clear and predictable, every call should return success or failure. Type-Safe Error Handling: By leveraging sealed classes for error types, kotlin-result ensures at compile-time that all possible outcomes are accounted for. This prevents runtime surprises, similar to how a monadic type system ensures all paths are considered. Reduces Error Boilerplate: The library allows operations to be chained with methods like map, flatMap, or andThen. This reduces the need for extensive try-catch blocks, promoting cleaner code by handling errors in a functional manner. Resource Management: Combining Kotlin's scope functions with Result simplifies resource management. Operations can ensure resources are released properly, even on failure, without the clutter of nested try-finally blocks. Promotes Composable Code: Functions return Result types, enabling them to be composed into more complex operations. This modularity and reusability reflect the functional programming ethos of treating functions as building blocks. Key Use Cases Replacing Traditional Exception Handling: When you want to avoid exceptions for scenarios where error is part of the normal flow, like input validation or network calls. Instead of exceptions, you return Result to explicitly handle both success and failure. API Design: When designing APIs, kotlin-result helps in creating interfaces that are clear about what can go wrong, allowing clients to handle errors gracefully without exception handling boilerplate. Error Propagation: In large codebases, propagating errors up the call stack can be done in a way that's clear and doesn't rely on exceptions, making the code easier to navigate and understand. Designing APIs with Error Handling in Mind When you're designing your Kotlin APIs, consider the following to ensure your error handling is effective: Use Exceptions only when strictly needed: Reserve exceptions for true programming errors where recovery is not feasible, like accessing an index out of bounds in an array. These signify bugs that should be caught and reported, not handled routinely. Use Result for Flow: For scenarios where failure is part of normal operation (like validation, network calls, or data parsing), return Result types. This makes error handling explicit, giving you control over how failures are managed without resorting to exceptions. Wrap logic and adapt it: When you're interfacing with legacy or external APIs that throw exceptions for conditions that aren't logic errors, wrap these calls. Create functions that transform exceptions into Result types, giving your API users a cleaner, more predictable interface: fun fetchUserData(userId: Int): Result&lt;UserData, NetworkError&gt; = runCatching { api.getUserData(userId) } .mapError { when (it) { is IOException -&gt; NetworkError.IOError(it.message ?: &quot;Network error&quot;) is TimeoutException -&gt; NetworkError.Timeout(&quot;Request timed out&quot;) else -&gt; throw it // ou mapear para outro erro genérico, se necessário } } // Where NetworkError could be defined as: sealed class NetworkError { data class IOError(val message: String) : NetworkError() data class Timeout(val message: String) : NetworkError() } Multiple Error Scenarios: For functions that can fail in various ways, define a sealed class to represent these outcomes: sealed class InputError { data class Empty(val field: String) : InputError() data class InvalidFormat(val field: String, val reason: String) : InputError() data class OutOfRange(val field: String) : InputError() } Addressing the Problems Listed in first section Using kotlin-result Example 1: Hidden Control Flow Instead of implicit exception flow: Problem: Exceptions make execution unpredictable—any call could derail the flow. Solution with Result: Make every step of the process explicit: fun processUser(user: User): Result&lt;ProcessedUser, UserError&gt; = validateUser(user) .andThen { updateProfile(it) } .andThen { notifyUser(it) } Why It Works: Instead of invisible jumps, each function returns a Result, letting you handle success or failure explicitly. The flow stays linear and predictable, directly addressing the &quot;non-local goto&quot; issue. Example 2: Checked vs. Unchecked Dilemma Problem: The approach listed in 1 bloats code with repetitive handling or leaves failures undocumented if unchecked exceptions are used. Solution with Result: Consolidate errors into a single, type-safe return type: sealed class OperationError { data class FileError(val message: String) : OperationError() data class NetworkError(val message: String) : OperationError() data class DatabaseError(val message: String) : OperationError() } fun performOperations(): Result&lt;SuccessData, OperationError&gt; = fileOperation().andThen { networkCall() }.andThen { dbOperation() } fun fileOperation(): Result&lt;Unit, OperationError.FileError&gt; = try { // File logic Result.success(Unit) } catch (e: IOException) { Result.failure(OperationError.FileError(e.message ?: &quot;File error&quot;)) } fun networkCall(): Result&lt;Unit, OperationError.NetworkError&gt; = try { // Network logic Result.success(Unit) } catch (e: NetworkException) { Result.failure(OperationError.NetworkError(e.message ?: &quot;Network error&quot;)) } fun dbOperation(): Result&lt;SuccessData, OperationError.DatabaseError&gt; = try { // DB logic Result.success(SuccessData()) } catch (e: SQLException) { Result.failure(OperationError.DatabaseError(e.message ?: &quot;DB error&quot;)) } Why It Works: Instead of the messy code with multiple catch blocks or risking hidden unchecked exceptions, Result wraps all possible failures into a single OperationError hierarchy. The andThen chaining ensures each step only proceeds if the previous one succeeds, making errors explicit in the function signatures. This eliminates verbosity, ensures type-safe handling with Kotlin when expression downstream, and resolves the checked vs. unchecked trade-off by making every failure mode clear and manageable. Example 3: Resource Management Complexity Problem: Managing resources with try-finally blocks are verbose and leak-prone. Solution with Result: Combine Result with Kotlin functions use: fun processOrders(): Result&lt;List&lt;Order&gt;, DBError&gt; = dataSource.connection.use { connection -&gt; connection.createStatement().use { statement -&gt; statement.executeQuery(&quot;SELECT * FROM orders&quot;).use { result -&gt; runCatching { result.toOrders() } .mapError { DBError.QueryFailed(it.message) } } } } Why It Works: use auto-closes resources, and Result captures errors, eliminating nesting. This directly simplifies the ugly, error-prone structure from before. Example 4: Loss of Type Safety Problem: Exceptions hide failure modes, breaking type safety. This is a very simple example, but possibly the biggest catch of the post. See how calling the function getUserProfile makes it now easier to understand and manage errors in the domain of the codebase. Solution with Result: Functions return Result with explicit error type: fun getUserProfile(id: String): Result&lt;UserProfile, UserFetchError&gt; = try { Result.success(database.getUserProfile(id)) } catch (e: SQLException) { Result.failure(UserFetchError.DatabaseError(e.message ?: &quot;Unknown error&quot;)) } Why It Works: The signature now declares possible failures, ensuring errors are handled upfront. This eliminates runtime surprises and strengthens the API contract, fixing the type safety gap. Conclusion Functional programming and monads, via kotlin-result, transform error handling into something explicit, type-safe, and composable. They tackle hidden control flow with clear paths, resolve the checked/unchecked mess with typed errors, simplify resource management, and restore type safety—all while boosting readability and maintainability. So, when should you go for Result or try-catch? Use Result: For expected failures in normal flow: validation errors, network timeouts, or parsing issues. These are business logic concerns where you want fine-grained control and explicit outcomes in your code. When designing APIs or libraries, to give users predictable, exception-free contracts. In functional pipelines, where chaining operations with error propagation feels natural. Use try-catch (Exceptions): For unexpected, unrecoverable errors: null dereferences, file corruption, or logic bugs. These signal something’s broken, not a routine failure, and are best caught at a higher level (e.g., app-wide handlers). When working with legacy code or external APIs that throw exceptions, and wrapping them in Result isn’t practical yet. For centralized recovery, like logging crashes or restarting a service, where granular handling isn’t the goal. Think of it this way: Result is for errors you plan to handle locally, while exceptions are for errors you escalate or crash on. Roman Elizarov’s take on Kotlin’s exception philosophy (link) echoes this: exceptions are for the exceptional, not the everyday. References Elizarov, Roman. &quot;Kotlin and Exceptions.&quot; Medium, 2021. https://elizarov.medium.com/kotlin-and-exceptions-8062f589d07. Bull, Michael. kotlin-result. GitHub repository. https://github.com/michaelbull/kotlin-result. Chen, Raymond. &quot;Exceptions are like non-local goto statements&quot; (paraphrased concept). Commonly referenced in discussions on exception handling, e.g., Microsoft Developer Blogs. Lipovaca, Miran. Learn You a Haskell for Great Good! Online book. http://learnyouahaskell.com/. Haskell Community. Haskell Official Website. https://www.haskell.org/."},{"title":"Microservices: Hidden Technical Debt (and how to possibly avoid the traps)","description":null,"tags":["microservices","technical-debt","distributed-systems","software-architecture"],"slug":"microservices-debt","html":"Introduction These days, I keep seeing microservices being treated as the answer to every software problem. Having worked with various architectures throughout my career, I've noticed how many players often jump into microservices without considering the long-term implications. It reminds me of a project where we turned a perfectly functional monolith into a distributed system just because &quot;that's what modern companies do.&quot; But here's the thing: microservices aren't a silver bullet. In fact, they can become a massive technical debt that's incredibly hard to pay off. Matt Ranney, DoorDash's Scalability Engineer Lead, makes this point brilliantly in his talk &quot;Microservices are Technical Debt.&quot; After experiencing similar challenges, I decided to dive into this topic, including some cientific papers that covered similar issues like &quot;Microservices Anti-Patterns: A Taxonomy,&quot; to understand what's really going on. In this post, I'll try to cover: Why microservices are often misunderstood and misused Common anti-patterns I've encountered (and how to avoid them) Evidence-based approaches to build systems that actually work The Overuse of Microservices Why Do We Default to Microservices? The Hype Factor Companies often adopt microservices because it's trendy, without analyzing their actual needs. Example: Imagine you’re at a startup trying to attract investors. Your team decides to split a simple app into 10 microservices just to showcase &quot;scalability.&quot; Fast forward six months: you’re drowning in Kubernetes configurations and service mesh setups, while competitors with monoliths ship features twice as fast. The Independence Illusion While microservices promise independent teams and deployments, this only works with proper service boundaries. Example: Picture working on a user service. You update a field in the UserProfile class, only to discover the notifications service crashes because it hardcoded a dependency on the old field structure. Now you’re stuck updating three services for what should have been a simple change. The Monolith Misconception I've noticed a strange fear of monoliths in the industry. But here's what I've learned: for many applications, a well-structured modular monolith is actually the better choice. Example: A software development team spent months breaking their monolith into microservices, only to realize they had created a distributed monolith—a system that was just as hard to manage, but with added complexity. What if it was just a simplified collection of function calls instead of a bunch of meaningless HTTP calls? Microservices as Technical Debt Matt Ranney makes a compelling case for why microservices can be considered technical debt. Here’s why: Initial Speed, Long-Term Pain Microservices can speed up development in the short term, but they often lead to long-term maintenance challenges. Example: Imagine you’re building a feature to let users reset passwords. You start with a simple service: public void resetPassword(String userId, String newPassword) { User user = userService.getUser(userId); try{ userService.updateProfile(user, newPassword); } } But obviously the requirements grow and now this evolves into: // look at the complexity added to the same function try { userService.updateProfile(user, newPassword); notificationService.notifyProfileUpdate(user.getId()); authService.refreshUserSession(user.getId()); analyticsService.trackProfileUpdate(user.getId()); } catch (ServiceException e) { // now you need complex rollback logic =D compensationService.handleFailure(user.getId(), &quot;PROFILE_UPDATE&quot;); } Now, a simple password reset requires four services to work perfectly together. Miss one, and you’ve got angry users or security holes and a fresh war room to deal. 2. The Distributed Monolith Trap Let’s get real: most companies end up with distributed monoliths, not true microservices. Here’s why this happens and why it’s worse than a traditional monolith. Practical Example: The Loyalty Points Nightmare Imagine you’re working on an e-commerce system. You need to add a loyaltyPoints field to user profiles. Here’s what happens: User Service: public class User { private String id; private int loyaltyPoints; // New field } Payments Service: public class PaymentProcessor { public void applyDiscount(String userId) { User user = userService.getUser(userId); if (user.getLoyaltyPoints() &gt; 1000) { // Now depends on User's new field applyDiscount(); } } } Analytics Service: public class Analytics { public void trackPurchase(String userId) { User user = userService.getUser(userId); log(&quot;Purchase by user with &quot; + user.getLoyaltyPoints() + &quot; points&quot;); } } Suddenly, updating a single field requires: Coordinating deployments across three teams Ensuring all services update dependencies simultaneously Risking system-wide failures if any service lags This is the distributed monolith trap—a system with all the complexity of microservices but none of the benefits. As Newman (2021) notes, this anti-pattern is rampant in teams that prioritize speed over thoughtful design. 3. Hidden Costs of Microservices Microservices introduce hidden costs, such as network latency, service discovery, and inter-service communication. Example: Imagine you’re debugging why user sessions expire randomly. After days of checking code, you discover a 200ms delay between the auth service and session service. The timeout configuration didn’t account for this latency, causing sporadic failures. The fix? Hours and hours wasted of meaningless debugging time, as the root cause for the problem was a bad-optimized code deployed by the auth team. Common Anti-Patterns in Microservices The paper Microservices Anti-Patterns: A Taxonomy by Taibi et al. (2018) provides a solid framework for understanding these issues. Here are some key anti-patterns and real-world examples: 1. The Shared Database Anti-Pattern One of the most common mistakes is sharing databases between microservices. This creates tight coupling and defeats the purpose of having independent services. Example: Imagine you’re working on a notifications service that shares a database with the user service: -- Both services read from the same table SELECT email FROM users WHERE id = '123'; When the user service adds a new is_email_verified column and starts deleting unverified accounts, your notifications service starts failing because it wasn’t updated to handle the new logic. 2. Hardcoded URLs and Tight Coupling Hardcoding URLs or endpoints between services is a recipe for disaster. It creates tight coupling and makes the system more fragile. Example: Picture this code in your payments service: // Bad: Hardcoded URL String userServiceUrl = &quot;http://user-service-prod:8080/api/users&quot;; When you try to test this service locally, it fails because it can’t reach the production user service. 3. The &quot;Too Many Services&quot; Problem This one is a classig example... Splitting your system into too many tiny services can lead to chaos. Each service adds overhead in terms of deployment, monitoring, and maintenance. Example: Imagine you’re working on a food delivery app with these services: user-service restaurant-service menu-service (for restaurant menus) menu-item-service (for individual dishes) menu-category-service (for dish categories) Now, displaying a restaurant’s menu requires calls to three services. A simple feature like adding a new dish category takes weeks to implement across teams. 4. Lack of Governance Without proper guidelines, teams end up creating services that overlap or don’t integrate well. Example: A very known company had two teams building nearly identical services because there was no governance in place to coordinate their efforts. Solutions and Best Practices So, how do we avoid these pitfalls? First, it's important to recognize that both monolith decomposition and microservices modeling are complex fields, extensively studied in research and industry, but in general here are some widely adopted strategies: 1. Monolith First As Martin Fowler suggests, &quot;monolith first.&quot; Build a monolith, and only split it into microservices when necessary. Example: Imagine you’re building a new fitness tracking app. Start with a monolith: public class Workout { private String userId; private LocalDateTime startTime; private int durationMinutes; } Only split into microservices when: Different components have clearly different scaling needs. Teams are large enough to justify separate ownership. There's a structured governance process in your company over building decoupled services. Automated tests are available for each service. Automated deployments are available for each service. Live monitoring, distributed tracing and health checks are available for each service. Automated rollback during deployment are also available for each service. 2. Domain-Driven Design (DDD) Define clear boundaries for each service based on business domains. This helps avoid tight coupling and ensures that services are truly independent. Example: For an e-commerce platform: Bounded Context: Payments Bounded Context: Inventory Bounded Context: Shipping Each context has its own database and API boundaries. Changes to payment logic don’t affect shipping =D. 3. The Reverse Conway Maneuver The Conway’s Law states that organizations design systems that mirror their communication structures. The Reverse Conway Maneuver flips this around: design your teams to match the architecture you want, basically, the teams designing a go-to system architecture based on current needs, tech debts and throttles. Just like a &quot;reference architecture&quot; based on reverse engineering. This way you have clear boundaries designed for each team and let they execute their goal software architecture independently. Example: Consider a given company X, instead of having frontend, backend, and ops teams working in silos, they restructured teams around business capabilities: a 'Payments Team' owning both backend and UI for payments, and a 'Shipping Team' handling logistics end-to-end. This allowed them to scale services independently while keeping architecture cohesive. Conclusion I didn't mean to be the devil’s advocate here, but I tried to highlight some key points because, in many cases, the direction that microservices adoption takes ends up being unsustainable. That’s why it’s crucial to pay attention to the trade-offs and pitfalls discussed. This is not supposed to be an exhaustive analysis—far from it. As microservices, technical debt, and distributed architectures are vast and evolving fields, hundreds of thousands of cientific papers already discussed these topics. My goal was to cover some of the major issues I’ve encountered, and hopefully, this discussion helps you navigate the complexities of microservices with a more critical perspective. References Matt Ranney, Microservices are Technical Debt link. Taibi et al., Microservices Anti-Patterns: A Taxonomy (2018). Martin Fowler, Monolith First link. Conway’s Law and the Reverse Conway Maneuver (Various Sources)."},{"title":"Understanding and Mitigating AWS Lambda Throttling in High-Concurrency Workloads","description":null,"tags":["aws","lambda","throttling"],"slug":"lambda-throttling","html":"Introduction When dealing with high-concurrency workloads, scaling AWS Lambda effectively while avoiding throttling can become a challenge. This post explores a real-world scenario where an application(just like a worker), written in Kotlin, processed over 1,000,000 records in a blob located in S3 using a custom asynchronous iteration method. Each record triggered an asynchronous Lambda invocation that interacted with DynamoDB. However, the setup led to 429 Too Many Requests errors occurring consistently during peak loads exceeding 10,000 TPS, indicating throttling issues with AWS Lambda. The article will: Outline the problem faced while processing high-concurrency workloads. Explain AWS Lambda throttling mechanisms, based on the AWS Compute Blog article by James Beswick. Discuss solutions to mitigate throttling. TBD Maybe in the future I'll Provide a real-world proof of concept (POC) to evaluate each mitigation technique. Use Case To better illustrate the challenges and solutions, consider the following use case: Dataset: The workload involves processing a large file with 1 million records stored in an S3 bucket. Data Characteristics: Each record contains 8 columns of strings, primarily UUIDs (36 bytes each). This results in approximately 288 bytes per record. Worker Configuration: The application is deployed on a SINGLE node with the following specifications: vCPUs: 4 RAM: 8 GB Resource Calculations Memory Requirements: Each record occupies 288 bytes. For 100 concurrent coroutines: ( 288 * 100 = 28,800 bytes approx 28.8KB ) Adding a 20 KB overhead per coroutine for runtime management: ( 100 * 20KB = 2,000KB approx 2MB ) Total memory consumption: ( 28.8KB + 2,000KB = 2.028MB ) CPU Considerations: Let's assume each vCPU can handle approximately 100-150 threads (or coroutines) effectively, actually it could handle much more depending on workload. But we can safely assume this number of threads as a safe margin for the given setup, based on Kotlin async coroutines benchmark. For this use case, 4 vCPUs are sufficient to manage 100 concurrent coroutines with minimal contention. This setup ensures that the system remains stable while processing a high volume of records efficiently. The Challenge Problem Context A workload involving processing a large file of over 1,000,000 records can utilize concurrency in Kotlin to invoke AWS Lambda for each record. The Lambda function in this case performed a putItem operation on DynamoDB. Here’s an example of the Kotlin code for mapAsync: suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync( transformation: suspend (T) -&gt; R ): List&lt;R&gt; = coroutineScope { this@mapAsync .map { async { transformation(it) } } .awaitAll() } suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync( concurrency: Int, transformation: suspend (T) -&gt; R ): List&lt;R&gt; = coroutineScope { val semaphore = Semaphore(concurrency) this@mapAsync .map { async { semaphore.withPermit { transformation(it) } } } .awaitAll() } This method processes records significantly faster than a standard for loop, but it can flood the system with Lambda invocations, triggering throttling. The 429 Too Many Requests errors can be attributed to: Concurrency Limits: AWS imposes a limit on the number of concurrent executions per account. TPS (Transactions Per Second) Limits: High TPS can overwhelm the Invoke Data Plane. Burst Limits: Limits the rate at which concurrency can scale, governed by the token bucket algorithm. Observed Errors 429 Too Many Requests: Errors indicate that the Lambda invocations exceeded allowed concurrency or burst limits. DynamoDB “Provisioned Throughput Exceeded”: Errors occurred during spikes in DynamoDB writes. But this error won't be covered in this post, maybe in the future I can discuss strategies to work directly with dynamodb IO optimization, for now let's just ignore this one. AWS Lambda Throttling Mechanisms AWS enforces three key throttle limits to protect its infrastructure and ensure fair resource distribution: 1. Concurrency Limits Concurrency limits determine the number of in-flight Lambda executions allowed at a time. For example, with a concurrency limit of 1,000, up to 1,000 Lambda functions can execute simultaneously across all Lambdas in the account and region. 2. TPS Limits TPS is derived from concurrency and function duration. For instance: Function duration: 100 ms (equivalent to 100ms =100 × 10-3 = 0.1s) Concurrency: 1,000 TPS = Concurrency / Function Duration = 10,000 TPS If the function duration drops below 100 ms, TPS is capped at 10x the concurrency. 3. Burst Limits The burst limit ensures gradual scaling of concurrency, avoiding large spikes in cold starts. AWS uses the token bucket algorithm to enforce this: Each invocation consumes a token. Tokens refill at a fixed rate (e.g., 500 tokens per minute). The bucket has a maximum capacity (e.g., 1,000 tokens). For more details, refer to the AWS Lambda Burst Limits. Mitigation Strategies That being said, several approaches can be employed to mitigate the throttling scenarios observed in this case. These techniques aim to address the specific constraints and challenges imposed by the problem: 1. Limit Concurrency Using Semaphore Concurrency in Kotlin can be limited using the mapAsync function with a specified concurrency level: val results = records.mapAsync(concurrency = 100) { record -&gt; invokeLambda(record) } This implementation leverages coroutines in Kotlin to handle asynchronous operations efficiently. We don't want to deep dive here in how coroutines work, but think of it as a tool that allow lightweight threads to run without blocking, making it possible to manage multiple tasks concurrently without overwhelming system resources. In the use case described, where the workload involves processing millions of records within 100 concurrent coroutines, the concurrency level of 100 was chosen as a reasonable limit. This decision balances the capacity of the node, configured with 4 vCPUs and 8 GB of RAM, against the resource requirements of each coroutine. For example, each coroutine processes records with a memory overhead of approximately 28.8 KB per record, plus 20 KB for runtime management. This setup ensures stability while maximizing throughput within the system’s constraints. By introducing a Semaphore, the number of concurrent tasks can be restricted to this specified level. This prevents overloading the Lambda concurrency limits and reduces the risk of 429 Too Many Requests errors, ensuring that the system remains stable and performs reliably. Estimated Time to Process Using the following parameters: T: Execution time for a single Lambda invocation. n: Number of concurrent Lambda invocations. Total Records: Total number of records to process. The total processing time can be calculated as: Total Time = (Total Records / n) * T Example with T = 100 ms Given: Total Records = 1,000,000 n = 100 T = 100 ms Substituting into the formula: Total Time = (1,000,000 / 100) * 100 ms Simplifying: Total Time = 10,000 * 100 ms = 1,000,000 ms Converting to seconds and minutes: Total Time = 1,000,000 ms = 1,000 seconds = 16.67 minutes Key Advantages: Simple Implementation: Adding a Semaphore to the mapAsync function involves minimal changes to the codebase. Effective Throttling Control: The implementation ensures that the number of concurrent Lambda invocations does not exceed the predefined limit, maintaining system stability. Trade-offs: Increased Processing Time: While throttling prevents errors, it may result in longer overall processing times due to the limitation on simultaneous executions. No Guarantee: While this approach prevents the majority of 429 Too Many Requests errors, it does not guarantee that such errors will not occur again. This is because, even when the number of concurrent Lambdas in execution is controlled, the system might still exceed burst limits, which are governed by the token bucket algorithm. Difficult to Manage in Distributed Systems: This approach is more practical in scenarios with a single node running the application. In distributed systems with multiple nodes running the same application (e.g., 10 instances), it becomes challenging to coordinate a distributed TPS control mechanism. Each node would need to communicate and share state to ensure the total TPS remains within AWS limits, which significantly increases complexity. 2. Retry with Exponential Backoff Retries with exponential backoff handle throttled requests effectively by spreading out retry attempts over time. This reduces the chance of overwhelming the system further when transient issues or throttling limits occur. The exponential backoff algorithm increases the delay between retries after each failed attempt, making it particularly useful in high-concurrency systems and also in services/calls that might fail at times. How It Works: The implementation retries an AWS Lambda invocation up to a specified number of attempts, introducing exponentially increasing delays between retries. For example: suspend fun invokeWithRetry(record: Record, retries: Int = 3) { var attempts = 0 while (attempts &lt; retries) { try { invokeLambda(record) break } catch (e: Exception) { if (++attempts == retries) throw e delay((2.0.pow(attempts) * 100).toLong()) } } } Estimated Time to Process Assume: Each retry introduces a delay that doubles after every attempt. D: Cumulative delay for retries. r: Number of retry attempts per record. Cumulative delay is given by: D = Σ (2^i * T_retry) for i = 1 to r Where: T_retry = Base retry delay (e.g., 100 ms). Example with T_retry = 100 ms and r = 3: D = (2^1 * 100 ms) + (2^2 * 100 ms) + (2^3 * 100 ms) D = 200 ms + 400 ms + 800 ms = 1,400 ms If 10% of records require retries, the retry time is: Retry Time = (Total Records * 10%) * D / n Retry Time = (1,000,000 * 0.1) * 1,400 ms / 100 Retry Time = 1,400,000 ms = 1,400 seconds = 23.33 minutes Adding this to the initial processing time: Total Time = Initial Time + Retry Time Total Time = 16.67 minutes + 23.33 minutes = 40 minutes Pros: Handles transient errors gracefully: Retries ensure that temporary issues, such as short-lived throttling or network disruptions, do not result in failed processing. Distributed systems friendly: Can be independently implemented in each node, avoiding the need for centralized control mechanisms. Reduces system load during failures: The increasing delay between retries prevents the system from being overwhelmed. Cons: Adds latency: The exponential backoff mechanism inherently increases the time taken to complete processing, can take even BIGGER times when considering worst case scenarios(potentially 10x more the total time discussed). Increases code complexity and testability: Requires additional logic to manage retries and delays and testing those scenarios when only part of the requests fail. 3. Use SQS for Decoupling Amazon Simple Queue Service (SQS) can act as a buffer between producers (e.g., the application processing records) and consumers (e.g., AWS Lambda), enabling controlled, asynchronous processing of requests. This approach decouples the producer and consumer, ensuring the workload is processed at a rate the system can handle. How It Works: The application writes each record to an SQS queue instead of invoking AWS Lambda directly. AWS Lambda is configured to process messages from the queue at a controlled rate, dictated by the batch size and concurrency settings. This ensures that the rate of Lambda invocations remains within the account's concurrency and TPS limits. Additional Pattern: AWS Serverless Land Example This approach aligns with a pattern presented on AWS Serverless Land: Create a Lambda function that batch writes to DynamoDB from SQS. This pattern deploys an SQS queue, a Lambda Function, and a DynamoDB table, allowing batch writes from SQS messages to DynamoDB. It demonstrates how to leverage a batch processing mechanism to handle high-throughput scenarios effectively. The provided SAM template uses Java 11, SQS, Lambda, and DynamoDB to create a cost-effective, serverless architecture: AWSTemplateFormatVersion: 2010-09-09 Transform: AWS::Serverless-2016-10-31 Description: sqs-lambda-dynamodb Globals: Function: Runtime: java11 MemorySize: 512 Timeout: 25 Resources: OrderConsumer: Type: AWS::Serverless::Function Properties: FunctionName: OrderConsumer Handler: com.example.OrderConsumer::handleRequest CodeUri: target/sourceCode.zip Environment: Variables: QUEUE_URL: !Sub 'https://sqs.${AWS::Region}.amazonaws.com/${AWS::AccountId}/OrdersQueue' REGION: !Sub '${AWS::Region}' TABLE_NAME: !Ref OrdersTable Policies: - AWSLambdaSQSQueueExecutionRole - AmazonDynamoDBFullAccess OrdersQueue: Type: AWS::SQS::Queue Properties: QueueName: OrdersQueue OrdersTable: Type: 'AWS::DynamoDB::Table' Properties: TableName: OrdersTable AttributeDefinitions: - AttributeName: orderId AttributeType: S KeySchema: - AttributeName: orderId KeyType: HASH ProvisionedThroughput: ReadCapacityUnits: 5 WriteCapacityUnits: 5 Estimated Time to Process Assume: T_batch: Execution time for processing a batch. k: Overhead due to batching. b: Number of messages per batch. n: Lambda concurrency. The total processing time is: Total Time = (Total Records / (b * n)) * (T + k) Example with: T = 100 ms k = 20 ms b = 10 n = 100 Total Records = 1,000,000 Substitute into the formula: Total Time = (1,000,000 / (10 * 100)) * (100 ms + 20 ms) Total Time = (1,000,000 / 1,000) * 120 ms Total Time = 1,000 * 120 ms = 120,000 ms Convert to seconds and minutes: Total Time = 120,000 ms = 120 seconds = 2 minutes The Importance of FIFO Queues To maintain consistency in DynamoDB, it is essential to configure the SQS queue as FIFO (First-In, First-Out) in this case. This ensures that messages are processed in the exact order they are received, which is critical in systems where the order of operations affects the final state of the database. For example: Out-of-Order Processing Issues: If two updates to the same DynamoDB record are processed out of order (e.g., Update1 followed by Update2), but Update2 depends on Update1, the database could end up in an inconsistent state. FIFO queues prevent this by enforcing strict order. For our case, there was not duplicated entries on the file so FIFO was not in considerated despite being absolutely important for this usecase. Idempotency Challenges: Even when Lambda functions are designed to be idempotent, out-of-order processing can lead to unexpected behavior if operations rely on sequential execution. For instance, appending logs or incrementing counters requires a guarantee of order. Trade-offs with FIFO: While FIFO queues provide consistency, they come with some limitations: Lower Throughput: FIFO queues have a maximum throughput of 300 transactions per second with batching (or 3,000 if using high-throughput mode). Increased Latency: Enforcing order may introduce slight delays in message processing. Pros: Decouples producers and consumers: The producer can continue adding messages to the queue regardless of the Lambda processing speed. Prevents throttling: SQS regulates the rate at which messages are delivered to Lambda, avoiding sudden spikes that could exceed AWS limits. Distributed systems friendly: Works seamlessly in multi-node systems, as all nodes write to the same queue without requiring coordination. Cons: Adds architectural complexity: Introducing SQS requires additional components and configuration. Adds code complexity: Introduce code complexity to the insertion lambda, so its responsible for managing sqs batch write operations, reading on SQS source and also being able to operate by asynchronous invocation for legacy systems. Introduces latency: Messages may wait in the queue before being processed, depending on the Lambda polling rate and queue depth. For example, a queue depth of 10,000 messages and a polling rate of 1,000 messages per second would result in a processing delay. Conclusion AWS Lambda throttling issues, particularly for high-concurrency workloads, can be effectively managed using a combination of strategies such as concurrency control, retry mechanisms, and decoupling with SQS. Each of these approaches has its strengths and trade-offs: Limit Concurrency Using Semaphore: A straightforward solution for single-node setups, providing reliable throttling control at the cost of slightly increased processing time. However, it requires additional considerations for distributed systems. Retry with Exponential Backoff: A robust technique for handling transient failures, distributing load over time and avoiding unnecessary retries. Yet, it can add significant latency in worst-case scenarios and increase implementation complexity. Use SQS for Decoupling: The most scalable and efficient approach when T_batch = T + k, with k being sufficiently small. While it introduces latency and complexity, its benefits make it the go-to solution for large-scale systems. As an ending insight, we can assure that for small workloads, async invocation can provide faster results, as it avoids the latency of queuing and batch processing. However, as the number of requests increases, direct invocation becomes inefficient and computationally expensive due to the high TPS demand and risk of breaching AWS limits. In contrast, decoupled architectures using SQS and batch processing scale more efficiently, ensuring stability and cost-effectiveness under heavy loads. Next Steps: Implementing a POC While this post has focused on explaining the challenges, strategies, and theoretical calculations for mitigation, an actual Proof of Concept (POC) would be very cool to validate and visualize these solutions in practice. A future post might explore how to design and execute a POC to measure the overall performance in a real-world scenario. For more details on Lambda throttling, refer to the AWS Lambda Developer Guide and the AWS Compute Blog."},{"title":"Choosing a Garbage Collector for Your Java/Kotlin Application: Things I Wish I Knew Back Then","description":null,"tags":["java","garbage-collector","kotlin","jvm"],"slug":"garbage-collector","html":"Introduction When I first started building Java and Kotlin applications, I didn’t really pay much attention to garbage collection. It was this magical process that &quot;just worked.&quot; But as I moved into more complex systems—batch processing, high-throughput APIs, and distributed architectures—I realized that choosing the right garbage collector could make or break my application’s performance, and also prevent some later production incidents. Some of my early APIs even experienced breakdowns due to memory leaks, leading to unresponsive systems under heavy load. These episodes taught me the critical importance of understanding how GC works and how to configure it for specific workloads. Failing to consider GC for high-throughput APIs, for example, can lead to severe latency spikes, memory fragmentation, or outright crashes. This article is a guide for those who, like me, wish they had a clearer understanding of JVM garbage collectors earlier. I will try to cover: How garbage collection works in the JVM. The different types of GCs available. Real-world use cases and configs for each GC. Choosing the right garbage collector (references for informed decision-making). Conclusion &amp; Exercises ;-). Let’s dive in and make garbage collection work for you, not against you. How Garbage Collection Works in the JVM Garbage collection in the JVM is all about managing heap memory(imagine it's the playground where all your objects live). When objects are no longer referenced, they become eligible for garbage collection, freeing up memory for new allocations. But the process isn’t always seamless—GC pauses and overhead can significantly impact performance. Key Concepts Heap Memory Eden Space (in the Young Generation): Purpose: This is where new objects are first allocated. Garbage Collection Behavior: Objects in Eden are short-lived and quickly collected during a minor GC cycle if they are no longer in use. Example: Suppose you’re creating multiple instances of a Minion class. And those minions are from League of Legends or Despicable Me—your choice: for (int i = 0; i &lt; 1000; i++) { Minion minion = new Minion(&quot;Minion &quot; + i); } All these minions will initially be created in the Eden space. If they are not referenced anymore after their creation, they will be collected during the next minor GC. Survivor Spaces (in the Young Generation): Purpose: Objects that survive one or more minor GC cycles in Eden are moved to Survivor spaces. Garbage Collection Behavior: Survivor spaces act as a staging area before objects are promoted to the Old Generation. Example: In a game application, temporary data like dead minions or player movement logs might survive for a short time in Survivor spaces before being discarded or promoted if reused frequently. Old Generation: Purpose: Objects that have a long lifespan or survive multiple minor GC cycles are moved to the Old Generation. Garbage Collection Behavior: Garbage collection here is less frequent but more time-consuming. Example: Imagine you’re building a game where each Player represents a connected user on the match. These objects are long-lived compared to temporary data like minions or projectiles and may look like this: public class Player { private final String name; private final Inventory inventory; public Player(String name) { this.name = name; this.inventory = new Inventory(); } } A Player object, which holds data such as the player’s inventory and stats, will likely reside in the Old Generation as it persists for the entire application session. Metaspace: Purpose: Think of Metaspace as the library(outside the heap) of your application—it keeps the blueprints (class metadata) for all the objects your application creates. Garbage Collection Behavior: Metaspace grows dynamically as new class loaders are introduced and is cleaned up when those class loaders are no longer needed. This ensures that unused blueprints don’t mess up your libraries. Example: Imagine you’re running a game that supports mods, and players can load new heroes dynamically. Each mod represents a new class dynamically loaded at runtime: Class&lt;?&gt; heroClass = Class.forName(&quot;com.game.dynamic.Hero&quot;); Object hero = heroClass.getDeclaredConstructor().newInstance(); The blueprint for the Hero class will be stored in Metaspace. When the mod is unloaded or the player exits the game, the class loader is no longer needed, and the JVM will clean up the associated Metaspace memory. This ensures that your application remains efficient, even with dynamic features. Garbage Collector Phases Mark: Purpose: Identify live objects by traversing references starting from the root set (e.g., static fields, local variables). Practical Example: Consider this code: Player player = new Player(&quot;Hero&quot;); player.hitMinion(); The player object is reachable because it’s referenced in the method. During the Mark phase, the GC identifies player and its dependencies as live objects. Sweep: Purpose: Reclaim memory occupied by objects not marked as live. Practical Example: If the player reference is set to null: player = null; The next GC cycle’s Sweep phase will reclaim the memory occupied by the player object and its associated data. Compact: Purpose: Reduce fragmentation by moving objects closer together in memory. Practical Example: After reclaiming memory, gaps may exist in the heap. Compacting ensures efficient allocation for future objects: // Before compaction: [Minion 1][ ][Minion 3][ ] // After compaction: [Minion 1][Minion 3][ ] This step is particularly important in systems with frequent allocations and deallocations(Related to CPU efficiency). For a deep understanding, the JVM GC documentation provides wider insights (source). Types of JVM Garbage Collectors 1. Serial Garbage Collector (Serial GC) Overview: The Serial GC is single-threaded and optimized for simplicity. It processes the Young and Old Generations one at a time, pausing application threads during GC. When to Use: VERY SMALL applications with SINGLE-THREAD workloads. Low-memory environments (e.g., embedded systems). Limitations: Not suitable for high-concurrency, high-throughput systems. Maximum throughput is low due to its single-threaded nature. Example: Consider a system managing API calls for IoT devices that periodically send sensor data (e.g., room temperature). Each device sends minimal data in a predictable pattern, and the system handles only one request per thread. The Serial GC ensures predictable, low-overhead memory management, making it an ideal choice for such an environment. Docker Example: FROM openjdk:17-jdk-slim CMD java -XX:+UseSerialGC -Xmx512m -jar app.jar 2. Parallel Garbage Collector (Parallel GC) Overview: Parallel GC, also known as the Throughput Collector, uses multiple threads to speed up garbage collection. It aims to maximize application throughput by minimizing the total GC time. You can check some crazy a** graphs and get better explanation at the official documentation here. When to Use: Batch processing systems. Applications prioritizing throughput over low latency. Example: Imagine a financial service API that consolidates transactions into daily reports. Since the workload prioritizes throughput over latency, Parallel GC is ideal for processing large transaction sets efficiently. Docker Example: FROM openjdk:17-jdk-slim CMD java -XX:+UseParallelGC -Xmx2g -jar app.jar 3. G1 Garbage Collector (G1GC) Overview: G1GC divides the heap into regions and collects garbage incrementally, making it a good balance between throughput and low latency. When to Use: General-purpose applications. Systems requiring predictable pause times. Example: Any SaaS platform serving user requests in under 200ms with moderate traffic spikes. Docker Example: FROM openjdk:17-jdk-slim CMD java -XX:+UseG1GC -Xmx4g -XX:MaxGCPauseMillis=200 -jar app.jar Important considerations about G1GC: You might be wondering: &quot;If G1GC supports both good throughput and low latency, why not use it for every application? Sounds like a no-brainer...&quot; But well, not quite. While G1GC is a fantastic general-purpose garbage collector, it’s not the universal solution for all workloads. Think of it as the &quot;jack of all trades&quot; of GCs—good at many things, but not necessarily the best at any one thing. Poof! Now that you’re out of the cave, let’s analyze: Throughput-Focused Applications: If your application doesn’t care about pause times—for example, batch processing systems or data aggregation pipelines—why would you burden it with G1GC’s incremental collection overhead? Parallel GC is better suited here, offering raw performance without worrying about predictable pauses. Ultra-Low Latency Needs: If you’re building a real-time trading system or managing huge heaps (think terabytes), G1GC might struggle to meet your strict latency requirements. Collectors like ZGC or Shenandoah GC are designed specifically for these use cases, offering sub-10ms pause times. In short, G1GC is like that versatile tool in your toolbox—it works well for a variety of tasks, especially if you’re building the classic CRUD API (yes pretty much all of your messy simple Spring CRUDs). But if you’re running specialized workloads, you’ll want to pick a collector that’s optimized to your needs. 4. Z Garbage Collector (ZGC) Overview: ZGC is designed for ultra-low-latency applications with large heaps (up to terabytes). Its pause times are typically under 10 milliseconds. When to Use: Real-time systems. Applications with very large heaps. When to DO NOT use: Imagine you have a batch processing system using ZGC. There is very high chance of facing inceased CPU utilization($$$) without any latency benefit. For example, a data ingestion pipeline optimized for high throughput but insensitive to pause times would waste resources managing unnecessary low-latency GC cycles. Example: A trading system processing market data streams in real time. Docker Example: FROM openjdk:17-jdk-slim CMD java -XX:+UseZGC -Xmx16g -jar app.jar 5. Shenandoah Garbage Collector Overview: Shenandoah GC minimizes pause times by performing concurrent compaction. It’s ideal for latency-sensitive applications. When to Use: Payment gateways with strict SLA requirements for latency. APIs with spiky traffic patterns, such as social media feeds or live voting systems. Applications where reducing GC pause time is critical to user experience, such as gaming servers or interactive web applications. When to DO NOT use: Using Shenandoah GC for batch processing systems or workloads optimized for high throughput over low latency (e.g., nightly data aggregation) may lead to inefficient CPU utilization. The additional overhead of concurrent compaction provides no benefits when predictable pauses are acceptable, reducing overall throughput compared to Parallel GC. For exampe, a financial reconciliation batch process configured with Shenandoah might experience reduced throughput due to unnecessary focus on low pause times, delaying report generation. Example: A payment processing API handling high transaction volumes cannot afford GC-induced latency spikes during peak hours. Shenandoah’s low-pause nature ensures that transaction processing continues smoothly even under heavy load. Another example is a real-time multiplayer gaming server, where latency spikes could lead to a poor player experience. Shenandoah ensures consistent frame updates and server responsiveness. Docker Example: FROM openjdk:17-jdk-slim CMD java -XX:+UseShenandoahGC -Xmx8g -XX:+UnlockExperimentalVMOptions -jar app.jar Choosing the Right Garbage Collector Here you can find a cheatsheet. But remember... you should always evaluate your own workload before choosing it's garbage collector. Garbage Collector Best For JVM Version Support Serial GC Small, single-threaded apps All versions Parallel GC High-throughput batch systems All versions G1GC General-purpose apps Java 9+ ZGC Real-time, large heap apps Java 11+ Shenandoah GC Low-latency apps Java 11+ Conclusion Choosing the right garbage collector for your application requires some knowledge over the tools I discussed in this post. But once you learn about it, you may have the power of taking decisions, and this is extremely valuable in Software Engineering field, also, by selecting the right GC you can significantly improve performance, stability and save some costs for your future applications based on JVM. Don’t let GC be a black box—embrace it, tune it, and let it work for you. Training: Real-World Scenarios and Solutions Scenario 1: Payment Gateway Latency You are building a payment gateway API that must process transactions in real-time with strict SLA requirements. The workload is spiky, with heavy traffic during sales events or specific times of the day. Which garbage collector would you choose to ensure low latency? Scenario 2: Batch Data Processing System Your application processes daily financial reconciliation batches, which involve large amounts of data. Latency is not a concern, but throughput must be maximized to complete processing as fast as possible. Which garbage collector fits this use case? Scenario 3: Real-Time Multiplayer Game You are designing a server for a real-time multiplayer game. The server must manage thousands of players, each generating events continuously. Latency spikes during garbage collection are unacceptable as they could lead to lag and a poor user experience. What GC configuration would you use? Solutions Solution 1: Payment Gateway Latency Use Shenandoah GC to ensure low latency and consistent response times. Its concurrent compaction minimizes pause times, making it ideal for latency-sensitive workloads. Solution 2: Batch Data Processing System Use Parallel GC to maximize throughput. Since latency isn’t a concern, the Parallel GC’s focus on high efficiency during garbage collection fits this workload. Solution 3: Real-Time Multiplayer Game Use ZGC to achieve ultra-low latency and scale with large heaps. It ensures that garbage collection does not interfere with real-time gameplay. References: Java Garbage Collection Basics - Oracle"},{"title":"About me","description":null,"tags":[],"slug":"about","html":"I frequently discuss topics related to Cloud Architectures(mostly AWS), Distributed Systems, Highly Available Web Services, Banking Technology and Performance Optimization. Open to connecting globally for collaboration and knowledge sharing. Email me at pedro.hnblopes@gmail.com Follow me on linkedin https://www.linkedin.com/in/pedro-lopes-4563a7196/"}]