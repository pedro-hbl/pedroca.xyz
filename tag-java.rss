<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pedro Lopes</title><link></link><description>Comput. Sci.</description><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate><lastBuildDate>Wed, 09 Apr 2025 17:04:22 GMT</lastBuildDate><generator>marmite</generator><item><title>Common multithreading issues in Java</title><link>/java-spring-multithreading-pitfalls.html</link><author>pedrohbl_</author><category>java</category><category>spring</category><category>multithreading</category><category>concurrency</category><category>synchronization</category><category>best-practices</category><guid>/java-spring-multithreading-pitfalls.html</guid><pubDate>Wed, 02 Apr 2025 00:00:00 GMT</pubDate><source url="">tag-java</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>Multithreading in Java enables concurrent execution of multiple threads within a single application, potentially improving performance and resource utilization. However, improper thread management can cause a lot of bugs that only manifest under specific conditions.</p>
<p>In this post, I'll walk through some of the multithreading pitfalls I've encountered, along with some solutions and usecases/anti-patterns to help you avoid the same mistakes I've mapped here, in the following order:</p>
<ol>
<li><strong>Race Conditions</strong></li>
<li><strong>Memory Visibility Issues</strong></li>
<li><strong>Deadlocks</strong></li>
<li><strong>Thread Pool Configuration</strong></li>
</ol>
<blockquote>
<p><strong><em>NOTE:</em></strong> Maybe in a future post, I'll cover Spring-specific multithreading issues like @Async limitations, transaction boundaries, and event processing.*</p>
</blockquote>
<h2><a href="#race-conditions" aria-hidden="true" class="anchor" id="race-conditions"></a>Race Conditions</h2>
<h3><a href="#what-are-race-conditions" aria-hidden="true" class="anchor" id="what-are-race-conditions"></a>What Are Race Conditions?</h3>
<p>A race condition occurs when multiple threads access and modify shared data concurrently, leading to unpredictable behavior. The outcome depends on thread execution timing, making it difficult to reproduce and debug. According to a study published in the IEEE Transactions on Software Engineering, race conditions account for approximately 29% of all concurrency bugs in production systems [1]. In my experience, they account for about 99% of my hair lost.</p>
<h3><a href="#example" aria-hidden="true" class="anchor" id="example"></a>Example</h3>
<p>Consider a simple counter implementation:</p>
<pre><code class="language-java">public class Counter {
    private int count = 0;
    
    public void increment() {
        count++;
    }
    
    public int getCount() {
        return count;
    }
}
</code></pre>
<p>When multiple threads call <code>increment()</code> concurrently, the expected value might differ from actual results. This happens because <code>count++</code> is not atomic—it involves three operations:</p>
<ol>
<li>Read the current value of count</li>
<li>Increment the value</li>
<li>Write the new value back to count</li>
</ol>
<p>So basically if Trhead A and Thread B both read the initial value before either updates it, one increment will be lost.</p>
<h3><a href="#detecting-race-conditions" aria-hidden="true" class="anchor" id="detecting-race-conditions"></a>Detecting Race Conditions</h3>
<p>Race conditions can be detected using:</p>
<ol>
<li><strong>Java Thread Dumps</strong>: Analyze thread dumps when application behavior is inconsistent</li>
<li><strong>Code Reviews</strong>: Look for shared mutable state accessed by multiple threads</li>
<li><strong>Testing Tools</strong>: Tools like Java PathFinder, FindBugs, and JCStress can detect potential race conditions</li>
</ol>
<h3><a href="#solving-race-conditions-in-java" aria-hidden="true" class="anchor" id="solving-race-conditions-in-java"></a>Solving Race Conditions in Java</h3>
<h4><a href="#1-using-synchronized-keyword" aria-hidden="true" class="anchor" id="1-using-synchronized-keyword"></a>1. Using Synchronized Keyword</h4>
<pre><code class="language-java">public class Counter {
    private int count = 0;
    
    public synchronized void increment() {
        count++;
    }
    
    public synchronized int getCount() {
        return count;
    }
}
</code></pre>
<p>The <code>synchronized</code> keyword ensures that only one thread can execute the method at a time. It achieves this by acquiring an intrinsic lock (monitor) on the object instance. However, synchronization introduces overhead as threads must wait for the lock to be released (think of it as putting a bouncer at the door who only lets one person in at a time — secure, but creates a line).</p>
<h4><a href="#2-using-atomicinteger" aria-hidden="true" class="anchor" id="2-using-atomicinteger"></a>2. Using AtomicInteger</h4>
<p>Java's <code>java.util.concurrent.atomic</code> package provides thread-safe primitive types:</p>
<pre><code class="language-java">import java.util.concurrent.atomic.AtomicInteger;

public class Counter {
    private AtomicInteger count = new AtomicInteger(0);
    
    public void increment() {
        count.incrementAndGet();
    }
    
    public int getCount() {
        return count.get();
    }
}
</code></pre>
<p><code>AtomicInteger</code> uses Compare-And-Swap (CAS) operations, which are typically more efficient than synchronization for simple operations.</p>
<h4><a href="#3-using-lock-interface" aria-hidden="true" class="anchor" id="3-using-lock-interface"></a>3. Using Lock Interface</h4>
<p>For more complex scenarios, the <code>java.util.concurrent.locks</code> package offers more flexible locking mechanisms (when you need a more sophisticated bouncer):</p>
<pre><code class="language-java">import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

public class Counter {
    private int count = 0;
    private final Lock lock = new ReentrantLock();
    
    public void increment() {
        lock.lock();
        try {
            count++;
        } finally {
            lock.unlock(); 
        }
    }
    
    public int getCount() {
        lock.lock();
        try {
            return count;
        } finally {
            lock.unlock();
        }
    }
}
</code></pre>
<h3><a href="#real-world-example" aria-hidden="true" class="anchor" id="real-world-example"></a>Real-World Example</h3>
<p>Consider a payment processing gateway that handles high-volume financial transactions. In this scenario, a transaction processing system occasionally showed incorrect account balances. Analysis of production logs revealed a classic race condition where multiple threads were updating the same account simultaneously(I took this example from an interview I did couple of years ago):</p>
<pre><code class="language-java">public void processPayment(Long accountId, BigDecimal amount) {
    Account account = accountRepository.findById(accountId).orElseThrow();
    BigDecimal newBalance = account.getBalance().add(amount);
    account.setBalance(newBalance);
    accountRepository.save(account);
}
</code></pre>
<p>This implementation has a critical race condition:</p>
<ol>
<li>Thread A reads account balance (1000)</li>
<li>Thread B reads the same account balance (1000)</li>
<li>Thread A calculates new balance (1000 + 200 = 1200)</li>
<li>Thread B calculates new balance (1000 + 500 = 1500)</li>
<li>Thread A saves the new balance (1200)</li>
<li>Thread B saves the new balance (1500)</li>
</ol>
<p>In this scenario, the 200 payment processed by Thread A is effectively lost because Thread B overwrote it with its calculation based on the original balance. The system has lost 200 units of currency - a serious financial discrepancy.</p>
<p>The solution requires implementing proper transaction isolation. In database terms, this is precisely what the SERIALIZABLE isolation level is designed to prevent:</p>
<pre><code class="language-java">@Transactional(isolation = Isolation.SERIALIZABLE)
public void processPayment(Long accountId, BigDecimal amount) {
    Account account = accountRepository.findById(accountId).orElseThrow();
    BigDecimal newBalance = account.getBalance().add(amount);
    account.setBalance(newBalance);
    accountRepository.save(account);
}
</code></pre>
<p>With SERIALIZABLE isolation, the database ensures that concurrent transactions behave as if they were executed sequentially. This prevents the race condition by ensuring that:</p>
<ol>
<li>Thread A begins transaction and reads account (1000)</li>
<li>Thread B begins transaction and tries to read the same account</li>
<li>Thread B is blocked (or gets a version error, depending on implementation) until Thread A completes</li>
<li>Thread A updates balance to 1200 and commits</li>
<li>Thread B now reads the updated account (1200)</li>
<li>Thread B updates balance to 1700 (1200 + 500) and commits</li>
</ol>
<p>When implementing this solution, be aware that SERIALIZABLE has performance implications:</p>
<ol>
<li><strong>Concurrency reduction</strong>: It reduces the number of concurrent operations the system can perform</li>
<li><strong>Deadlock risk</strong>: Higher isolation levels increase the risk of deadlocks</li>
<li><strong>Performance cost</strong>: There's a tradeoff between consistency and throughput</li>
</ol>
<p>For some financial systems, a more scalable approach is optimistic locking with version control. Think of it like a Google Doc's version history - if two people edit the same document simultaneously, the system detects the conflict and handles it gracefully.</p>
<h4><a href="#how-optimistic-locking-works" aria-hidden="true" class="anchor" id="how-optimistic-locking-works"></a>How Optimistic Locking Works</h4>
<ol>
<li><strong>Version Tracking</strong>: Each record keeps track of its version number</li>
<li><strong>Read Phase</strong>: When reading a record, we store both its data AND version number</li>
<li><strong>Update Phase</strong>: When saving, we check if the version is still the same</li>
<li><strong>Conflict Detection</strong>: If someone else changed the record (version mismatch), we handle the conflict</li>
</ol>
<p>Here's a practical implementation using Spring:</p>
<pre><code class="language-java">@Entity
public class Account {
    @Id
    private Long id;
    
    private BigDecimal balance;
    
    @Version  // this annotation tells JPA to handle versioning
    private Long version;  // automatically incremented on each update
    
    }

@Service
@Transactional(isolation = Isolation.READ_COMMITTED)
public class PaymentService {
    private final AccountRepository accountRepository;
    
    public void processPayment(Long accountId, BigDecimal amount) {
        // the first thread reads version = 1
        Account account = accountRepository.findById(accountId).orElseThrow();
        BigDecimal newBalance = account.getBalance().add(amount);
        account.setBalance(newBalance);
        
        try {
            // if another thread updated the account (now version = 2),
            // this save will fail with OptimisticLockException
            accountRepository.save(account);
        } catch (OptimisticLockException e) {
            // handle the conflict(like a retry or something)
            throw new PaymentConflictException(&quot;Payment failed - please retry&quot;);
        }
    }
    
    // retry example
    @Retryable(maxAttempts = 3, backoff = @Backoff(delay = 100))
    public void processPaymentWithRetry(Long accountId, BigDecimal amount) {
        processPayment(accountId, amount);
    }
}
</code></pre>
<p>Let's see how this prevents the race condition:</p>
<ol>
<li>Thread A reads account (balance = 1000, version = 1)</li>
<li>Thread B reads account (balance = 1000, version = 1)</li>
<li>Thread A calculates new balance (1000 + 200 = 1200)</li>
<li>Thread B calculates new balance (1000 + 500 = 1500)</li>
<li>Thread A saves changes → success (balance = 1200, version = 2)</li>
<li>Thread B tries to save → FAILS because version changed</li>
<li>Thread B retries with fresh data (balance = 1200, version = 2)</li>
<li>Thread B calculates new balance (1200 + 500 = 1700)</li>
<li>Thread B saves changes → success (balance = 1700, version = 3)</li>
</ol>
<p>The key advantages of optimistic locking over SERIALIZABLE isolation:</p>
<ol>
<li><strong>Better Performance</strong>: No need to lock records - we only check versions when saving</li>
<li><strong>Higher Throughput</strong>: Multiple transactions can read the same data simultaneously</li>
<li><strong>Deadlock Prevention</strong>: No locks means no deadlocks</li>
<li><strong>Automatic Conflict Detection</strong>: The database handles version checking automatically</li>
</ol>
<p>The main trade-off is that you need to handle the retry logic when conflicts occur. This is usually acceptable because conflicts are typically rare in real-world scenarios - they only happen when two users try to modify the exact same record at the exact same time.</p>
<h2><a href="#memory-visibility-issues" aria-hidden="true" class="anchor" id="memory-visibility-issues"></a>Memory Visibility Issues</h2>
<h3><a href="#what-are-memory-visibility-issues" aria-hidden="true" class="anchor" id="what-are-memory-visibility-issues"></a>What Are Memory Visibility Issues?</h3>
<p>In Java's memory model, threads may cache variables locally instead of reading them from main memory. This can lead to a situation where updates made by one thread aren't visible to others. In the Java Memory Model (JMM) specification, variable updates without proper synchronization aren't guaranteed to be visible across threads.</p>
<blockquote>
<p><strong>Nerds alert</strong>: If you want to go for a deep dive into memory visibility, you'll need to dive into the Java Memory Model (JMM). For a comprehensive explanation, you might want to check &quot;Java Concurrency in Practice&quot; by Brian Goetz [4]. For the brave souls who want the formal specification(which I can't even explain explicitly), you'll be able to find it in Chapter 17.4 of the oracle doc(<a href="https://docs.oracle.com/javase/specs/jls/se17/html/jls-17.html#jls-17.4">link</a>).</p>
</blockquote>
<h3><a href="#example-of-memory-visibility-issues" aria-hidden="true" class="anchor" id="example-of-memory-visibility-issues"></a>Example of Memory Visibility Issues</h3>
<p>Consider a flag to control thread execution:</p>
<pre><code class="language-java">public class TaskManager {
    private boolean stopped = false;
    
    public void stop() {
        stopped = true;
    }
    
    public void runTask() {
        while (!stopped) {
            //do sometihng
        }
    }
}
</code></pre>
<p>Thread A could call <code>stop()</code> but Thread B might never see the update, resulting in an infinite loop. This is a classic memory visibility problem – the thread executing <code>runTask()</code> may maintain a local copy of <code>stopped</code> in its cache and never see the update made by another thread.</p>
<h3><a href="#solving-memory-visibility-issues" aria-hidden="true" class="anchor" id="solving-memory-visibility-issues"></a>Solving Memory Visibility Issues</h3>
<p>There are three main approaches to solving memory visibility issues, each with specific use cases:</p>
<h4><a href="#1-using-volatile-keyword" aria-hidden="true" class="anchor" id="1-using-volatile-keyword"></a>1. Using volatile Keyword</h4>
<p>The <code>volatile</code> keyword ensures that reads and writes go directly to main memory(like a &quot;no-cache&quot; flag):</p>
<pre><code class="language-java">public class TaskManager {
    private volatile boolean stopped = false;
    
    public void stop() {
        stopped = true;
    }
    
    public void runTask() {
        while (!stopped) {
            //do sometihng
        }
    }
}
</code></pre>
<p><strong>When to use <code>volatile</code>:</strong></p>
<ul>
<li>For simple variables that function as flags/signals</li>
<li>When you only need visibility guarantees (without atomicity)</li>
<li>When performance is critical (it's the lightest solution)</li>
<li><strong>Important:</strong> Doesn't guarantee atomicity for compound operations like <code>i++</code> (read + increment + write)</li>
</ul>
<h4><a href="#2-using-synchronized-access" aria-hidden="true" class="anchor" id="2-using-synchronized-access"></a>2. Using Synchronized Access</h4>
<p><code>synchronized</code> blocks provide both visibility guarantees and mutual exclusion (only one thread can execute the code at a time):</p>
<pre><code class="language-java">public class TaskManager {
    private boolean stopped = false;
    
    public synchronized void stop() {
        stopped = true;
    }
    
    public void runTask() {
        while (!isStopped()) {
            //do sometihng
        }
    }
    
    private synchronized boolean isStopped() { // &lt; -- thread safe access
        return stopped;
    }
}
</code></pre>
<p><strong>When to use <code>synchronized</code>:</strong></p>
<ul>
<li>When you need mutual exclusion in addition to visibility</li>
<li>To protect compound operations that must be atomic</li>
<li>When the same thread needs to check and update multiple related variables</li>
<li><strong>Disadvantage:</strong> Less peformartic when compared to <code>volatile</code></li>
</ul>
<h4><a href="#3-using-atomic-variables" aria-hidden="true" class="anchor" id="3-using-atomic-variables"></a>3. Using Atomic Variables</h4>
<p>Classes from the <code>java.util.concurrent.atomic</code> package combine visibility with atomic operations(the CAS):</p>
<pre><code class="language-java">import java.util.concurrent.atomic.AtomicBoolean;

public class TaskManager {
    private AtomicBoolean stopped = new AtomicBoolean(false);
    
    public void stop() {
        stopped.set(true);
    }
    
    public void runTask() {
        while (!stopped.get()) {
            //do sometihng
        }
    }
}
</code></pre>
<p><strong>When to use <code>Atomic</code>:</strong></p>
<ul>
<li>When you need atomic operations without the overhead of full locking</li>
<li>For counters, accumulators, and flags that need atomic operations</li>
<li>To implement high-performance lock-free algorithms</li>
<li>When you need atomic compound operations like compareAndSet()</li>
<li><strong>Advantage:</strong> Better scalability under high contention compared to <code>synchronized</code></li>
</ul>
<h3><a href="#comparison-between-the-approaches" aria-hidden="true" class="anchor" id="comparison-between-the-approaches"></a>Comparison between the approaches</h3>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Visibility</th>
<th>Atomicity</th>
<th>Locking</th>
<th>Performance</th>
<th>Use Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>volatile</code></td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
<td>Excellent</td>
<td>Simple flags, visibility only</td>
</tr>
<tr>
<td><code>synchronized</code></td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>Good/Medium</td>
<td>Protecting complex shared state</td>
</tr>
<tr>
<td><code>Atomic</code></td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>Very good</td>
<td>Counters, CAS operations, high concurrency</td>
</tr>
</tbody>
</table>
<h2><a href="#deadlocks" aria-hidden="true" class="anchor" id="deadlocks"></a>Deadlocks</h2>
<h3><a href="#what-are-deadlocks" aria-hidden="true" class="anchor" id="what-are-deadlocks"></a>What Are Deadlocks?</h3>
<p>In technical terms, a deadlock occurs when two or more threads each hold a resource that the other needs to continue execution. This creates a circular dependency where each thread is blocked indefinitely, waiting for resources that will never be released.</p>
<p><strong>The four necessary conditions for a deadlock (all must be present):</strong></p>
<ol>
<li><strong>Mutual Exclusion</strong>: Resources cannot be shared simultaneously</li>
<li><strong>Hold and Wait</strong>: Threads hold resources while waiting for others</li>
<li><strong>No Preemption</strong>: Resources cannot be forcibly taken from threads</li>
<li><strong>Circular Wait</strong>: A circular chain of threads, each waiting for a resource held by the next</li>
</ol>
<p>I think of deadlocks like an episode of Foster's Home For Imaginary Friends(which I watched as a kid), where Wilt (or Minguado in Portuguese) is that super polite imaginary friend who gets stuck at a doorway: &quot;After you!&quot; &quot;No, after you!&quot; &quot;I insist, after you!&quot; &quot;No, after you!&quot; So he remains stuck forever. In real life, social awkwardness would eventually break this standoff as someone gives in. But in your application, there's no episode ending or social pressure to resolve the situation—just an unresponsive system that needs rebooting.</p>
<!-- ![Wilt - or Minguado for brazilians](/media/multithreading/wilt.jpg) -->
<div style="text-align: center; margin: 20px 0;">
  <img src="/media/multithreading/wilt.jpg" alt="Wilt from Foster's Home For Imaginary Friends" style="width: 350px; max-width: 100%;">
  <p style="font-style: italic; font-size: 0.9em;">Wilt - or Minguado for brazilians.</p>
</div>
<h3><a href="#example-of-deadlock" aria-hidden="true" class="anchor" id="example-of-deadlock"></a>Example of Deadlock</h3>
<p>Here's a classic deadlock scenario with two resources and two threads(This one I also took from an interview I did some time ago):</p>
<pre><code class="language-java">public class BankTransferDeadlock {
    private final Object accountALock = new Object();
    private final Object accountBLock = new Object();
    
    private double accountABalance = 1000;
    private double accountBBalance = 1000;
    
    // Thread 1 IS EXECUTING THIS
    public void transferAtoB(double amount) {
        synchronized(accountALock) { // acquire lock on account A
            System.out.println(&quot;Thread 1: Locked account A&quot;);
            
            // simulate some work before trying to acquire second lock
            try { Thread.sleep(100); } catch (InterruptedException e) {}
            
            accountABalance -= amount;
            
            synchronized(accountBLock) { // try to acquire lock on account B
                System.out.println(&quot;Thread 1: Locked account B&quot;);
                accountBBalance += amount;
                System.out.println(&quot;Transfer from A to B complete&quot;);
            }
        }
    }
    
    // Thread 2 EXECUITING THIS
    public void transferBtoA(double amount) {
        synchronized(accountBLock) { // acquire lock on account B
            System.out.println(&quot;Thread 2: Locked account B&quot;);
            
            // simulate some work before trying to acquire second lock
            try { Thread.sleep(100); } catch (InterruptedException e) {}
            
            accountBBalance -= amount;
            
            synchronized(accountALock) { // try to acquire lock on account A
                System.out.println(&quot;Thread 2: Locked account A&quot;);
                accountABalance += amount;
                System.out.println(&quot;Transfer from B to A complete&quot;);
            }
        }
    }
}
</code></pre>
<p>Here's what happens when this deadlock occurs:</p>
<ol>
<li>Thread 1 calls <code>transferAtoB()</code> and acquires the lock on account A</li>
<li>Thread 2 calls <code>transferBtoA()</code> and acquires the lock on account B</li>
<li>Thread 1 tries to lock account B, but it's already locked by Thread 2 → blocks</li>
<li>Thread 2 tries to lock account A, but it's already locked by Thread 1 → blocks</li>
<li>Both threads are now waiting for resources held by the other.</li>
</ol>
<p>The application appears to freeze with no error message - one of the most frustrating bugs to troubleshoot. In production systems, this often manifests as an application that suddenly stops responding and requires a restart. Unless saying that your algorithm runs on O(∞) in the worst case is acceptable, you should avoid it.</p>
<h3><a href="#preventing-deadlocks" aria-hidden="true" class="anchor" id="preventing-deadlocks"></a>Preventing Deadlocks</h3>
<h4><a href="#1-lock-ordering" aria-hidden="true" class="anchor" id="1-lock-ordering"></a>1. Lock Ordering</h4>
<p>Always acquire locks in the same order:</p>
<pre><code class="language-java">public class ResourceManager {
    private final Object resourceA = new Object();
    private final Object resourceB = new Object();
    
    public void process1() {
        synchronized(resourceA) {
            synchronized(resourceB) {
                // do something with both resources
            }
        }
    }
    
    public void process2() {
        synchronized(resourceA) { // now acquiring in same order as process1
            synchronized(resourceB) {
                // do something with both resources
            }
        }
    }
}
</code></pre>
<h4><a href="#2-using-trylock-with-timeout" aria-hidden="true" class="anchor" id="2-using-trylock-with-timeout"></a>2. Using tryLock with Timeout</h4>
<p>The <code>Lock</code> interface provides <code>tryLock()</code> with timeout to avoid indefinite waiting:</p>
<pre><code class="language-java">import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;
import java.util.concurrent.TimeUnit;

public class ResourceManager {
    private final Lock lockA = new ReentrantLock();
    private final Lock lockB = new ReentrantLock();
    
    public void process() throws InterruptedException {
        boolean gotBothLocks = false;
        
        try {
            boolean gotLockA = lockA.tryLock(1, TimeUnit.SECONDS);
            if (gotLockA) {
                try {
                    boolean gotLockB = lockB.tryLock(1, TimeUnit.SECONDS);
                    gotBothLocks = gotLockB;
                } finally {
                    if (!gotBothLocks) {
                        lockA.unlock(); // release first lock if couldn't get second
                    }
                }
            }
            
            if (gotBothLocks) {
                // do something with both resources
            } else {
                log.warn(&quot;Failed to acquire locks, will retry later&quot;);
            }
        } finally {
            if (gotBothLocks) {
                lockB.unlock();
                lockA.unlock();
            }
        }
    }
}
</code></pre>
<h4><a href="#3-using-javautilconcurrent-classes" aria-hidden="true" class="anchor" id="3-using-javautilconcurrent-classes"></a>3. Using java.util.concurrent Classes</h4>
<p>Higher-level concurrency utilities often handle lock management internally (remember the inversion of control):</p>
<pre><code class="language-java">import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ConcurrentHashMap;

public class ResourceManager {
    private final ExecutorService executor = Executors.newFixedThreadPool(10);
    private final ConcurrentHashMap&lt;String, Resource&gt; resources = new ConcurrentHashMap&lt;&gt;();
    
    // no explicit locking needed for many operations
}
</code></pre>
<h2><a href="#thread-pool-configuration-issues" aria-hidden="true" class="anchor" id="thread-pool-configuration-issues"></a>Thread Pool Configuration Issues</h2>
<h3><a href="#common-thread-pool-problems" aria-hidden="true" class="anchor" id="common-thread-pool-problems"></a>Common Thread Pool Problems</h3>
<p>Java applications often use thread pools via ExecutorService. Incorrect configuration can lead to:</p>
<ol>
<li><strong>Thread starvation</strong>: When all threads are busy and tasks queue up</li>
<li><strong>Resource exhaustion</strong>: When too many threads consume too much memory</li>
</ol>
<h3><a href="#thread-pool-best-practices" aria-hidden="true" class="anchor" id="thread-pool-best-practices"></a>Thread Pool Best Practices</h3>
<h4><a href="#1-size-thread-pools-appropriately" aria-hidden="true" class="anchor" id="1-size-thread-pools-appropriately"></a>1. Size Thread Pools Appropriately</h4>
<p>For CPU-bound tasks:</p>
<pre><code class="language-java">int cpuCores = Runtime.getRuntime().availableProcessors();
ExecutorService executor = Executors.newFixedThreadPool(cpuCores);
</code></pre>
<p>For I/O-bound tasks (things that wait a lot):</p>
<pre><code class="language-java">// I/O bound tasks can benefit from more threads
// (because threads spend most of their time waiting)
int threadPoolSize = Runtime.getRuntime().availableProcessors() * 2;
ExecutorService executor = Executors.newFixedThreadPool(threadPoolSize);
</code></pre>
<h4><a href="#2-use-different-thread-pools-for-different-types-of-tasks" aria-hidden="true" class="anchor" id="2-use-different-thread-pools-for-different-types-of-tasks"></a>2. Use Different Thread Pools for Different Types of Tasks</h4>
<pre><code class="language-java">// image the scenario you make your ferrari wait behind a garbage truck
ExecutorService cpuBoundTasks = Executors.newFixedThreadPool(cpuCores);
ExecutorService ioBoundTasks = Executors.newFixedThreadPool(cpuCores * 2);
</code></pre>
<h4><a href="#3-use-bounded-queues" aria-hidden="true" class="anchor" id="3-use-bounded-queues"></a>3. Use Bounded Queues</h4>
<p>Choosing the right queue type and size is crucial for thread pool performance. The queue acts as a buffer between task submission and execution, but an unbounded queue can lead to OutOfMemoryError if tasks are submitted faster than they can be processed.</p>
<p>There are three main queuing strategies:</p>
<ol>
<li>
<p><strong>Direct handoff</strong> (SynchronousQueue): Tasks are handed directly to threads. If no thread is available, the task submission is rejected. Best for CPU-intensive tasks where queuing would just add overhead.</p>
</li>
<li>
<p><strong>Bounded queue</strong> (ArrayBlockingQueue): Provides a buffer but with a limit, preventing resource exhaustion. Best for mixed workloads where some queuing helps smooth out bursts of requests.</p>
</li>
<li>
<p><strong>Unbounded queue</strong> (LinkedBlockingQueue): Can grow indefinitely. Only appropriate when task submission rate is naturally limited or when you have infinite memory (spoiler: you don't).</p>
</li>
</ol>
<p>Here's how to implement a bounded queue with appropriate rejection handling:</p>
<pre><code class="language-java">int corePoolSize = 5;
int maxPoolSize = 10;
long keepAliveTime = 60L;
BlockingQueue&lt;Runnable&gt; workQueue = new ArrayBlockingQueue&lt;&gt;(100);

ThreadPoolExecutor executor = new ThreadPoolExecutor(
    corePoolSize, 
    maxPoolSize, 
    keepAliveTime, 
    TimeUnit.SECONDS, 
    workQueue,
    new ThreadPoolExecutor.CallerRunsPolicy()); // saturation policy
</code></pre>
<p>The rejection/saturation policy determines what happens when both the queue and the thread pool are full:</p>
<ul>
<li><strong>CallerRunsPolicy</strong>: Executes the task in the caller's thread (as shown above)</li>
<li><strong>AbortPolicy</strong>: Throws RejectedExecutionException (default)</li>
<li><strong>DiscardPolicy</strong>: Silently drops the task</li>
<li><strong>DiscardOldestPolicy</strong>: Drops the oldest queued task to make room</li>
</ul>
<p>Goetz recommends the CallerRunsPolicy as it provides a form of throttling - when the system is overloaded, the submitting threads start executing tasks themselves, naturally slowing down the submission rate.</p>
<p>A real-world sizing example:</p>
<h5><a href="#scenario-1-order-processor-with-unbounded-queue" aria-hidden="true" class="anchor" id="scenario-1-order-processor-with-unbounded-queue"></a>Scenario 1: Order Processor with Unbounded Queue</h5>
<pre><code class="language-java">@Service
public class OrderProcessor {
    private final ExecutorService executor = new ThreadPoolExecutor(
        10, 10, 60L, TimeUnit.SECONDS,
        new LinkedBlockingQueue&lt;&gt;() // unbounded queue - DANGER!
    );
    
    public void processOrder(Order order) {
        executor.submit(() -&gt; {
            // process order (validate payment, reserve inventory, etc)
            // takes ~500ms per order
        });
    }
}
</code></pre>
<p><strong>Problem</strong>: During high-load events like Black Friday:</p>
<ol>
<li>Thousands of orders arrive per second</li>
<li>All orders get accepted into memory</li>
<li>Java heap grows indefinitely</li>
<li>Eventually: OutOfMemoryError</li>
</ol>
<h5><a href="#scenario-2-order-processor-with-bounded-queue" aria-hidden="true" class="anchor" id="scenario-2-order-processor-with-bounded-queue"></a>Scenario 2: Order Processor with Bounded Queue</h5>
<pre><code class="language-java">@Service
public class OrderProcessor {
    private static final int CORE_THREADS = 10;
    private static final int MAX_THREADS = 20;

    // based on available memory 
    //(maybe in the future I can write how to calculate and monitor it based on the container you're using)
    private static final int QUEUE_CAPACITY = 500; 
    
    private final ThreadPoolExecutor executor = new ThreadPoolExecutor(
        CORE_THREADS,
        MAX_THREADS,
        60L, TimeUnit.SECONDS,
        new ArrayBlockingQueue&lt;&gt;(QUEUE_CAPACITY),
        new ThreadPoolExecutor.CallerRunsPolicy()
    );
    
    public void processOrder(Order order) {
        executor.submit(() -&gt; {
            // do processing, but now with backpressure
        });
    }

}
</code></pre>
<p>With this configuration:</p>
<ol>
<li>First 500 excess tasks go to the queue</li>
<li>When queue fills, CallerRunsPolicy makes the caller thread execute the task</li>
<li>This naturally slows down upstream systems (API Gateway, Load Balancer)</li>
<li>System remains stable even under extreme load</li>
</ol>
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>Of course that compared to Brian Goetz and Doug Lea (who literally wrote the concurrency library in Java), I'm just a protozoan on their shoe. What I've shared here is merely a light breath on the most important concepts. To truly master these topics, I strongly recommend reading Goetz's &quot;Java Concurrency in Practice&quot; - it's the definitive resource that helped me understand these complex concepts.</p>
<p>But to sum everything up, when designing java multithreaded applications always favor simplicity and proven patterns over complex custom solutions. Java's concurrency utilities in the <code>java.util.concurrent</code> package, combined with a solid understanding of the principles I've covered in this post, will help you build robust, thread-safe applications.</p>
<p>As Goetz states: <em>&quot;Write thread-safe code, but don't use more synchronization than necessary.&quot;</em> This balance between safety and performance is the key to effective concurrent programming - a lesson I'm still learning every day :)</p>
<h2><a href="#references" aria-hidden="true" class="anchor" id="references"></a>References</h2>
<ul>
<li>
<p>Lu, S., Park, S., Seo, E., &amp; Zhou, Y. &quot;Learning from mistakes: a comprehensive study on real world concurrency bug characteristics.&quot; ACM SIGARCH Computer Architecture News, 36(1), 2008. <a href="https://dl.acm.org/doi/10.1145/1346281.1346323">https://dl.acm.org/doi/10.1145/1346281.1346323</a>.</p>
</li>
<li>
<p>Manson, J., Pugh, W., &amp; Adve, S. V. &quot;The Java memory model.&quot; ACM SIGPLAN Notices, 40(1), 2005. <a href="https://dl.acm.org/doi/10.1145/1040305.1040336">https://dl.acm.org/doi/10.1145/1040305.1040336</a>.</p>
</li>
<li>
<p>Goetz, B., Peierls, T., Bloch, J., Bowbeer, J., Holmes, D., &amp; Lea, D. <em>Java Concurrency in Practice</em>. Addison-Wesley Professional, 2006. <a href="https://github.com/AngelSanchezT/books-1/blob/master/concurrency/Java%20Concurrency%20in%20Practice.pdf">https://github.com/AngelSanchezT/books-1/blob/master/concurrency/Java%20Concurrency%20in%20Practice.pdf</a>.</p>
</li>
</ul>
<hr />
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Choosing a Garbage Collector for Your Java/Kotlin Application: Things I Wish I Knew Back Then</title><link>/garbage-collector.html</link><author>pedrohbl_</author><category>java</category><category>garbage-collector</category><category>kotlin</category><category>jvm</category><guid>/garbage-collector.html</guid><pubDate>Sun, 12 Jan 2025 00:00:00 GMT</pubDate><source url="">tag-java</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>When I first started building Java and Kotlin applications, I didn’t really pay much attention to garbage collection. It was this magical process that &quot;just worked.&quot; But as I moved into more complex systems—batch processing, high-throughput APIs, and distributed architectures—I realized that choosing the right garbage collector could make or break my application’s performance, and also prevent some later production incidents.</p>
<p>Some of my early APIs even experienced breakdowns due to memory leaks, leading to unresponsive systems under heavy load. These episodes taught me the critical importance of understanding how GC works and how to configure it for specific workloads. Failing to consider GC for high-throughput APIs, for example, can lead to severe latency spikes, memory fragmentation, or outright crashes.</p>
<p>This article is a guide for those who, like me, wish they had a clearer understanding of JVM garbage collectors earlier. I will try to cover:</p>
<ol>
<li>How garbage collection works in the JVM.</li>
<li>The different types of GCs available.</li>
<li>Real-world use cases and configs for each GC.</li>
<li>Choosing the right garbage collector (references for informed decision-making).</li>
<li>Conclusion &amp; Exercises ;-).</li>
</ol>
<p>Let’s dive in and make garbage collection work <em>for</em> you, not against you.</p>
<hr />
<h2><a href="#how-garbage-collection-works-in-the-jvm" aria-hidden="true" class="anchor" id="how-garbage-collection-works-in-the-jvm"></a>How Garbage Collection Works in the JVM</h2>
<p>Garbage collection in the JVM is all about managing heap memory(imagine it's the playground where all your objects live). When objects are no longer referenced, they become eligible for garbage collection, freeing up memory for new allocations. But the process isn’t always seamless—GC pauses and overhead can significantly impact performance.</p>
<h3><a href="#key-concepts" aria-hidden="true" class="anchor" id="key-concepts"></a>Key Concepts</h3>
<h4><a href="#heap-memory" aria-hidden="true" class="anchor" id="heap-memory"></a>Heap Memory</h4>
<ol>
<li>
<p><strong>Eden Space (in the Young Generation):</strong></p>
<ul>
<li><strong>Purpose:</strong> This is where new objects are first allocated.</li>
<li><strong>Garbage Collection Behavior:</strong> Objects in Eden are short-lived and quickly collected during a minor GC cycle if they are no longer in use.</li>
<li><strong>Example:</strong> Suppose you’re creating multiple instances of a <code>Minion</code> class. And those minions are from <em>League of Legends</em> or <em>Despicable Me</em>—your choice:
<pre><code class="language-java">for (int i = 0; i &lt; 1000; i++) {
    Minion minion = new Minion(&quot;Minion &quot; + i);
}
</code></pre>
All these minions will initially be created in the Eden space. If they are not referenced anymore after their creation, they will be collected during the next minor GC.</li>
</ul>
</li>
<li>
<p><strong>Survivor Spaces (in the Young Generation):</strong></p>
<ul>
<li><strong>Purpose:</strong> Objects that survive one or more minor GC cycles in Eden are moved to Survivor spaces.</li>
<li><strong>Garbage Collection Behavior:</strong> Survivor spaces act as a staging area before objects are promoted to the Old Generation.</li>
<li><strong>Example:</strong> In a game application, temporary data like dead minions or player movement logs might survive for a short time in Survivor spaces before being discarded or promoted if reused frequently.</li>
</ul>
</li>
<li>
<p><strong>Old Generation:</strong></p>
<ul>
<li><strong>Purpose:</strong> Objects that have a long lifespan or survive multiple minor GC cycles are moved to the Old Generation.</li>
<li><strong>Garbage Collection Behavior:</strong> Garbage collection here is less frequent but more time-consuming.</li>
<li><strong>Example:</strong> Imagine you’re building a game where each <code>Player</code> represents a connected user on the match. These objects are long-lived compared to temporary data like minions or projectiles and may look like this:
<pre><code class="language-java">public class Player {
    private final String name;
    private final Inventory inventory;

    public Player(String name) {
        this.name = name;
        this.inventory = new Inventory();
    }
}
</code></pre>
A <code>Player</code> object, which holds data such as the player’s inventory and stats, will likely reside in the Old Generation as it persists for the entire application session.</li>
</ul>
</li>
<li>
<p><strong>Metaspace:</strong></p>
<ul>
<li><strong>Purpose:</strong> Think of Metaspace as the library(outside the heap) of your application—it keeps the blueprints (class metadata) for all the objects your application creates.</li>
<li><strong>Garbage Collection Behavior:</strong> Metaspace grows dynamically as new class loaders are introduced and is cleaned up when those class loaders are no longer needed. This ensures that unused blueprints don’t mess up your libraries.</li>
<li><strong>Example:</strong> Imagine you’re running a game that supports mods, and players can load new heroes dynamically. Each mod represents a new class dynamically loaded at runtime:
<pre><code class="language-java">Class&lt;?&gt; heroClass = Class.forName(&quot;com.game.dynamic.Hero&quot;);
Object hero = heroClass.getDeclaredConstructor().newInstance();
</code></pre>
The blueprint for the <code>Hero</code> class will be stored in Metaspace. When the mod is unloaded or the player exits the game, the class loader is no longer needed, and the JVM will clean up the associated Metaspace memory. This ensures that your application remains efficient, even with dynamic features.</li>
</ul>
</li>
</ol>
<h4><a href="#garbage-collector-phases" aria-hidden="true" class="anchor" id="garbage-collector-phases"></a>Garbage Collector Phases</h4>
<ol>
<li>
<p><strong>Mark:</strong></p>
<ul>
<li><strong>Purpose:</strong> Identify live objects by traversing references starting from the root set (e.g., static fields, local variables).</li>
<li><strong>Practical Example:</strong> Consider this code:
<pre><code class="language-java">Player player = new Player(&quot;Hero&quot;);
player.hitMinion();
</code></pre>
The <code>player</code> object is reachable because it’s referenced in the method. During the Mark phase, the GC identifies <code>player</code> and its dependencies as live objects.</li>
</ul>
</li>
<li>
<p><strong>Sweep:</strong></p>
<ul>
<li><strong>Purpose:</strong> Reclaim memory occupied by objects not marked as live.</li>
<li><strong>Practical Example:</strong> If the <code>player</code> reference is set to <code>null</code>:
<pre><code class="language-java">player = null;
</code></pre>
The next GC cycle’s Sweep phase will reclaim the memory occupied by the <code>player</code> object and its associated data.</li>
</ul>
</li>
<li>
<p><strong>Compact:</strong></p>
<ul>
<li><strong>Purpose:</strong> Reduce fragmentation by moving objects closer together in memory.</li>
<li><strong>Practical Example:</strong> After reclaiming memory, gaps may exist in the heap. Compacting ensures efficient allocation for future objects:
<pre><code class="language-java">// Before compaction: [Minion 1][   ][Minion 3][   ]
// After compaction:  [Minion 1][Minion 3][       ]
</code></pre>
This step is particularly important in systems with frequent allocations and deallocations(Related to CPU efficiency).</li>
</ul>
</li>
</ol>
<p>For a deep understanding, the JVM GC documentation provides wider insights (<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/">source</a>).</p>
<hr />
<h2><a href="#types-of-jvm-garbage-collectors" aria-hidden="true" class="anchor" id="types-of-jvm-garbage-collectors"></a>Types of JVM Garbage Collectors</h2>
<h3><a href="#1-serial-garbage-collector-serial-gc" aria-hidden="true" class="anchor" id="1-serial-garbage-collector-serial-gc"></a>1. Serial Garbage Collector (Serial GC)</h3>
<h4><a href="#overview" aria-hidden="true" class="anchor" id="overview"></a>Overview:</h4>
<p>The Serial GC is single-threaded and optimized for simplicity. It processes the Young and Old Generations one at a time, pausing application threads during GC.</p>
<h4><a href="#when-to-use" aria-hidden="true" class="anchor" id="when-to-use"></a>When to Use:</h4>
<ul>
<li>VERY SMALL applications with SINGLE-THREAD workloads.</li>
<li>Low-memory environments (e.g., embedded systems).</li>
</ul>
<h4><a href="#limitations" aria-hidden="true" class="anchor" id="limitations"></a>Limitations:</h4>
<ul>
<li>
<p>Not suitable for high-concurrency, high-throughput systems.</p>
</li>
<li>
<p>Maximum throughput is low due to its single-threaded nature.</p>
</li>
</ul>
<h4><a href="#example" aria-hidden="true" class="anchor" id="example"></a>Example:</h4>
<p>Consider a system managing API calls for IoT devices that periodically send sensor data (e.g., room temperature). Each device sends minimal data in a predictable pattern, and the system handles only one request per thread. The Serial GC ensures predictable, low-overhead memory management, making it an ideal choice for such an environment.</p>
<h4><a href="#docker-example" aria-hidden="true" class="anchor" id="docker-example"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseSerialGC -Xmx512m -jar app.jar
</code></pre>
<hr />
<h3><a href="#2-parallel-garbage-collector-parallel-gc" aria-hidden="true" class="anchor" id="2-parallel-garbage-collector-parallel-gc"></a>2. Parallel Garbage Collector (Parallel GC)</h3>
<h4><a href="#overview-1" aria-hidden="true" class="anchor" id="overview-1"></a>Overview:</h4>
<p>Parallel GC, also known as the Throughput Collector, uses multiple threads to speed up garbage collection. It aims to maximize application throughput by minimizing the total GC time. You can check some crazy a** graphs and get better explanation at the official documentation <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/parallel.html#gen_arrangement_parallel">here</a>.</p>
<h4><a href="#when-to-use-1" aria-hidden="true" class="anchor" id="when-to-use-1"></a>When to Use:</h4>
<ul>
<li>Batch processing systems.</li>
<li>Applications prioritizing throughput over low latency.</li>
</ul>
<h4><a href="#example-1" aria-hidden="true" class="anchor" id="example-1"></a>Example:</h4>
<p>Imagine a financial service API that consolidates transactions into daily reports. Since the workload prioritizes throughput over latency, Parallel GC is ideal for processing large transaction sets efficiently.</p>
<h4><a href="#docker-example-1" aria-hidden="true" class="anchor" id="docker-example-1"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseParallelGC -Xmx2g -jar app.jar
</code></pre>
<hr />
<h3><a href="#3-g1-garbage-collector-g1gc" aria-hidden="true" class="anchor" id="3-g1-garbage-collector-g1gc"></a>3. G1 Garbage Collector (G1GC)</h3>
<h4><a href="#overview-2" aria-hidden="true" class="anchor" id="overview-2"></a>Overview:</h4>
<p>G1GC divides the heap into regions and collects garbage incrementally, making it a good balance between throughput and low latency.</p>
<h4><a href="#when-to-use-2" aria-hidden="true" class="anchor" id="when-to-use-2"></a>When to Use:</h4>
<ul>
<li>General-purpose applications.</li>
<li>Systems requiring predictable pause times.</li>
</ul>
<h4><a href="#example-2" aria-hidden="true" class="anchor" id="example-2"></a>Example:</h4>
<p>Any SaaS platform serving user requests in under 200ms with moderate traffic spikes.</p>
<h4><a href="#docker-example-2" aria-hidden="true" class="anchor" id="docker-example-2"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseG1GC -Xmx4g -XX:MaxGCPauseMillis=200 -jar app.jar
</code></pre>
<h4><a href="#important-considerations-about-g1gc" aria-hidden="true" class="anchor" id="important-considerations-about-g1gc"></a>Important considerations about G1GC:</h4>
<p>You might be wondering: &quot;If G1GC supports both good throughput and low latency, why not use it for every application? Sounds like a no-brainer...&quot;</p>
<p>But well, not quite. While G1GC is a fantastic general-purpose garbage collector, it’s not the universal solution for all workloads. Think of it as the &quot;jack of all trades&quot; of GCs—good at many things, but not necessarily the best at any one thing. <em>Poof!</em> Now that you’re out of the cave, let’s analyze:</p>
<ul>
<li>
<p><strong>Throughput-Focused Applications:</strong> If your application doesn’t care about pause times—for example, batch processing systems or data aggregation pipelines—why would you burden it with G1GC’s incremental collection overhead? Parallel GC is better suited here, offering raw performance without worrying about predictable pauses.</p>
</li>
<li>
<p><strong>Ultra-Low Latency Needs:</strong> If you’re building a real-time trading system or managing huge heaps (think terabytes), G1GC might struggle to meet your strict latency requirements. Collectors like ZGC or Shenandoah GC are designed specifically for these use cases, offering sub-10ms pause times.</p>
</li>
</ul>
<p>In short, G1GC is like that versatile tool in your toolbox—it works well for a variety of tasks, especially if you’re building the classic CRUD API (yes pretty much all of your messy simple Spring CRUDs). But if you’re running specialized workloads, you’ll want to pick a collector that’s optimized to your needs.</p>
<hr />
<h3><a href="#4-z-garbage-collector-zgc" aria-hidden="true" class="anchor" id="4-z-garbage-collector-zgc"></a>4. Z Garbage Collector (ZGC)</h3>
<h4><a href="#overview-3" aria-hidden="true" class="anchor" id="overview-3"></a>Overview:</h4>
<p>ZGC is designed for ultra-low-latency applications with large heaps (up to terabytes). Its pause times are typically under 10 milliseconds.</p>
<h4><a href="#when-to-use-3" aria-hidden="true" class="anchor" id="when-to-use-3"></a>When to Use:</h4>
<ul>
<li>Real-time systems.</li>
<li>Applications with very large heaps.</li>
</ul>
<h4><a href="#when-to-do-not-use" aria-hidden="true" class="anchor" id="when-to-do-not-use"></a>When to DO NOT use:</h4>
<ul>
<li>Imagine you have a batch processing system using ZGC. There is very high chance of facing inceased CPU utilization($$$) without any latency benefit. For example, a data ingestion pipeline optimized for high throughput but insensitive to pause times would waste resources managing unnecessary low-latency GC cycles.</li>
</ul>
<h4><a href="#example-3" aria-hidden="true" class="anchor" id="example-3"></a>Example:</h4>
<p>A trading system processing market data streams in real time.</p>
<h4><a href="#docker-example-3" aria-hidden="true" class="anchor" id="docker-example-3"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseZGC -Xmx16g -jar app.jar
</code></pre>
<hr />
<h3><a href="#5-shenandoah-garbage-collector" aria-hidden="true" class="anchor" id="5-shenandoah-garbage-collector"></a>5. Shenandoah Garbage Collector</h3>
<h4><a href="#overview-4" aria-hidden="true" class="anchor" id="overview-4"></a>Overview:</h4>
<p>Shenandoah GC minimizes pause times by performing concurrent compaction. It’s ideal for latency-sensitive applications.</p>
<h4><a href="#when-to-use-4" aria-hidden="true" class="anchor" id="when-to-use-4"></a>When to Use:</h4>
<ul>
<li><strong>Payment gateways</strong> with strict SLA requirements for latency.</li>
<li><strong>APIs with spiky traffic</strong> patterns, such as social media feeds or live voting systems.</li>
<li>Applications where reducing GC pause time is critical to user experience, such as gaming servers or interactive web applications.</li>
</ul>
<h4><a href="#when-to-do-not-use-1" aria-hidden="true" class="anchor" id="when-to-do-not-use-1"></a>When to DO NOT use:</h4>
<p>Using Shenandoah GC for batch processing systems or workloads optimized for high throughput over low latency (e.g., nightly data aggregation) may lead to inefficient CPU utilization. The additional overhead of concurrent compaction provides no benefits when predictable pauses are acceptable, reducing overall throughput compared to <strong>Parallel GC</strong>.</p>
<p>For exampe, a financial reconciliation batch process configured with Shenandoah might experience reduced throughput due to unnecessary focus on low pause times, delaying report generation.</p>
<h4><a href="#example-4" aria-hidden="true" class="anchor" id="example-4"></a>Example:</h4>
<p>A payment processing API handling high transaction volumes cannot afford GC-induced latency spikes during peak hours. Shenandoah’s low-pause nature ensures that transaction processing continues smoothly even under heavy load.</p>
<p>Another example is a real-time multiplayer gaming server, where latency spikes could lead to a poor player experience. Shenandoah ensures consistent frame updates and server responsiveness.</p>
<h4><a href="#docker-example-4" aria-hidden="true" class="anchor" id="docker-example-4"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseShenandoahGC -Xmx8g -XX:+UnlockExperimentalVMOptions -jar app.jar
</code></pre>
<hr />
<h2><a href="#choosing-the-right-garbage-collector" aria-hidden="true" class="anchor" id="choosing-the-right-garbage-collector"></a>Choosing the Right Garbage Collector</h2>
<p>Here you can find a cheatsheet. But remember... you should always evaluate your own workload before choosing it's garbage collector.</p>
<table>
<thead>
<tr>
<th>Garbage Collector</th>
<th>Best For</th>
<th>JVM Version Support</th>
</tr>
</thead>
<tbody>
<tr>
<td>Serial GC</td>
<td>Small, single-threaded apps</td>
<td>All versions</td>
</tr>
<tr>
<td>Parallel GC</td>
<td>High-throughput batch systems</td>
<td>All versions</td>
</tr>
<tr>
<td>G1GC</td>
<td>General-purpose apps</td>
<td>Java 9+</td>
</tr>
<tr>
<td>ZGC</td>
<td>Real-time, large heap apps</td>
<td>Java 11+</td>
</tr>
<tr>
<td>Shenandoah GC</td>
<td>Low-latency apps</td>
<td>Java 11+</td>
</tr>
</tbody>
</table>
<hr />
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>Choosing the right garbage collector for your application requires some knowledge over the tools I discussed in this post. But once you learn about it, you may have the power of taking decisions, and this is extremely valuable in Software Engineering field, also, by selecting the right GC you can significantly improve performance, stability and save some costs for your future applications based on JVM. Don’t let GC be a black box—embrace it, tune it, and let it work for you.</p>
<hr />
<h2><a href="#training-real-world-scenarios-and-solutions" aria-hidden="true" class="anchor" id="training-real-world-scenarios-and-solutions"></a>Training: Real-World Scenarios and Solutions</h2>
<h3><a href="#scenario-1-payment-gateway-latency" aria-hidden="true" class="anchor" id="scenario-1-payment-gateway-latency"></a>Scenario 1: Payment Gateway Latency</h3>
<p>You are building a payment gateway API that must process transactions in real-time with strict SLA requirements. The workload is spiky, with heavy traffic during sales events or specific times of the day. Which garbage collector would you choose to ensure low latency?</p>
<h3><a href="#scenario-2-batch-data-processing-system" aria-hidden="true" class="anchor" id="scenario-2-batch-data-processing-system"></a>Scenario 2: Batch Data Processing System</h3>
<p>Your application processes daily financial reconciliation batches, which involve large amounts of data. Latency is not a concern, but throughput must be maximized to complete processing as fast as possible. Which garbage collector fits this use case?</p>
<h3><a href="#scenario-3-real-time-multiplayer-game" aria-hidden="true" class="anchor" id="scenario-3-real-time-multiplayer-game"></a>Scenario 3: Real-Time Multiplayer Game</h3>
<p>You are designing a server for a real-time multiplayer game. The server must manage thousands of players, each generating events continuously. Latency spikes during garbage collection are unacceptable as they could lead to lag and a poor user experience. What GC configuration would you use?</p>
<hr />
<h2><a href="#solutions" aria-hidden="true" class="anchor" id="solutions"></a>Solutions</h2>
<h3><a href="#solution-1-payment-gateway-latency" aria-hidden="true" class="anchor" id="solution-1-payment-gateway-latency"></a>Solution 1: Payment Gateway Latency</h3>
<p>Use <strong>Shenandoah GC</strong> to ensure low latency and consistent response times. Its concurrent compaction minimizes pause times, making it ideal for latency-sensitive workloads.</p>
<h3><a href="#solution-2-batch-data-processing-system" aria-hidden="true" class="anchor" id="solution-2-batch-data-processing-system"></a>Solution 2: Batch Data Processing System</h3>
<p>Use <strong>Parallel GC</strong> to maximize throughput. Since latency isn’t a concern, the Parallel GC’s focus on high efficiency during garbage collection fits this workload.</p>
<h3><a href="#solution-3-real-time-multiplayer-game" aria-hidden="true" class="anchor" id="solution-3-real-time-multiplayer-game"></a>Solution 3: Real-Time Multiplayer Game</h3>
<p>Use <strong>ZGC</strong> to achieve ultra-low latency and scale with large heaps. It ensures that garbage collection does not interfere with real-time gameplay.</p>
<h3><a href="#references" aria-hidden="true" class="anchor" id="references"></a>References:</h3>
<ol>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/">Java Garbage Collection Basics - Oracle</a></li>
</ol>
<hr />
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item></channel></rss>