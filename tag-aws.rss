<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pedro Lopes</title><link></link><description>Comput. Sci.</description><pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate><lastBuildDate>Fri, 07 Mar 2025 13:26:49 GMT</lastBuildDate><generator>marmite</generator><item><title>Understanding and Mitigating AWS Lambda Throttling in High-Concurrency Workloads</title><link>/lambda-throttling.html</link><author>pedrohbl_</author><category>aws</category><category>lambda</category><category>throttling</category><guid>/lambda-throttling.html</guid><pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate><source url="">tag-aws</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>When dealing with high-concurrency workloads, scaling AWS Lambda effectively while avoiding throttling can become a challenge. This post explores a real-world scenario where an application(just like a worker), written in Kotlin, processed over 1,000,000 records in a blob located in S3 using a custom asynchronous iteration method. Each record triggered an asynchronous Lambda invocation that interacted with DynamoDB. However, the setup led to 429 Too Many Requests errors occurring consistently during peak loads exceeding 10,000 TPS, indicating throttling issues with AWS Lambda.
The article will:</p>
<ol>
<li>
<p><strong>Outline the problem</strong> faced while processing high-concurrency workloads.</p>
</li>
<li>
<p><strong>Explain AWS Lambda throttling mechanisms</strong>, based on the <a href="https://aws.amazon.com/blogs/compute/understanding-aws-lambdas-invoke-throttle-limits/">AWS Compute Blog article by James Beswick</a>.</p>
</li>
<li>
<p><strong>Discuss solutions</strong> to mitigate throttling.</p>
</li>
<li>
<p><strong>TBD</strong> Maybe in the future I'll Provide a real-world proof of concept <strong>(POC)</strong> to evaluate each mitigation technique.</p>
</li>
</ol>
<hr />
<h2><a href="#use-case" aria-hidden="true" class="anchor" id="use-case"></a>Use Case</h2>
<p>To better illustrate the challenges and solutions, consider the following use case:</p>
<ul>
<li><strong>Dataset:</strong> The workload involves processing a large file with 1 million records stored in an S3 bucket.</li>
<li><strong>Data Characteristics:</strong> Each record contains 8 columns of strings, primarily UUIDs (36 bytes each). This results in approximately 288 bytes per record.</li>
<li><strong>Worker Configuration:</strong> The application is deployed on a SINGLE node with the following specifications:
<ul>
<li><strong>vCPUs:</strong> 4</li>
<li><strong>RAM:</strong> 8 GB</li>
</ul>
</li>
</ul>
<h3><a href="#resource-calculations" aria-hidden="true" class="anchor" id="resource-calculations"></a>Resource Calculations</h3>
<ol>
<li>
<p><strong>Memory Requirements:</strong></p>
<ul>
<li>Each record occupies 288 bytes.</li>
<li>For 100 concurrent coroutines:
<ul>
<li>( 288 * 100 = 28,800 bytes approx 28.8KB )</li>
</ul>
</li>
<li>Adding a 20 KB overhead per coroutine for runtime management:
<ul>
<li>( 100 * 20KB = 2,000KB approx 2MB )</li>
</ul>
</li>
<li>Total memory consumption:
<ul>
<li>( 28.8KB + 2,000KB = 2.028MB )</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CPU Considerations:</strong></p>
<ul>
<li>Let's assume each vCPU can handle approximately 100-150 threads (or coroutines) effectively, actually it could handle much more depending on workload. But we can safely assume this number of threads as a safe margin for the given setup, based on <a href="https://github.com/mmoraesbr/kotlin-coroutine-benchmark">Kotlin async coroutines benchmark</a>.</li>
<li>For this use case, 4 vCPUs are sufficient to manage 100 concurrent coroutines with minimal contention.</li>
</ul>
</li>
</ol>
<p>This setup ensures that the system remains stable while processing a high volume of records efficiently.</p>
<h2><a href="#the-challenge" aria-hidden="true" class="anchor" id="the-challenge"></a>The Challenge</h2>
<h3><a href="#problem-context" aria-hidden="true" class="anchor" id="problem-context"></a>Problem Context</h3>
<p>A workload involving processing a large file of over 1,000,000 records can utilize concurrency in Kotlin to invoke AWS Lambda for each record. The Lambda function in this case performed a putItem operation on DynamoDB.</p>
<p>Here’s an example of the Kotlin code for mapAsync:</p>
<pre><code class="language-kotlin">suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync(
    transformation: suspend (T) -&gt; R
): List&lt;R&gt; = coroutineScope {
    this@mapAsync
        .map { async { transformation(it) } }
        .awaitAll()
}

suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync(
    concurrency: Int,
    transformation: suspend (T) -&gt; R
): List&lt;R&gt; = coroutineScope {
    val semaphore = Semaphore(concurrency)
    this@mapAsync
        .map { async { semaphore.withPermit { transformation(it) } } }
        .awaitAll()
}
</code></pre>
<p>This method processes records significantly faster than a standard for loop, but it can flood the system with Lambda invocations, triggering throttling. The 429 Too Many Requests errors can be attributed to:</p>
<ol>
<li><strong>Concurrency Limits</strong>: AWS imposes a limit on the number of concurrent executions per account.</li>
<li><strong>TPS (Transactions Per Second) Limits</strong>: High TPS can overwhelm the Invoke Data Plane.</li>
<li><strong>Burst Limits</strong>: Limits the rate at which concurrency can scale, governed by the token bucket algorithm.</li>
</ol>
<h3><a href="#observed-errors" aria-hidden="true" class="anchor" id="observed-errors"></a>Observed Errors</h3>
<ul>
<li><strong>429 Too Many Requests</strong>: Errors indicate that the Lambda invocations exceeded allowed concurrency or burst limits.</li>
<li><strong>DynamoDB “Provisioned Throughput Exceeded”</strong>: Errors occurred during spikes in DynamoDB writes. But this error won't be covered in this post, maybe in the future I can discuss strategies to work directly with dynamodb IO optimization, for now let's just ignore this one.</li>
</ul>
<hr />
<h2><a href="#aws-lambda-throttling-mechanisms" aria-hidden="true" class="anchor" id="aws-lambda-throttling-mechanisms"></a>AWS Lambda Throttling Mechanisms</h2>
<p>AWS enforces three key throttle limits to protect its infrastructure and ensure fair resource distribution:</p>
<h3><a href="#1-concurrency-limits" aria-hidden="true" class="anchor" id="1-concurrency-limits"></a>1. <strong>Concurrency Limits</strong></h3>
<p>Concurrency limits determine the number of in-flight Lambda executions allowed at a time. For example, with a concurrency limit of 1,000, up to 1,000 Lambda functions can execute simultaneously across all Lambdas in the account and region.</p>
<h3><a href="#2-tps-limits" aria-hidden="true" class="anchor" id="2-tps-limits"></a>2. <strong>TPS Limits</strong></h3>
<p>TPS is derived from concurrency and function duration. For instance:</p>
<ul>
<li>Function duration: 100 ms (equivalent to 100ms =100 × 10<sup>-3</sup> = 0.1s)</li>
<li>Concurrency: 1,000</li>
</ul>
<pre><code class="language-html">TPS = Concurrency / Function Duration = 10,000 TPS
</code></pre>
<p>If the function duration drops below 100 ms, TPS is capped at 10x the concurrency.</p>
<h3><a href="#3-burst-limits" aria-hidden="true" class="anchor" id="3-burst-limits"></a>3. <strong>Burst Limits</strong></h3>
<p>The burst limit ensures gradual scaling of concurrency, avoiding large spikes in cold starts. AWS uses the token bucket algorithm to enforce this:</p>
<ul>
<li>Each invocation consumes a token.</li>
<li>Tokens refill at a fixed rate (e.g., 500 tokens per minute).</li>
<li>The bucket has a maximum capacity (e.g., 1,000 tokens).</li>
</ul>
<p>For more details, refer to the <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html">AWS Lambda Burst Limits</a>.</p>
<hr />
<h2><a href="#mitigation-strategies" aria-hidden="true" class="anchor" id="mitigation-strategies"></a>Mitigation Strategies</h2>
<p>That being said, several approaches can be employed to mitigate the throttling scenarios observed in this case. These techniques aim to address the specific constraints and challenges imposed by the problem:</p>
<h3><a href="#1-limit-concurrency-using-semaphore" aria-hidden="true" class="anchor" id="1-limit-concurrency-using-semaphore"></a>1. <strong>Limit Concurrency Using Semaphore</strong></h3>
<p>Concurrency in Kotlin can be limited using the mapAsync function with a specified concurrency level:</p>
<pre><code class="language-kotlin">val results = records.mapAsync(concurrency = 100) { record -&gt;
    invokeLambda(record)
}
</code></pre>
<p>This implementation leverages coroutines in Kotlin to handle asynchronous operations efficiently. We don't want to deep dive here in how coroutines work, but think of it as a tool that allow lightweight threads to run without blocking, making it possible to manage multiple tasks concurrently without overwhelming system resources.</p>
<p>In the use case described, where the workload involves processing millions of records within 100 concurrent coroutines, the concurrency level of 100 was chosen as a reasonable limit. This decision balances the capacity of the node, configured with 4 vCPUs and 8 GB of RAM, against the resource requirements of each coroutine. For example, each coroutine processes records with a memory overhead of approximately 28.8 KB per record, plus 20 KB for runtime management. This setup ensures stability while maximizing throughput within the system’s constraints.</p>
<p>By introducing a Semaphore, the number of concurrent tasks can be restricted to this specified level. This prevents overloading the Lambda concurrency limits and reduces the risk of 429 Too Many Requests errors, ensuring that the system remains stable and performs reliably.</p>
<h4><a href="#estimated-time-to-process" aria-hidden="true" class="anchor" id="estimated-time-to-process"></a>Estimated Time to Process</h4>
<p>Using the following parameters:</p>
<ul>
<li><code>T</code>: Execution time for a single Lambda invocation.</li>
<li><code>n</code>: Number of concurrent Lambda invocations.</li>
<li><code>Total Records</code>: Total number of records to process.</li>
</ul>
<p>The total processing time can be calculated as:</p>
<pre><code class="language-html">Total Time = (Total Records / n) * T
</code></pre>
<h4><a href="#example-with-t--100-ms" aria-hidden="true" class="anchor" id="example-with-t--100-ms"></a>Example with <code>T = 100 ms</code></h4>
<p>Given:</p>
<ul>
<li><code>Total Records = 1,000,000</code></li>
<li><code>n = 100</code></li>
<li><code>T = 100 ms</code></li>
</ul>
<p>Substituting into the formula:</p>
<pre><code class="language-html">Total Time = (1,000,000 / 100) * 100 ms
</code></pre>
<p>Simplifying:</p>
<pre><code class="language-html">Total Time = 10,000 * 100 ms = 1,000,000 ms
</code></pre>
<p>Converting to seconds and minutes:</p>
<pre><code class="language-html">Total Time = 1,000,000 ms = 1,000 seconds = 16.67 minutes
</code></pre>
<h4><a href="#key-advantages" aria-hidden="true" class="anchor" id="key-advantages"></a>Key Advantages:</h4>
<ul>
<li><strong>Simple Implementation:</strong> Adding a Semaphore to the mapAsync function involves minimal changes to the codebase.</li>
<li><strong>Effective Throttling Control:</strong> The implementation ensures that the number of concurrent Lambda invocations does not exceed the predefined limit, maintaining system stability.</li>
</ul>
<h4><a href="#trade-offs" aria-hidden="true" class="anchor" id="trade-offs"></a>Trade-offs:</h4>
<ul>
<li><strong>Increased Processing Time:</strong> While throttling prevents errors, it may result in longer overall processing times due to the limitation on simultaneous executions.</li>
<li><strong>No Guarantee:</strong> While this approach prevents the majority of 429 Too Many Requests errors, it does not guarantee that such errors will not occur again. This is because, even when the number of concurrent Lambdas in execution is controlled, the system might still exceed burst limits, which are governed by the token bucket algorithm.</li>
<li><strong>Difficult to Manage in Distributed Systems:</strong> This approach is more practical in scenarios with a single node running the application. In distributed systems with multiple nodes running the same application (e.g., 10 instances), it becomes challenging to coordinate a distributed TPS control mechanism. Each node would need to communicate and share state to ensure the total TPS remains within AWS limits, which significantly increases complexity.</li>
</ul>
<h3><a href="#2-retry-with-exponential-backoff" aria-hidden="true" class="anchor" id="2-retry-with-exponential-backoff"></a>2. <strong>Retry with Exponential Backoff</strong></h3>
<p>Retries with exponential backoff handle throttled requests effectively by spreading out retry attempts over time. This reduces the chance of overwhelming the system further when transient issues or throttling limits occur. The exponential backoff algorithm increases the delay between retries after each failed attempt, making it particularly useful in high-concurrency systems and also in services/calls that might fail at times.</p>
<h4><a href="#how-it-works" aria-hidden="true" class="anchor" id="how-it-works"></a>How It Works:</h4>
<p>The implementation retries an AWS Lambda invocation up to a specified number of attempts, introducing exponentially increasing delays between retries. For example:</p>
<pre><code class="language-kotlin">suspend fun invokeWithRetry(record: Record, retries: Int = 3) {
    var attempts = 0
    while (attempts &lt; retries) {
        try {
            invokeLambda(record)
            break
        } catch (e: Exception) {
            if (++attempts == retries) throw e
            delay((2.0.pow(attempts) * 100).toLong())
        }
    }
}
</code></pre>
<h4><a href="#estimated-time-to-process-1" aria-hidden="true" class="anchor" id="estimated-time-to-process-1"></a>Estimated Time to Process</h4>
<p>Assume:</p>
<ul>
<li>Each retry introduces a delay that doubles after every attempt.</li>
<li><code>D</code>: Cumulative delay for retries.</li>
<li><code>r</code>: Number of retry attempts per record.</li>
</ul>
<p>Cumulative delay is given by:</p>
<pre><code class="language-html">D = Σ (2^i * T_retry) for i = 1 to r
</code></pre>
<p>Where:</p>
<ul>
<li><code>T_retry</code> = Base retry delay (e.g., 100 ms).</li>
</ul>
<p>Example with <code>T_retry = 100 ms</code> and <code>r = 3</code>:</p>
<pre><code class="language-html">D = (2^1 * 100 ms) + (2^2 * 100 ms) + (2^3 * 100 ms)
D = 200 ms + 400 ms + 800 ms = 1,400 ms
</code></pre>
<p>If 10% of records require retries, the retry time is:</p>
<pre><code class="language-html">Retry Time = (Total Records * 10%) * D / n
Retry Time = (1,000,000 * 0.1) * 1,400 ms / 100
Retry Time = 1,400,000 ms = 1,400 seconds = 23.33 minutes
</code></pre>
<p>Adding this to the initial processing time:</p>
<pre><code class="language-html">Total Time = Initial Time + Retry Time
Total Time = 16.67 minutes + 23.33 minutes = 40 minutes
</code></pre>
<hr />
<h4><a href="#pros" aria-hidden="true" class="anchor" id="pros"></a>Pros:</h4>
<ul>
<li><strong>Handles transient errors gracefully:</strong> Retries ensure that temporary issues, such as short-lived throttling or network disruptions, do not result in failed processing.</li>
<li><strong>Distributed systems friendly:</strong> Can be independently implemented in each node, avoiding the need for centralized control mechanisms.</li>
<li><strong>Reduces system load during failures:</strong> The increasing delay between retries prevents the system from being overwhelmed.</li>
</ul>
<h4><a href="#cons" aria-hidden="true" class="anchor" id="cons"></a>Cons:</h4>
<ul>
<li><strong>Adds latency:</strong> The exponential backoff mechanism inherently increases the time taken to complete processing, can take even BIGGER times when considering worst case scenarios(potentially 10x more the total time discussed).</li>
<li><strong>Increases code complexity and testability:</strong> Requires additional logic to manage retries and delays and testing those scenarios when only part of the requests fail.</li>
</ul>
<h3><a href="#3-use-sqs-for-decoupling" aria-hidden="true" class="anchor" id="3-use-sqs-for-decoupling"></a>3. <strong>Use SQS for Decoupling</strong></h3>
<p>Amazon Simple Queue Service (SQS) can act as a buffer between producers (e.g., the application processing records) and consumers (e.g., AWS Lambda), enabling controlled, asynchronous processing of requests. This approach decouples the producer and consumer, ensuring the workload is processed at a rate the system can handle.</p>
<h4><a href="#how-it-works-1" aria-hidden="true" class="anchor" id="how-it-works-1"></a>How It Works:</h4>
<ol>
<li>The application writes each record to an SQS queue instead of invoking AWS Lambda directly.</li>
<li>AWS Lambda is configured to process messages from the queue at a controlled rate, dictated by the batch size and concurrency settings.</li>
<li>This ensures that the rate of Lambda invocations remains within the account's concurrency and TPS limits.</li>
</ol>
<h4><a href="#additional-pattern-aws-serverless-land-example" aria-hidden="true" class="anchor" id="additional-pattern-aws-serverless-land-example"></a>Additional Pattern: AWS Serverless Land Example</h4>
<p>This approach aligns with a pattern presented on <a href="https://serverlessland.com/patterns/sqs-lambda-ddb-sam-java">AWS Serverless Land</a>: <strong>Create a Lambda function that batch writes to DynamoDB from SQS</strong>. This pattern deploys an SQS queue, a Lambda Function, and a DynamoDB table, allowing batch writes from SQS messages to DynamoDB. It demonstrates how to leverage a batch processing mechanism to handle high-throughput scenarios effectively.</p>
<p>The provided SAM template uses Java 11, SQS, Lambda, and DynamoDB to create a cost-effective, serverless architecture:</p>
<pre><code class="language-yaml">AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31
Description: sqs-lambda-dynamodb

Globals:
  Function:
    Runtime: java11
    MemorySize: 512
    Timeout: 25

Resources:
  OrderConsumer:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: OrderConsumer
      Handler: com.example.OrderConsumer::handleRequest
      CodeUri: target/sourceCode.zip
      Environment:
        Variables:
          QUEUE_URL: !Sub 'https://sqs.${AWS::Region}.amazonaws.com/${AWS::AccountId}/OrdersQueue'
          REGION: !Sub '${AWS::Region}'
          TABLE_NAME: !Ref OrdersTable
      Policies:
        - AWSLambdaSQSQueueExecutionRole
        - AmazonDynamoDBFullAccess

  OrdersQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: OrdersQueue

  OrdersTable:
    Type: 'AWS::DynamoDB::Table'
    Properties:
      TableName: OrdersTable
      AttributeDefinitions:
        - AttributeName: orderId
          AttributeType: S
      KeySchema:
        - AttributeName: orderId
          KeyType: HASH
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5
</code></pre>
<h4><a href="#estimated-time-to-process-2" aria-hidden="true" class="anchor" id="estimated-time-to-process-2"></a>Estimated Time to Process</h4>
<p>Assume:</p>
<ul>
<li><code>T_batch</code>: Execution time for processing a batch.</li>
<li><code>k</code>: Overhead due to batching.</li>
<li><code>b</code>: Number of messages per batch.</li>
<li><code>n</code>: Lambda concurrency.</li>
</ul>
<p>The total processing time is:</p>
<pre><code class="language-html">Total Time = (Total Records / (b * n)) * (T + k)
</code></pre>
<p>Example with:</p>
<ul>
<li><code>T = 100 ms</code></li>
<li><code>k = 20 ms</code></li>
<li><code>b = 10</code></li>
<li><code>n = 100</code></li>
<li><code>Total Records = 1,000,000</code></li>
</ul>
<p>Substitute into the formula:</p>
<pre><code class="language-html">Total Time = (1,000,000 / (10 * 100)) * (100 ms + 20 ms)
Total Time = (1,000,000 / 1,000) * 120 ms
Total Time = 1,000 * 120 ms = 120,000 ms
</code></pre>
<p>Convert to seconds and minutes:</p>
<pre><code class="language-html">Total Time = 120,000 ms = 120 seconds = 2 minutes
</code></pre>
<h4><a href="#the-importance-of-fifo-queues" aria-hidden="true" class="anchor" id="the-importance-of-fifo-queues"></a>The Importance of FIFO Queues</h4>
<p>To maintain consistency in DynamoDB, it is essential to configure the SQS queue as FIFO (First-In, First-Out) in this case. This ensures that messages are processed in the exact order they are received, which is critical in systems where the order of operations affects the final state of the database. For example:</p>
<ol>
<li>
<p><strong>Out-of-Order Processing Issues:</strong> If two updates to the same DynamoDB record are processed out of order (e.g., Update1 followed by Update2), but Update2 depends on Update1, the database could end up in an inconsistent state. FIFO queues prevent this by enforcing strict order. For our case, there was not duplicated entries on the file so FIFO was not in considerated despite being absolutely important for this usecase.</p>
</li>
<li>
<p><strong>Idempotency Challenges:</strong> Even when Lambda functions are designed to be idempotent, out-of-order processing can lead to unexpected behavior if operations rely on sequential execution. For instance, appending logs or incrementing counters requires a guarantee of order.</p>
</li>
<li>
<p><strong>Trade-offs with FIFO:</strong> While FIFO queues provide consistency, they come with some limitations:</p>
<ul>
<li><strong>Lower Throughput:</strong> FIFO queues have a maximum throughput of 300 transactions per second with batching (or 3,000 if using high-throughput mode).</li>
<li><strong>Increased Latency:</strong> Enforcing order may introduce slight delays in message processing.</li>
</ul>
</li>
</ol>
<hr />
<h4><a href="#pros-1" aria-hidden="true" class="anchor" id="pros-1"></a>Pros:</h4>
<ul>
<li><strong>Decouples producers and consumers:</strong> The producer can continue adding messages to the queue regardless of the Lambda processing speed.</li>
<li><strong>Prevents throttling:</strong> SQS regulates the rate at which messages are delivered to Lambda, avoiding sudden spikes that could exceed AWS limits.</li>
<li><strong>Distributed systems friendly:</strong> Works seamlessly in multi-node systems, as all nodes write to the same queue without requiring coordination.</li>
</ul>
<h4><a href="#cons-1" aria-hidden="true" class="anchor" id="cons-1"></a>Cons:</h4>
<ul>
<li><strong>Adds architectural complexity:</strong> Introducing SQS requires additional components and configuration.</li>
<li><strong>Adds code complexity:</strong> Introduce code complexity to the insertion lambda, so its responsible for managing sqs batch write operations, reading on SQS source and also being able to operate by asynchronous invocation for legacy systems.</li>
<li><strong>Introduces latency:</strong> Messages may wait in the queue before being processed, depending on the Lambda polling rate and queue depth. For example, a queue depth of 10,000 messages and a polling rate of 1,000 messages per second would result in a processing delay.</li>
</ul>
<hr />
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>AWS Lambda throttling issues, particularly for high-concurrency workloads, can be effectively managed using a combination of strategies such as concurrency control, retry mechanisms, and decoupling with SQS. Each of these approaches has its strengths and trade-offs:</p>
<ul>
<li>
<p><strong>Limit Concurrency Using Semaphore</strong>: A straightforward solution for single-node setups, providing reliable throttling control at the cost of slightly increased processing time. However, it requires additional considerations for distributed systems.</p>
</li>
<li>
<p><strong>Retry with Exponential Backoff</strong>: A robust technique for handling transient failures, distributing load over time and avoiding unnecessary retries. Yet, it can add significant latency in worst-case scenarios and increase implementation complexity.</p>
</li>
<li>
<p><strong>Use SQS for Decoupling</strong>: The most scalable and efficient approach when <code>T_batch = T + k</code>, with <code>k</code> being sufficiently small. While it introduces latency and complexity, its benefits make it the go-to solution for large-scale systems.</p>
</li>
</ul>
<p>As an ending insight, we can assure that for small workloads, async invocation can provide faster results, as it avoids the latency of queuing and batch processing. However, as the number of requests increases, direct invocation becomes inefficient and computationally expensive due to the high TPS demand and risk of breaching AWS limits. In contrast, decoupled architectures using SQS and batch processing scale more efficiently, ensuring stability and cost-effectiveness under heavy loads.</p>
<h3><a href="#next-steps-implementing-a-poc" aria-hidden="true" class="anchor" id="next-steps-implementing-a-poc"></a>Next Steps: Implementing a POC</h3>
<p>While this post has focused on explaining the challenges, strategies, and theoretical calculations for mitigation, an actual Proof of Concept (POC) would be very cool to validate and visualize these solutions in practice. A future post might explore how to design and execute a POC to measure the overall performance in a real-world scenario.</p>
<p>For more details on Lambda throttling, refer to the <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html">AWS Lambda Developer Guide</a> and the <a href="https://aws.amazon.com/blogs/compute/understanding-aws-lambdas-invoke-throttle-limits/">AWS Compute Blog</a>.</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item></channel></rss>