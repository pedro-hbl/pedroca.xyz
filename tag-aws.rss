<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pedro Lopes</title><link></link><description>Comput. Sci.</description><pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate><lastBuildDate>Wed, 09 Apr 2025 17:04:22 GMT</lastBuildDate><generator>marmite</generator><item><title>AWS Lambda: Cold Starts, Infrastructure, and Enterprise Applications</title><link>/understanding-lambdas.html</link><author>pedrohbl_</author><category>aws</category><category>lambda</category><category>serverless</category><category>infrastructure</category><category>cold-starts</category><guid>/understanding-lambdas.html</guid><pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate><source url="">tag-aws</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<blockquote>
<p><strong><em>NOTE:</em></strong> This post is still under construction come back later please</p>
</blockquote>
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>When AWS introduced Lambda in 2014, it fundamentally changed how developers think about building and deploying applications. The serverless paradigm promised freedom from infrastructure management, automatic scaling, and pay-per-use pricing. But beneath this simple abstraction lies a complex, fascinating infrastructure that impacts application performance, reliability, and cost-effectiveness.</p>
<p>In this post, I'll take you deep into the world of AWS Lambda, focusing on three critical aspects that aren't widely understood:</p>
<ol>
<li><strong>Cold starts and AWS's hidden warming mechanisms</strong></li>
<li><strong>The underlying infrastructure that powers Lambda functions</strong></li>
<li><strong>Real-world enterprise use cases and why infrastructure knowledge matters</strong></li>
</ol>
<p>Understanding these elements will help you make better architectural decisions, optimize performance, and build more reliable serverless applications. Let's dive in.</p>
<hr />
<h2><a href="#cold-starts-awss-secret-warming-mechanism" aria-hidden="true" class="anchor" id="cold-starts-awss-secret-warming-mechanism"></a>Cold Starts: AWS's Secret Warming Mechanism</h2>
<p>Cold starts remain one of the most discussed challenges of serverless computing. When a Lambda function is invoked after being idle or for the first time, AWS must initialize an execution environment, load your code, and start the runtime. This initialization process adds latency—anywhere from 100ms to several seconds—before your function can begin processing the request.</p>
<h3><a href="#the-traditional-cold-start-model" aria-hidden="true" class="anchor" id="the-traditional-cold-start-model"></a>The Traditional Cold Start Model</h3>
<p>The standard understanding of Lambda cold starts looks like this:</p>
<p><figure><img src="/media/lambda/lambda_execution_life.png" alt="Lambda Cold Start Traditional Model" /></figure></p>
<p>This diagram from AWS documentation shows the initialization phase, which includes:</p>
<ol>
<li>Downloading your function code</li>
<li>Starting the execution environment</li>
<li>Running any initialization code outside the handler</li>
<li>Finally executing your handler code</li>
</ol>
<p>According to AWS, &quot;cold starts typically occur in under 1% of invocations&quot; in production workloads. However, this seemingly small percentage can significantly impact real user experience, especially in latency-sensitive applications. Just imagine being Netflix with 220+ million subscribers - that 1% would mean +/- 2 million people experiencing delays... I'm sure your shareholders would be totally fine with that .-.</p>
<h3><a href="#proactive-initialization-the-hidden-mechanism" aria-hidden="true" class="anchor" id="proactive-initialization-the-hidden-mechanism"></a>Proactive Initialization: The Hidden Mechanism</h3>
<p>In 2023, AWS updated their documentation with the following statement:</p>
<blockquote>
<p>&quot;For functions using unreserved (on-demand) concurrency, Lambda may proactively initialize a function instance, even if there's no invocation.&quot;</p>
</blockquote>
<p>This statement, documented by AWS in response to research by AWS Serverless Hero AJ Stuyvenberg (<a href="https://aaronstuyvenberg.com/">Stuyvenberg website</a> I really recommend his lectures btw), revealed that Lambda performs proactive warming of functions—without developers having to implement workarounds or pay for Provisioned Concurrency. Stuyvenberg's research showed that between 50-85% of Lambda initializations can be proactive rather than true cold starts.</p>
<p>Here's what happens during proactive initialization:</p>
<p><figure><img src="https://i.ibb.co/K6hkvBC/proactive-init.png" alt="Proactive Initialization" /></figure></p>
<!-- The function is initialized without a pending invocation, and remains warm and ready to process requests. When a request arrives, it's processed immediately without the cold start delay—similar to Provisioned Concurrency, but without the additional cost.

### Detecting Proactive Initialization

You can detect when AWS has proactively initialized your function by measuring the time between initialization and the first invocation. If this gap exceeds 10 seconds (Lambda's initialization timeout), you're experiencing a proactive initialization: -->
<!-- ```javascript
const coldStartSystemTime = new Date()
let functionDidColdStart = true

exports.handler = async (event, context) => {
  if (functionDidColdStart) {
    const handlerWrappedTime = new Date()
    const proactiveInitialization = handlerWrappedTime - coldStartSystemTime > 10000 ? 1 : 0
    console.log({proactiveInitialization})
    functionDidColdStart = false
  }
  // Function logic here
}
```

### Why Does This Matter?

Understanding proactive initialization changes how we think about Lambda performance and cold start mitigation:

1. **Cost optimization**: You might not need to pay for Provisioned Concurrency in all scenarios
2. **Performance expectations**: Your functions might perform better than expected if AWS is proactively warming them
3. **Architecture decisions**: Certain patterns become more viable when cold starts are less frequent

Research published by Ran Isenberg in ["AWS Lambda Cold Starts Explained"](https://www.ranthebuilder.cloud/post/is-aws-lambda-cold-start-still-an-issue-in-2024) highlights the situations where cold starts truly matter:

- **Critical customer-facing flows**: When performance is essential, even for 1% of customers
- **Erratic traffic patterns**: Unpredictable traffic can lead to more cold starts than average
- **Chained cold starts**: When microservices call each other, cold starts can compound (shown below)

![Chained Cold Starts](https://static.wixstatic.com/media/9d29a8_eade38a3b5c04172bcb70e5f122d4aa1~mv2.png/v1/fill/w_740,h_496,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/9d29a8_eade38a3b5c04172bcb70e5f122d4aa1~mv2.png)

This diagram from Isenberg's research shows how cold starts can cascade across services, creating a compound delay for end users. If three microservices each experience a 1-second cold start, the user might experience a 3-second delay.

---

## The Infrastructure Behind Lambda: Firecracker, MicroVMs, and Worker Hosts

To truly understand Lambda's behavior, we need to examine its underlying infrastructure. Lambda functions don't run in isolation—they're part of a sophisticated distributed system that balances performance, security, and resource efficiency.

### Firecracker MicroVMs: The Foundation

At the core of Lambda is Firecracker, a virtualization technology developed by AWS and open-sourced in 2018. Written in Rust for security and performance, Firecracker creates lightweight micro virtual machines (microVMs) that:

1. Start in as little as 125ms
2. Consume only about 5 MiB of memory per microVM
3. Provide strong security isolation through multiple protection mechanisms

![Firecracker Architecture](https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/11/26/firecracker-architecture.png)

Firecracker's minimalist approach is key to Lambda's performance. It implements a stripped-down device model with only essential virtualized devices: a network device, block I/O, timer, serial console, and partial keyboard—minimizing the attack surface.

### Worker Hosts and Execution Environments

Lambda functions run on a fleet of EC2 instances called "worker hosts"—most commonly using Spot instances for cost efficiency. Each worker host runs multiple microVMs, with each microVM hosting a single execution environment.

As outlined in Joud Awad's ["AWS Lambda Architecture Deep Dive"](https://medium.com/@joudwawad/aws-lambda-architecture-deep-dive-bef856b9b2c4), the structure looks like this:

![Worker and MicroVM Structure](https://miro.medium.com/v2/resize:fit:828/format:webp/1*Kz2fQAXdBUrgVw-33hB3Lg.png)

Each execution environment is unique to a specific function version and can only process one invocation at a time. After processing, the environment remains "warm" for reuse, typically for hours but with a maximum lifetime of 14 hours.

### The Invoke Data Plane

When you trigger a Lambda function, your request goes through Lambda's Invoke Data Plane—a series of web services that:

1. Select or create an execution environment for your function
2. Route your request to that environment
3. Enforce throttle limits to protect the service

This architecture explains why Lambda has multiple throttle limits that we'll explore later.

### Function Placement and Scaling

Behind the scenes, Lambda uses a component called the Placement Service to determine where to run your function:

![Lambda Placement Service](https://miro.medium.com/v2/resize:fit:828/format:webp/1*m2K7KWQXAVDt0XYNLvF05A.png)

The Placement Service is responsible for efficiently distributing Lambda functions across worker hosts, balancing factors like:

- Proximity to data sources and consumers
- Current resource utilization
- Function resource requirements
- Availability zone health

This service is also what enables proactive initialization—it can predict demand and pre-warm execution environments before actual invocations arrive.

### Why Does This Matter?

Understanding Lambda's infrastructure helps explain many of its behaviors and constraints:

1. **Cold start patterns**: The microVM architecture explains the latency profile of cold starts
2. **Concurrency and burst limits**: The worker host model explains why Lambda throttles rapid scaling
3. **Memory/performance correlation**: Since Lambda allocates CPU proportional to memory, the underlying VM architecture explains why more memory equals better performance
4. **Execution time limits**: The 15-minute maximum execution time relates to the efficient utilization of the worker fleet

By understanding the infrastructure, we can make more informed decisions about function configuration, deployment practices, and architectural patterns.

---

## Real-World Enterprise Applications: Why Infrastructure Knowledge Matters

Understanding Lambda's cold starts and infrastructure isn't just academic—it has direct implications for real-world applications. Let's examine several enterprise use cases and why deep infrastructure knowledge makes a difference.

### Financial Services: Real-Time Decision Making

**Case Study: Financial Engines**

Financial Engines, the largest independent investment advisor in the US by assets under management, moved their core platform to Lambda. According to AWS case studies, they achieved:

- Request rates of up to 60,000 per minute
- Zero downtime
- Significant cost savings

**Why Infrastructure Knowledge Matters:**

For financial services companies, understanding cold starts is critical for real-time decision making. When milliseconds can impact trading decisions or customer experiences, knowing how to minimize and predict cold starts becomes essential.

Financial services companies can use this knowledge to:

1. **Strategically deploy Provisioned Concurrency**: Only paying for it on critical customer-facing flows
2. **Monitor proactive initialization patterns**: Using metrics to understand when AWS is warming functions
3. **Architect for minimal cold start impact**: Placing latency-sensitive operations in separate functions

### Media Streaming: High-Volume Processing

**Case Study: Netflix**

Netflix, one of the world's largest media streaming providers, uses Lambda extensively. According to their engineering blog, they process:

- Billions of daily events
- Tens of thousands of concurrent executions
- Petabytes of data

**Why Infrastructure Knowledge Matters:**

For high-volume media processing, understanding Lambda's infrastructure helps optimize for cost and performance:

1. **MicroVM architecture**: Allows Netflix to optimize memory allocation per function
2. **Worker placement understanding**: Helps them design for multi-region resilience
3. **Throttle limits**: Guides their implementation of backpressure mechanisms

### Real-Time Gaming: Latency-Critical Operations

**Case Study: Square Enix**

Square Enix uses AWS Lambda for image processing in its Massively Multiplayer Online Role-Playing Games (MMORPGs). They achieved:

- Handling traffic spikes of up to 30x normal levels
- Reducing image processing time from hours to seconds
- Lower infrastructure and operational costs

**Why Infrastructure Knowledge Matters:**

For gaming companies, understanding Lambda's infrastructure enables:

1. **Predictable player experiences**: Minimizing cold starts for user-facing operations
2. **Cost-effective scaling**: Handling unpredictable player counts without over-provisioning
3. **Region-specific optimizations**: Understanding how worker placement affects global players

### Telecommunications: Mission-Critical Systems

**Case Study: T-Mobile**

T-Mobile, serving more than 70 million customers, has a "serverless first" policy for new services. They use Lambda for mission-critical applications that serve millions of users.

**Why Infrastructure Knowledge Matters:**

For telecommunications companies, infrastructure knowledge directly impacts reliability:

1. **Execution environment lifecycle**: Helps predict and mitigate potential service degradations
2. **Cross-region design**: Understanding worker placement guides multi-region architectures
3. **Throttle limit handling**: Critical for managing high-volume messaging and notification systems

### E-Commerce: Handling Traffic Spikes

**Case Study: MatHem**

MatHem, Sweden's largest online-only grocery retailer, uses a serverless architecture that allows them to:

- Bring new features to customers up to 10x faster
- Handle seasonal shopping spikes
- Reduce operational overhead

**Why Infrastructure Knowledge Matters:**

For e-commerce platforms, especially those with seasonal traffic patterns, understanding Lambda's infrastructure enables:

1. **Optimized cold start handling**: Essential during flash sales or holiday shopping
2. **Cost-effective scaling**: Understanding when to use Provisioned Concurrency vs. on-demand
3. **Efficient resource allocation**: Matching memory allocation to workload characteristics

---

## Optimizing Lambda for Your Use Case

Armed with understanding of cold starts, infrastructure, and real-world applications, let's explore how to optimize Lambda for specific scenarios.

### For Latency-Sensitive Applications

If your application requires consistent, low-latency responses (like financial services or gaming):

1. **Runtime selection matters**: According to AWS research, cold start times vary significantly by runtime. Consider using Rust, Go, or the Lambda Low Latency Runtime (LLRT) for the shortest cold starts
2. **Memory configuration**: Higher memory allocations not only provide more computing power but also reduce cold start times
3. **Code optimization**: Minimize imports and dependencies to reduce initialization time
4. **Strategic Provisioned Concurrency**: Use it for critical flows based on traffic patterns

### For High-Throughput Systems

If your system processes large volumes of data or requests (like media processing or analytics):

1. **Concurrency limits**: Understand your account's limits and request increases if needed
2. **Burst handling**: Implement client-side throttling or queuing to handle burst limits
3. **Batch processing**: Use SQS with batching to optimize throughput and costs
4. **Infrastructure awareness**: Design with Lambda's scaling behavior in mind

### For Cost-Sensitive Workloads

If cost optimization is a primary concern:

1. **Right-size memory**: Use tools like AWS Lambda Power Tuning to find the optimal balance of cost and performance
2. **Exploit proactive initialization**: Design your system to benefit from AWS's warming without paying for Provisioned Concurrency
3. **Function consolidation**: Balance the microservices approach with function consolidation to reduce cold starts
4. **Execution time optimization**: Reduce billable duration by optimizing code and dependencies

---

## Conclusion: The Future of Lambda Infrastructure

Understanding Lambda's cold starts, infrastructure, and real-world applications puts you ahead of most developers. As AWS continues to evolve the service, several trends are emerging:

1. **Improved cold start handling**: Mechanisms like Proactive Initialization demonstrate AWS's commitment to addressing this challenge
2. **More efficient infrastructure**: Firecracker continues to evolve, becoming faster and more resource-efficient
3. **Specialized runtimes**: The introduction of LLRT shows AWS's focus on latency-sensitive workloads
4. **Enterprise-focused features**: As more critical workloads move to Lambda, expect more features addressing enterprise needs

By staying informed about Lambda's inner workings, you'll be better positioned to leverage these advancements and build more efficient, reliable, and cost-effective serverless applications.

## References

1. Stuyvenberg, A. (2023). "Understanding AWS Lambda Proactive Initialization." Retrieved from https://aaronstuyvenberg.com/posts/understanding-proactive-initialization
2. Isenberg, R. (2024). "AWS Lambda Cold Starts Explained: What They Are & How to Reduce Them." Retrieved from https://www.ranthebuilder.cloud/post/is-aws-lambda-cold-start-still-an-issue-in-2024
3. AWS Documentation. (2024). "Understanding the Lambda execution environment lifecycle." Retrieved from https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtime-environment.html
4. Barr, J. (2018). "Firecracker – Lightweight Virtualization for Serverless Computing." AWS News Blog. Retrieved from https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/
5. Awad, J. W. (2024). "AWS Lambda Architecture Deep Dive." Medium. Retrieved from https://medium.com/@joudwawad/aws-lambda-architecture-deep-dive-bef856b9b2c4
6. AWS. (2024). "AWS Lambda Customer Case Studies." Retrieved from https://aws.amazon.com/lambda/resources/customer-case-studies/

What aspects of Lambda's infrastructure have you found most impactful in your applications? Have you experienced the benefits of proactive initialization? I'd love to hear about your experiences in the comments.  -->
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Understanding and Mitigating AWS Lambda Throttling in High-Concurrency Workloads</title><link>/lambda-throttling.html</link><author>pedrohbl_</author><category>aws</category><category>lambda</category><category>throttling</category><guid>/lambda-throttling.html</guid><pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate><source url="">tag-aws</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>When dealing with high-concurrency workloads, scaling AWS Lambda effectively while avoiding throttling can become a challenge. This post explores a real-world scenario where an application(just like a worker), written in Kotlin, processed over 1,000,000 records in a blob located in S3 using a custom asynchronous iteration method. Each record triggered an asynchronous Lambda invocation that interacted with DynamoDB. However, the setup led to 429 Too Many Requests errors occurring consistently during peak loads exceeding 10,000 TPS, indicating throttling issues with AWS Lambda.
The article will:</p>
<ol>
<li>
<p><strong>Outline the problem</strong> faced while processing high-concurrency workloads.</p>
</li>
<li>
<p><strong>Explain AWS Lambda throttling mechanisms</strong>, based on the <a href="https://aws.amazon.com/blogs/compute/understanding-aws-lambdas-invoke-throttle-limits/">AWS Compute Blog article by James Beswick</a>.</p>
</li>
<li>
<p><strong>Discuss solutions</strong> to mitigate throttling.</p>
</li>
<li>
<p><strong>TBD</strong> Maybe in the future I'll Provide a real-world proof of concept <strong>(POC)</strong> to evaluate each mitigation technique.</p>
</li>
</ol>
<hr />
<h2><a href="#use-case" aria-hidden="true" class="anchor" id="use-case"></a>Use Case</h2>
<p>To better illustrate the challenges and solutions, consider the following use case:</p>
<ul>
<li><strong>Dataset:</strong> The workload involves processing a large file with 1 million records stored in an S3 bucket.</li>
<li><strong>Data Characteristics:</strong> Each record contains 8 columns of strings, primarily UUIDs (36 bytes each). This results in approximately 288 bytes per record.</li>
<li><strong>Worker Configuration:</strong> The application is deployed on a SINGLE node with the following specifications:
<ul>
<li><strong>vCPUs:</strong> 4</li>
<li><strong>RAM:</strong> 8 GB</li>
</ul>
</li>
</ul>
<h3><a href="#resource-calculations" aria-hidden="true" class="anchor" id="resource-calculations"></a>Resource Calculations</h3>
<ol>
<li>
<p><strong>Memory Requirements:</strong></p>
<ul>
<li>Each record occupies 288 bytes.</li>
<li>For 100 concurrent coroutines:
<ul>
<li>( 288 * 100 = 28,800 bytes approx 28.8KB )</li>
</ul>
</li>
<li>Adding a 20 KB overhead per coroutine for runtime management:
<ul>
<li>( 100 * 20KB = 2,000KB approx 2MB )</li>
</ul>
</li>
<li>Total memory consumption:
<ul>
<li>( 28.8KB + 2,000KB = 2.028MB )</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CPU Considerations:</strong></p>
<ul>
<li>Let's assume each vCPU can handle approximately 100-150 threads (or coroutines) effectively, actually it could handle much more depending on workload. But we can safely assume this number of threads as a safe margin for the given setup, based on <a href="https://github.com/mmoraesbr/kotlin-coroutine-benchmark">Kotlin async coroutines benchmark</a>.</li>
<li>For this use case, 4 vCPUs are sufficient to manage 100 concurrent coroutines with minimal contention.</li>
</ul>
</li>
</ol>
<p>This setup ensures that the system remains stable while processing a high volume of records efficiently.</p>
<h2><a href="#the-challenge" aria-hidden="true" class="anchor" id="the-challenge"></a>The Challenge</h2>
<h3><a href="#problem-context" aria-hidden="true" class="anchor" id="problem-context"></a>Problem Context</h3>
<p>A workload involving processing a large file of over 1,000,000 records can utilize concurrency in Kotlin to invoke AWS Lambda for each record. The Lambda function in this case performed a putItem operation on DynamoDB.</p>
<p>Here’s an example of the Kotlin code for mapAsync:</p>
<pre><code class="language-kotlin">suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync(
    transformation: suspend (T) -&gt; R
): List&lt;R&gt; = coroutineScope {
    this@mapAsync
        .map { async { transformation(it) } }
        .awaitAll()
}

suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync(
    concurrency: Int,
    transformation: suspend (T) -&gt; R
): List&lt;R&gt; = coroutineScope {
    val semaphore = Semaphore(concurrency)
    this@mapAsync
        .map { async { semaphore.withPermit { transformation(it) } } }
        .awaitAll()
}
</code></pre>
<p>This method processes records significantly faster than a standard for loop, but it can flood the system with Lambda invocations, triggering throttling. The 429 Too Many Requests errors can be attributed to:</p>
<ol>
<li><strong>Concurrency Limits</strong>: AWS imposes a limit on the number of concurrent executions per account.</li>
<li><strong>TPS (Transactions Per Second) Limits</strong>: High TPS can overwhelm the Invoke Data Plane.</li>
<li><strong>Burst Limits</strong>: Limits the rate at which concurrency can scale, governed by the token bucket algorithm.</li>
</ol>
<h3><a href="#observed-errors" aria-hidden="true" class="anchor" id="observed-errors"></a>Observed Errors</h3>
<ul>
<li><strong>429 Too Many Requests</strong>: Errors indicate that the Lambda invocations exceeded allowed concurrency or burst limits.</li>
<li><strong>DynamoDB “Provisioned Throughput Exceeded”</strong>: Errors occurred during spikes in DynamoDB writes. But this error won't be covered in this post, maybe in the future I can discuss strategies to work directly with dynamodb IO optimization, for now let's just ignore this one.</li>
</ul>
<hr />
<h2><a href="#aws-lambda-throttling-mechanisms" aria-hidden="true" class="anchor" id="aws-lambda-throttling-mechanisms"></a>AWS Lambda Throttling Mechanisms</h2>
<p>AWS enforces three key throttle limits to protect its infrastructure and ensure fair resource distribution:</p>
<h3><a href="#1-concurrency-limits" aria-hidden="true" class="anchor" id="1-concurrency-limits"></a>1. <strong>Concurrency Limits</strong></h3>
<p>Concurrency limits determine the number of in-flight Lambda executions allowed at a time. For example, with a concurrency limit of 1,000, up to 1,000 Lambda functions can execute simultaneously across all Lambdas in the account and region.</p>
<h3><a href="#2-tps-limits" aria-hidden="true" class="anchor" id="2-tps-limits"></a>2. <strong>TPS Limits</strong></h3>
<p>TPS is derived from concurrency and function duration. For instance:</p>
<ul>
<li>Function duration: 100 ms (equivalent to 100ms =100 × 10<sup>-3</sup> = 0.1s)</li>
<li>Concurrency: 1,000</li>
</ul>
<pre><code class="language-html">TPS = Concurrency / Function Duration = 10,000 TPS
</code></pre>
<p>If the function duration drops below 100 ms, TPS is capped at 10x the concurrency.</p>
<h3><a href="#3-burst-limits" aria-hidden="true" class="anchor" id="3-burst-limits"></a>3. <strong>Burst Limits</strong></h3>
<p>The burst limit ensures gradual scaling of concurrency, avoiding large spikes in cold starts. AWS uses the token bucket algorithm to enforce this:</p>
<ul>
<li>Each invocation consumes a token.</li>
<li>Tokens refill at a fixed rate (e.g., 500 tokens per minute).</li>
<li>The bucket has a maximum capacity (e.g., 1,000 tokens).</li>
</ul>
<p>For more details, refer to the <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html">AWS Lambda Burst Limits</a>.</p>
<hr />
<h2><a href="#mitigation-strategies" aria-hidden="true" class="anchor" id="mitigation-strategies"></a>Mitigation Strategies</h2>
<p>That being said, several approaches can be employed to mitigate the throttling scenarios observed in this case. These techniques aim to address the specific constraints and challenges imposed by the problem:</p>
<h3><a href="#1-limit-concurrency-using-semaphore" aria-hidden="true" class="anchor" id="1-limit-concurrency-using-semaphore"></a>1. <strong>Limit Concurrency Using Semaphore</strong></h3>
<p>Concurrency in Kotlin can be limited using the mapAsync function with a specified concurrency level:</p>
<pre><code class="language-kotlin">val results = records.mapAsync(concurrency = 100) { record -&gt;
    invokeLambda(record)
}
</code></pre>
<p>This implementation leverages coroutines in Kotlin to handle asynchronous operations efficiently. We don't want to deep dive here in how coroutines work, but think of it as a tool that allow lightweight threads to run without blocking, making it possible to manage multiple tasks concurrently without overwhelming system resources.</p>
<p>In the use case described, where the workload involves processing millions of records within 100 concurrent coroutines, the concurrency level of 100 was chosen as a reasonable limit. This decision balances the capacity of the node, configured with 4 vCPUs and 8 GB of RAM, against the resource requirements of each coroutine. For example, each coroutine processes records with a memory overhead of approximately 28.8 KB per record, plus 20 KB for runtime management. This setup ensures stability while maximizing throughput within the system’s constraints.</p>
<p>By introducing a Semaphore, the number of concurrent tasks can be restricted to this specified level. This prevents overloading the Lambda concurrency limits and reduces the risk of 429 Too Many Requests errors, ensuring that the system remains stable and performs reliably.</p>
<h4><a href="#estimated-time-to-process" aria-hidden="true" class="anchor" id="estimated-time-to-process"></a>Estimated Time to Process</h4>
<p>Using the following parameters:</p>
<ul>
<li><code>T</code>: Execution time for a single Lambda invocation.</li>
<li><code>n</code>: Number of concurrent Lambda invocations.</li>
<li><code>Total Records</code>: Total number of records to process.</li>
</ul>
<p>The total processing time can be calculated as:</p>
<pre><code class="language-html">Total Time = (Total Records / n) * T
</code></pre>
<h4><a href="#example-with-t--100-ms" aria-hidden="true" class="anchor" id="example-with-t--100-ms"></a>Example with <code>T = 100 ms</code></h4>
<p>Given:</p>
<ul>
<li><code>Total Records = 1,000,000</code></li>
<li><code>n = 100</code></li>
<li><code>T = 100 ms</code></li>
</ul>
<p>Substituting into the formula:</p>
<pre><code class="language-html">Total Time = (1,000,000 / 100) * 100 ms
</code></pre>
<p>Simplifying:</p>
<pre><code class="language-html">Total Time = 10,000 * 100 ms = 1,000,000 ms
</code></pre>
<p>Converting to seconds and minutes:</p>
<pre><code class="language-html">Total Time = 1,000,000 ms = 1,000 seconds = 16.67 minutes
</code></pre>
<h4><a href="#key-advantages" aria-hidden="true" class="anchor" id="key-advantages"></a>Key Advantages:</h4>
<ul>
<li><strong>Simple Implementation:</strong> Adding a Semaphore to the mapAsync function involves minimal changes to the codebase.</li>
<li><strong>Effective Throttling Control:</strong> The implementation ensures that the number of concurrent Lambda invocations does not exceed the predefined limit, maintaining system stability.</li>
</ul>
<h4><a href="#trade-offs" aria-hidden="true" class="anchor" id="trade-offs"></a>Trade-offs:</h4>
<ul>
<li><strong>Increased Processing Time:</strong> While throttling prevents errors, it may result in longer overall processing times due to the limitation on simultaneous executions.</li>
<li><strong>No Guarantee:</strong> While this approach prevents the majority of 429 Too Many Requests errors, it does not guarantee that such errors will not occur again. This is because, even when the number of concurrent Lambdas in execution is controlled, the system might still exceed burst limits, which are governed by the token bucket algorithm.</li>
<li><strong>Difficult to Manage in Distributed Systems:</strong> This approach is more practical in scenarios with a single node running the application. In distributed systems with multiple nodes running the same application (e.g., 10 instances), it becomes challenging to coordinate a distributed TPS control mechanism. Each node would need to communicate and share state to ensure the total TPS remains within AWS limits, which significantly increases complexity.</li>
</ul>
<h3><a href="#2-retry-with-exponential-backoff" aria-hidden="true" class="anchor" id="2-retry-with-exponential-backoff"></a>2. <strong>Retry with Exponential Backoff</strong></h3>
<p>Retries with exponential backoff handle throttled requests effectively by spreading out retry attempts over time. This reduces the chance of overwhelming the system further when transient issues or throttling limits occur. The exponential backoff algorithm increases the delay between retries after each failed attempt, making it particularly useful in high-concurrency systems and also in services/calls that might fail at times.</p>
<h4><a href="#how-it-works" aria-hidden="true" class="anchor" id="how-it-works"></a>How It Works:</h4>
<p>The implementation retries an AWS Lambda invocation up to a specified number of attempts, introducing exponentially increasing delays between retries. For example:</p>
<pre><code class="language-kotlin">suspend fun invokeWithRetry(record: Record, retries: Int = 3) {
    var attempts = 0
    while (attempts &lt; retries) {
        try {
            invokeLambda(record)
            break
        } catch (e: Exception) {
            if (++attempts == retries) throw e
            delay((2.0.pow(attempts) * 100).toLong())
        }
    }
}
</code></pre>
<h4><a href="#estimated-time-to-process-1" aria-hidden="true" class="anchor" id="estimated-time-to-process-1"></a>Estimated Time to Process</h4>
<p>Assume:</p>
<ul>
<li>Each retry introduces a delay that doubles after every attempt.</li>
<li><code>D</code>: Cumulative delay for retries.</li>
<li><code>r</code>: Number of retry attempts per record.</li>
</ul>
<p>Cumulative delay is given by:</p>
<pre><code class="language-html">D = Σ (2^i * T_retry) for i = 1 to r
</code></pre>
<p>Where:</p>
<ul>
<li><code>T_retry</code> = Base retry delay (e.g., 100 ms).</li>
</ul>
<p>Example with <code>T_retry = 100 ms</code> and <code>r = 3</code>:</p>
<pre><code class="language-html">D = (2^1 * 100 ms) + (2^2 * 100 ms) + (2^3 * 100 ms)
D = 200 ms + 400 ms + 800 ms = 1,400 ms
</code></pre>
<p>If 10% of records require retries, the retry time is:</p>
<pre><code class="language-html">Retry Time = (Total Records * 10%) * D / n
Retry Time = (1,000,000 * 0.1) * 1,400 ms / 100
Retry Time = 1,400,000 ms = 1,400 seconds = 23.33 minutes
</code></pre>
<p>Adding this to the initial processing time:</p>
<pre><code class="language-html">Total Time = Initial Time + Retry Time
Total Time = 16.67 minutes + 23.33 minutes = 40 minutes
</code></pre>
<hr />
<h4><a href="#pros" aria-hidden="true" class="anchor" id="pros"></a>Pros:</h4>
<ul>
<li><strong>Handles transient errors gracefully:</strong> Retries ensure that temporary issues, such as short-lived throttling or network disruptions, do not result in failed processing.</li>
<li><strong>Distributed systems friendly:</strong> Can be independently implemented in each node, avoiding the need for centralized control mechanisms.</li>
<li><strong>Reduces system load during failures:</strong> The increasing delay between retries prevents the system from being overwhelmed.</li>
</ul>
<h4><a href="#cons" aria-hidden="true" class="anchor" id="cons"></a>Cons:</h4>
<ul>
<li><strong>Adds latency:</strong> The exponential backoff mechanism inherently increases the time taken to complete processing, can take even BIGGER times when considering worst case scenarios(potentially 10x more the total time discussed).</li>
<li><strong>Increases code complexity and testability:</strong> Requires additional logic to manage retries and delays and testing those scenarios when only part of the requests fail.</li>
</ul>
<h3><a href="#3-use-sqs-for-decoupling" aria-hidden="true" class="anchor" id="3-use-sqs-for-decoupling"></a>3. <strong>Use SQS for Decoupling</strong></h3>
<p>Amazon Simple Queue Service (SQS) can act as a buffer between producers (e.g., the application processing records) and consumers (e.g., AWS Lambda), enabling controlled, asynchronous processing of requests. This approach decouples the producer and consumer, ensuring the workload is processed at a rate the system can handle.</p>
<h4><a href="#how-it-works-1" aria-hidden="true" class="anchor" id="how-it-works-1"></a>How It Works:</h4>
<ol>
<li>The application writes each record to an SQS queue instead of invoking AWS Lambda directly.</li>
<li>AWS Lambda is configured to process messages from the queue at a controlled rate, dictated by the batch size and concurrency settings.</li>
<li>This ensures that the rate of Lambda invocations remains within the account's concurrency and TPS limits.</li>
</ol>
<h4><a href="#additional-pattern-aws-serverless-land-example" aria-hidden="true" class="anchor" id="additional-pattern-aws-serverless-land-example"></a>Additional Pattern: AWS Serverless Land Example</h4>
<p>This approach aligns with a pattern presented on <a href="https://serverlessland.com/patterns/sqs-lambda-ddb-sam-java">AWS Serverless Land</a>: <strong>Create a Lambda function that batch writes to DynamoDB from SQS</strong>. This pattern deploys an SQS queue, a Lambda Function, and a DynamoDB table, allowing batch writes from SQS messages to DynamoDB. It demonstrates how to leverage a batch processing mechanism to handle high-throughput scenarios effectively.</p>
<p>The provided SAM template uses Java 11, SQS, Lambda, and DynamoDB to create a cost-effective, serverless architecture:</p>
<pre><code class="language-yaml">AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31
Description: sqs-lambda-dynamodb

Globals:
  Function:
    Runtime: java11
    MemorySize: 512
    Timeout: 25

Resources:
  OrderConsumer:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: OrderConsumer
      Handler: com.example.OrderConsumer::handleRequest
      CodeUri: target/sourceCode.zip
      Environment:
        Variables:
          QUEUE_URL: !Sub 'https://sqs.${AWS::Region}.amazonaws.com/${AWS::AccountId}/OrdersQueue'
          REGION: !Sub '${AWS::Region}'
          TABLE_NAME: !Ref OrdersTable
      Policies:
        - AWSLambdaSQSQueueExecutionRole
        - AmazonDynamoDBFullAccess

  OrdersQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: OrdersQueue

  OrdersTable:
    Type: 'AWS::DynamoDB::Table'
    Properties:
      TableName: OrdersTable
      AttributeDefinitions:
        - AttributeName: orderId
          AttributeType: S
      KeySchema:
        - AttributeName: orderId
          KeyType: HASH
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5
</code></pre>
<h4><a href="#estimated-time-to-process-2" aria-hidden="true" class="anchor" id="estimated-time-to-process-2"></a>Estimated Time to Process</h4>
<p>Assume:</p>
<ul>
<li><code>T_batch</code>: Execution time for processing a batch.</li>
<li><code>k</code>: Overhead due to batching.</li>
<li><code>b</code>: Number of messages per batch.</li>
<li><code>n</code>: Lambda concurrency.</li>
</ul>
<p>The total processing time is:</p>
<pre><code class="language-html">Total Time = (Total Records / (b * n)) * (T + k)
</code></pre>
<p>Example with:</p>
<ul>
<li><code>T = 100 ms</code></li>
<li><code>k = 20 ms</code></li>
<li><code>b = 10</code></li>
<li><code>n = 100</code></li>
<li><code>Total Records = 1,000,000</code></li>
</ul>
<p>Substitute into the formula:</p>
<pre><code class="language-html">Total Time = (1,000,000 / (10 * 100)) * (100 ms + 20 ms)
Total Time = (1,000,000 / 1,000) * 120 ms
Total Time = 1,000 * 120 ms = 120,000 ms
</code></pre>
<p>Convert to seconds and minutes:</p>
<pre><code class="language-html">Total Time = 120,000 ms = 120 seconds = 2 minutes
</code></pre>
<h4><a href="#the-importance-of-fifo-queues" aria-hidden="true" class="anchor" id="the-importance-of-fifo-queues"></a>The Importance of FIFO Queues</h4>
<p>To maintain consistency in DynamoDB, it is essential to configure the SQS queue as FIFO (First-In, First-Out) in this case. This ensures that messages are processed in the exact order they are received, which is critical in systems where the order of operations affects the final state of the database. For example:</p>
<ol>
<li>
<p><strong>Out-of-Order Processing Issues:</strong> If two updates to the same DynamoDB record are processed out of order (e.g., Update1 followed by Update2), but Update2 depends on Update1, the database could end up in an inconsistent state. FIFO queues prevent this by enforcing strict order. For our case, there was not duplicated entries on the file so FIFO was not in considerated despite being absolutely important for this usecase.</p>
</li>
<li>
<p><strong>Idempotency Challenges:</strong> Even when Lambda functions are designed to be idempotent, out-of-order processing can lead to unexpected behavior if operations rely on sequential execution. For instance, appending logs or incrementing counters requires a guarantee of order.</p>
</li>
<li>
<p><strong>Trade-offs with FIFO:</strong> While FIFO queues provide consistency, they come with some limitations:</p>
<ul>
<li><strong>Lower Throughput:</strong> FIFO queues have a maximum throughput of 300 transactions per second with batching (or 3,000 if using high-throughput mode).</li>
<li><strong>Increased Latency:</strong> Enforcing order may introduce slight delays in message processing.</li>
</ul>
</li>
</ol>
<hr />
<h4><a href="#pros-1" aria-hidden="true" class="anchor" id="pros-1"></a>Pros:</h4>
<ul>
<li><strong>Decouples producers and consumers:</strong> The producer can continue adding messages to the queue regardless of the Lambda processing speed.</li>
<li><strong>Prevents throttling:</strong> SQS regulates the rate at which messages are delivered to Lambda, avoiding sudden spikes that could exceed AWS limits.</li>
<li><strong>Distributed systems friendly:</strong> Works seamlessly in multi-node systems, as all nodes write to the same queue without requiring coordination.</li>
</ul>
<h4><a href="#cons-1" aria-hidden="true" class="anchor" id="cons-1"></a>Cons:</h4>
<ul>
<li><strong>Adds architectural complexity:</strong> Introducing SQS requires additional components and configuration.</li>
<li><strong>Adds code complexity:</strong> Introduce code complexity to the insertion lambda, so its responsible for managing sqs batch write operations, reading on SQS source and also being able to operate by asynchronous invocation for legacy systems.</li>
<li><strong>Introduces latency:</strong> Messages may wait in the queue before being processed, depending on the Lambda polling rate and queue depth. For example, a queue depth of 10,000 messages and a polling rate of 1,000 messages per second would result in a processing delay.</li>
</ul>
<hr />
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>AWS Lambda throttling issues, particularly for high-concurrency workloads, can be effectively managed using a combination of strategies such as concurrency control, retry mechanisms, and decoupling with SQS. Each of these approaches has its strengths and trade-offs:</p>
<ul>
<li>
<p><strong>Limit Concurrency Using Semaphore</strong>: A straightforward solution for single-node setups, providing reliable throttling control at the cost of slightly increased processing time. However, it requires additional considerations for distributed systems.</p>
</li>
<li>
<p><strong>Retry with Exponential Backoff</strong>: A robust technique for handling transient failures, distributing load over time and avoiding unnecessary retries. Yet, it can add significant latency in worst-case scenarios and increase implementation complexity.</p>
</li>
<li>
<p><strong>Use SQS for Decoupling</strong>: The most scalable and efficient approach when <code>T_batch = T + k</code>, with <code>k</code> being sufficiently small. While it introduces latency and complexity, its benefits make it the go-to solution for large-scale systems.</p>
</li>
</ul>
<p>As an ending insight, we can assure that for small workloads, async invocation can provide faster results, as it avoids the latency of queuing and batch processing. However, as the number of requests increases, direct invocation becomes inefficient and computationally expensive due to the high TPS demand and risk of breaching AWS limits. In contrast, decoupled architectures using SQS and batch processing scale more efficiently, ensuring stability and cost-effectiveness under heavy loads.</p>
<h3><a href="#next-steps-implementing-a-poc" aria-hidden="true" class="anchor" id="next-steps-implementing-a-poc"></a>Next Steps: Implementing a POC</h3>
<p>While this post has focused on explaining the challenges, strategies, and theoretical calculations for mitigation, an actual Proof of Concept (POC) would be very cool to validate and visualize these solutions in practice. A future post might explore how to design and execute a POC to measure the overall performance in a real-world scenario.</p>
<p>For more details on Lambda throttling, refer to the <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html">AWS Lambda Developer Guide</a> and the <a href="https://aws.amazon.com/blogs/compute/understanding-aws-lambdas-invoke-throttle-limits/">AWS Compute Blog</a>.</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item></channel></rss>