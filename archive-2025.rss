<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pedro Lopes</title><link></link><description>Comput. Sci.</description><pubDate>Sat, 08 Feb 2025 00:00:00 GMT</pubDate><lastBuildDate>Mon, 10 Feb 2025 14:45:37 GMT</lastBuildDate><generator>marmite</generator><item><title>Microservices: Hidden Technical Debt (and how to possibly avoid the traps)</title><link>/microservices-debt.html</link><author>pedrohbl_</author><category>microservices</category><category>technical-debt</category><category>distributed-systems</category><category>software-architecture</category><guid>/microservices-debt.html</guid><pubDate>Sat, 08 Feb 2025 00:00:00 GMT</pubDate><source url="">archive-2025</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>These days, I keep seeing microservices being treated as the answer to every software problem. Having worked with various architectures throughout my career, I've noticed how many players often jump into microservices without considering the long-term implications. It reminds me of a project where we turned a perfectly functional monolith into a distributed system just because &quot;that's what modern companies do.&quot;</p>
<p>But here's the thing: microservices aren't a silver bullet. In fact, they can become a massive technical debt that's incredibly hard to pay off. Matt Ranney, DoorDash's Scalability Engineer Lead, makes this point brilliantly in his talk &quot;Microservices are Technical Debt.&quot; After experiencing similar challenges, I decided to dive into this topic, including some cientific papers that covered similar issues like &quot;Microservices Anti-Patterns: A Taxonomy,&quot; to understand what's really going on.</p>
<p>In this post, I'll try to cover:</p>
<ol>
<li>Why microservices are often misunderstood and misused</li>
<li>Common anti-patterns I've encountered (and how to avoid them)</li>
<li>Evidence-based approaches to build systems that actually work</li>
</ol>
<h2><a href="#the-overuse-of-microservices" aria-hidden="true" class="anchor" id="the-overuse-of-microservices"></a>The Overuse of Microservices</h2>
<h3><a href="#why-do-we-default-to-microservices" aria-hidden="true" class="anchor" id="why-do-we-default-to-microservices"></a>Why Do We Default to Microservices?</h3>
<ol>
<li>
<p><strong>The Hype Factor</strong></p>
<ul>
<li>Companies often adopt microservices because it's trendy, without analyzing their actual needs.</li>
<li><strong>Example:</strong> Imagine you’re at a startup trying to attract investors. Your team decides to split a simple app into 10 microservices just to showcase &quot;scalability.&quot; Fast forward six months: you’re drowning in Kubernetes configurations and service mesh setups, while competitors with monoliths ship features twice as fast.</li>
</ul>
</li>
<li>
<p><strong>The Independence Illusion</strong></p>
<ul>
<li>While microservices promise independent teams and deployments, this only works with proper service boundaries.</li>
<li><strong>Example:</strong> Picture working on a user service. You update a field in the <code>UserProfile</code> class, only to discover the notifications service crashes because it hardcoded a dependency on the old field structure. Now you’re stuck updating three services for what should have been a simple change.</li>
</ul>
</li>
<li>
<p><strong>The Monolith Misconception</strong></p>
<ul>
<li>I've noticed a strange fear of monoliths in the industry. But here's what I've learned: for many applications, a well-structured modular monolith is actually the better choice.</li>
<li><strong>Example:</strong> A software development team spent months breaking their monolith into microservices, only to realize they had created a <strong>distributed monolith</strong>—a system that was just as hard to manage, but with added complexity. What if it was just a simplified collection of function calls instead of a bunch of meaningless HTTP calls?</li>
</ul>
</li>
</ol>
<h2><a href="#microservices-as-technical-debt" aria-hidden="true" class="anchor" id="microservices-as-technical-debt"></a>Microservices as Technical Debt</h2>
<p>Matt Ranney makes a compelling case for why microservices can be considered <strong>technical debt</strong>. Here’s why:</p>
<ol>
<li><strong>Initial Speed, Long-Term Pain</strong>
<ul>
<li>Microservices can speed up development in the short term, but they often lead to long-term maintenance challenges.</li>
<li><strong>Example:</strong> Imagine you’re building a feature to let users reset passwords. You start with a simple service:</li>
</ul>
</li>
</ol>
<pre><code class="language-java">    public void resetPassword(String userId, String newPassword) {
         User user = userService.getUser(userId);
         try{
           userService.updateProfile(user, newPassword);
         }
     }
</code></pre>
<p>But obviously the requirements grow and now this evolves into:</p>
<pre><code class="language-java">  // look at the complexity added to the same function
  try {
      userService.updateProfile(user);
      notificationService.notifyProfileUpdate(user.getId());
      authService.refreshUserSession(user.getId());
      analyticsService.trackProfileUpdate(user.getId());
  } catch (ServiceException e) {
      // now you need complex rollback logic =D
      compensationService.handleFailure(user.getId(), &quot;PROFILE_UPDATE&quot;);
  }
</code></pre>
<p>Now, a simple password reset requires four services to work perfectly together. Miss one, and you’ve got angry users or security holes and a fresh war room to deal.</p>
<h2><a href="#2-the-distributed-monolith-trap" aria-hidden="true" class="anchor" id="2-the-distributed-monolith-trap"></a>2. The Distributed Monolith Trap</h2>
<p>Let’s get real: most companies end up with distributed monoliths, not true microservices. Here’s why this happens and why it’s worse than a traditional monolith.</p>
<h3><a href="#practical-example-the-loyalty-points-nightmare" aria-hidden="true" class="anchor" id="practical-example-the-loyalty-points-nightmare"></a>Practical Example: The Loyalty Points Nightmare</h3>
<p>Imagine you’re working on an e-commerce system. You need to add a <code>loyaltyPoints</code> field to user profiles. Here’s what happens:</p>
<ol>
<li>
<p><strong>User Service:</strong></p>
<pre><code class="language-java">public class User {
    private String id;
    private int loyaltyPoints; // New field
}
</code></pre>
</li>
<li>
<p><strong>Payments Service:</strong></p>
<pre><code class="language-java">public class PaymentProcessor {
    public void applyDiscount(String userId) {
        User user = userService.getUser(userId);
        if (user.getLoyaltyPoints() &gt; 1000) { // Now depends on User's new field
            applyDiscount();
        }
    }
}
</code></pre>
</li>
<li>
<p><strong>Analytics Service:</strong></p>
<pre><code class="language-java">public class Analytics {
    public void trackPurchase(String userId) {
        User user = userService.getUser(userId);
        log(&quot;Purchase by user with &quot; + user.getLoyaltyPoints() + &quot; points&quot;);
    }
}
</code></pre>
</li>
</ol>
<p>Suddenly, updating a single field requires:</p>
<ul>
<li>Coordinating deployments across three teams</li>
<li>Ensuring all services update dependencies simultaneously</li>
<li>Risking system-wide failures if any service lags</li>
</ul>
<p>This is the distributed monolith trap—a system with all the complexity of microservices but none of the benefits. As Newman (2021) notes, this anti-pattern is rampant in teams that prioritize speed over thoughtful design.</p>
<h2><a href="#3-hidden-costs-of-microservices" aria-hidden="true" class="anchor" id="3-hidden-costs-of-microservices"></a>3. Hidden Costs of Microservices</h2>
<ul>
<li>
<p>Microservices introduce hidden costs, such as network latency, service discovery, and inter-service communication.</p>
</li>
<li>
<p><strong>Example:</strong> Imagine you’re debugging why user sessions expire randomly. After days of checking code, you discover a 200ms delay between the auth service and session service. The timeout configuration didn’t account for this latency, causing sporadic failures. The fix? Hours and hours wasted of meaningless debugging time, as the root cause for the problem was a bad-optimized code deployed by the auth team.</p>
</li>
</ul>
<hr />
<h2><a href="#common-anti-patterns-in-microservices" aria-hidden="true" class="anchor" id="common-anti-patterns-in-microservices"></a>Common Anti-Patterns in Microservices</h2>
<p>The paper <em>Microservices Anti-Patterns: A Taxonomy</em> by Taibi et al. (2018) provides a solid framework for understanding these issues. Here are some key anti-patterns and real-world examples:</p>
<h3><a href="#1-the-shared-database-anti-pattern" aria-hidden="true" class="anchor" id="1-the-shared-database-anti-pattern"></a>1. <strong>The Shared Database Anti-Pattern</strong></h3>
<ul>
<li>One of the most common mistakes is sharing databases between microservices. This creates tight coupling and defeats the purpose of having independent services.</li>
<li><strong>Example:</strong> Imagine you’re working on a notifications service that shares a database with the user service:</li>
</ul>
<pre><code class="language-sql">   -- Both services read from the same table
   SELECT email FROM users WHERE id = '123';
</code></pre>
<ul>
<li>When the user service adds a new <code>is_email_verified</code> column and starts deleting unverified accounts, your notifications service starts failing because it wasn’t updated to handle the new logic.</li>
</ul>
<h3><a href="#2-hardcoded-urls-and-tight-coupling" aria-hidden="true" class="anchor" id="2-hardcoded-urls-and-tight-coupling"></a>2. <strong>Hardcoded URLs and Tight Coupling</strong></h3>
<ul>
<li>Hardcoding URLs or endpoints between services is a recipe for disaster. It creates tight coupling and makes the system more fragile.</li>
<li><strong>Example:</strong> Picture this code in your payments service:</li>
</ul>
<pre><code class="language-java">   // Bad: Hardcoded URL
   String userServiceUrl = &quot;http://user-service-prod:8080/api/users&quot;;
</code></pre>
<ul>
<li>When you try to test this service locally, it fails because it can’t reach the production user service.</li>
</ul>
<h3><a href="#3-the-too-many-services-problem" aria-hidden="true" class="anchor" id="3-the-too-many-services-problem"></a>3. <strong>The &quot;Too Many Services&quot; Problem</strong></h3>
<ul>
<li>This one is a classig example... Splitting your system into too many tiny services can lead to chaos. Each service adds overhead in terms of deployment, monitoring, and maintenance.</li>
<li><strong>Example:</strong> Imagine you’re working on a food delivery app with these services:
<ol>
<li><code>user-service</code></li>
<li><code>restaurant-service</code></li>
<li><code>menu-service</code> (for restaurant menus)</li>
<li><code>menu-item-service</code> (for individual dishes)</li>
<li><code>menu-category-service</code> (for dish categories)</li>
</ol>
</li>
</ul>
<p>Now, displaying a restaurant’s menu requires calls to three services. A simple feature like adding a new dish category takes weeks to implement across teams.</p>
<h3><a href="#4-lack-of-governance" aria-hidden="true" class="anchor" id="4-lack-of-governance"></a>4. <strong>Lack of Governance</strong></h3>
<ul>
<li>Without proper guidelines, teams end up creating services that overlap or don’t integrate well.</li>
<li><strong>Example:</strong> A very known company had two teams building nearly identical services because there was no governance in place to coordinate their efforts.</li>
</ul>
<hr />
<h2><a href="#solutions-and-best-practices" aria-hidden="true" class="anchor" id="solutions-and-best-practices"></a>Solutions and Best Practices</h2>
<p>So, how do we avoid these pitfalls? First, it's important to recognize that both monolith decomposition and microservices modeling are complex fields, extensively studied in research and industry, but in general here are some widely adopted strategies:</p>
<h3><a href="#1-monolith-first" aria-hidden="true" class="anchor" id="1-monolith-first"></a>1. <strong>Monolith First</strong></h3>
<ul>
<li>As Martin Fowler suggests, <em>&quot;monolith first.&quot;</em> Build a monolith, and only split it into microservices when necessary.</li>
<li><strong>Example:</strong> Imagine you’re building a new fitness tracking app. Start with a monolith:</li>
</ul>
<pre><code class="language-java">   public class Workout {
      private String userId;
      private LocalDateTime startTime;
      private int durationMinutes;
   }
</code></pre>
<ul>
<li>
<p><strong>Only split into microservices when</strong>:</p>
<ol>
<li>
<p>Different components have clearly different scaling needs.</p>
</li>
<li>
<p>Teams are large enough to justify separate ownership.</p>
</li>
<li>
<p>There's a structured governance process in your company over building decoupled services.</p>
</li>
<li>
<p>Automated tests are available for each service.</p>
</li>
<li>
<p>Automated deployments are available for each service.</p>
</li>
<li>
<p>Live monitoring, distributed tracing and health checks are available for each service.</p>
</li>
<li>
<p>Automated rollback during deployment are also available for each service.</p>
</li>
</ol>
</li>
</ul>
<h3><a href="#2-domain-driven-design-ddd" aria-hidden="true" class="anchor" id="2-domain-driven-design-ddd"></a>2. <strong>Domain-Driven Design (DDD)</strong></h3>
<ul>
<li>Define clear boundaries for each service based on business domains. This helps avoid tight coupling and ensures that services are truly independent.</li>
<li><strong>Example:</strong> For an e-commerce platform:
<ol>
<li><strong>Bounded Context:</strong> Payments</li>
<li><strong>Bounded Context:</strong> Inventory</li>
<li><strong>Bounded Context:</strong> Shipping</li>
</ol>
</li>
</ul>
<p>Each context has its own database and API boundaries. Changes to payment logic don’t affect shipping =D.</p>
<h3><a href="#3-the-reverse-conway-maneuver" aria-hidden="true" class="anchor" id="3-the-reverse-conway-maneuver"></a>3. <strong>The Reverse Conway Maneuver</strong></h3>
<ul>
<li>The <strong>Conway’s Law</strong> states that organizations design systems that mirror their communication structures. The <strong>Reverse Conway Maneuver</strong> flips this around: design your teams to match the architecture you want, basically, the teams designing a go-to system architecture based on current needs, tech debts and throttles. Just like a &quot;reference architecture&quot; based on reverse engineering. This way you have clear boundaries designed for each team and let they execute their goal software architecture independently.</li>
<li><strong>Example:</strong> Consider a given company X, instead of having frontend, backend, and ops teams working in silos, they restructured teams around business capabilities: a 'Payments Team' owning both backend and UI for payments, and a 'Shipping Team' handling logistics end-to-end. This allowed them to scale services independently while keeping architecture cohesive.</li>
</ul>
<hr />
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>I didn't mean to be the devil’s advocate here, but I tried to highlight some key points because, in many cases, the direction that microservices adoption takes ends up being unsustainable. That’s why it’s crucial to pay attention to the trade-offs and pitfalls discussed.</p>
<p>This is not supposed to be an exhaustive analysis—far from it. As microservices, technical debt, and distributed architectures are vast and evolving fields, hundreds of thousands of cientific papers already discussed these topics. My goal was to cover some of the major issues I’ve encountered, and hopefully, this discussion helps you navigate the complexities of microservices with a more critical perspective.</p>
<hr />
<h2><a href="#references" aria-hidden="true" class="anchor" id="references"></a>References</h2>
<ol>
<li>Matt Ranney, <em>Microservices are Technical Debt</em> <a href="https://www.youtube.com/watch?v=LcJKxPXYudE">link</a>.</li>
<li>Taibi et al., <em>Microservices Anti-Patterns: A Taxonomy</em> (2018).</li>
<li>Martin Fowler, <em>Monolith First</em>  <a href="https://martinfowler.com/bliki/MonolithFirst.html">link</a>.</li>
<li>Conway’s Law and the Reverse Conway Maneuver (Various Sources).</li>
</ol>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Understanding and Mitigating AWS Lambda Throttling in High-Concurrency Workloads</title><link>/lambda-throttling.html</link><author>pedrohbl_</author><category>aws</category><category>lambda</category><category>throttling</category><guid>/lambda-throttling.html</guid><pubDate>Sat, 25 Jan 2025 00:00:00 GMT</pubDate><source url="">archive-2025</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>When dealing with high-concurrency workloads, scaling AWS Lambda effectively while avoiding throttling can become a challenge. This post explores a real-world scenario where an application(just like a worker), written in Kotlin, processed over 1,000,000 records in a blob located in S3 using a custom asynchronous iteration method. Each record triggered an asynchronous Lambda invocation that interacted with DynamoDB. However, the setup led to 429 Too Many Requests errors occurring consistently during peak loads exceeding 10,000 TPS, indicating throttling issues with AWS Lambda.
The article will:</p>
<ol>
<li>
<p><strong>Outline the problem</strong> faced while processing high-concurrency workloads.</p>
</li>
<li>
<p><strong>Explain AWS Lambda throttling mechanisms</strong>, based on the <a href="https://aws.amazon.com/blogs/compute/understanding-aws-lambdas-invoke-throttle-limits/">AWS Compute Blog article by James Beswick</a>.</p>
</li>
<li>
<p><strong>Discuss solutions</strong> to mitigate throttling.</p>
</li>
<li>
<p><strong>TBD</strong> Maybe in the future I'll Provide a real-world proof of concept <strong>(POC)</strong> to evaluate each mitigation technique.</p>
</li>
</ol>
<hr />
<h2><a href="#use-case" aria-hidden="true" class="anchor" id="use-case"></a>Use Case</h2>
<p>To better illustrate the challenges and solutions, consider the following use case:</p>
<ul>
<li><strong>Dataset:</strong> The workload involves processing a large file with 1 million records stored in an S3 bucket.</li>
<li><strong>Data Characteristics:</strong> Each record contains 8 columns of strings, primarily UUIDs (36 bytes each). This results in approximately 288 bytes per record.</li>
<li><strong>Worker Configuration:</strong> The application is deployed on a SINGLE node with the following specifications:
<ul>
<li><strong>vCPUs:</strong> 4</li>
<li><strong>RAM:</strong> 8 GB</li>
</ul>
</li>
</ul>
<h3><a href="#resource-calculations" aria-hidden="true" class="anchor" id="resource-calculations"></a>Resource Calculations</h3>
<ol>
<li>
<p><strong>Memory Requirements:</strong></p>
<ul>
<li>Each record occupies 288 bytes.</li>
<li>For 100 concurrent coroutines:
<ul>
<li>( 288 * 100 = 28,800 bytes approx 28.8KB )</li>
</ul>
</li>
<li>Adding a 20 KB overhead per coroutine for runtime management:
<ul>
<li>( 100 * 20KB = 2,000KB approx 2MB )</li>
</ul>
</li>
<li>Total memory consumption:
<ul>
<li>( 28.8KB + 2,000KB = 2.028MB )</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CPU Considerations:</strong></p>
<ul>
<li>Let's assume each vCPU can handle approximately 100-150 threads (or coroutines) effectively, actually it could handle much more depending on workload. But we can safely assume this number of threads as a safe margin for the given setup, based on <a href="https://github.com/mmoraesbr/kotlin-coroutine-benchmark">Kotlin async coroutines benchmark</a>.</li>
<li>For this use case, 4 vCPUs are sufficient to manage 100 concurrent coroutines with minimal contention.</li>
</ul>
</li>
</ol>
<p>This setup ensures that the system remains stable while processing a high volume of records efficiently.</p>
<h2><a href="#the-challenge" aria-hidden="true" class="anchor" id="the-challenge"></a>The Challenge</h2>
<h3><a href="#problem-context" aria-hidden="true" class="anchor" id="problem-context"></a>Problem Context</h3>
<p>A workload involving processing a large file of over 1,000,000 records can utilize concurrency in Kotlin to invoke AWS Lambda for each record. The Lambda function in this case performed a putItem operation on DynamoDB.</p>
<p>Here’s an example of the Kotlin code for mapAsync:</p>
<pre><code class="language-kotlin">suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync(
    transformation: suspend (T) -&gt; R
): List&lt;R&gt; = coroutineScope {
    this@mapAsync
        .map { async { transformation(it) } }
        .awaitAll()
}

suspend fun &lt;T, R&gt; Iterable&lt;T&gt;.mapAsync(
    concurrency: Int,
    transformation: suspend (T) -&gt; R
): List&lt;R&gt; = coroutineScope {
    val semaphore = Semaphore(concurrency)
    this@mapAsync
        .map { async { semaphore.withPermit { transformation(it) } } }
        .awaitAll()
}
</code></pre>
<p>This method processes records significantly faster than a standard for loop, but it can flood the system with Lambda invocations, triggering throttling. The 429 Too Many Requests errors can be attributed to:</p>
<ol>
<li><strong>Concurrency Limits</strong>: AWS imposes a limit on the number of concurrent executions per account.</li>
<li><strong>TPS (Transactions Per Second) Limits</strong>: High TPS can overwhelm the Invoke Data Plane.</li>
<li><strong>Burst Limits</strong>: Limits the rate at which concurrency can scale, governed by the token bucket algorithm.</li>
</ol>
<h3><a href="#observed-errors" aria-hidden="true" class="anchor" id="observed-errors"></a>Observed Errors</h3>
<ul>
<li><strong>429 Too Many Requests</strong>: Errors indicate that the Lambda invocations exceeded allowed concurrency or burst limits.</li>
<li><strong>DynamoDB “Provisioned Throughput Exceeded”</strong>: Errors occurred during spikes in DynamoDB writes. But this error won't be covered in this post, maybe in the future I can discuss strategies to work directly with dynamodb IO optimization, for now let's just ignore this one.</li>
</ul>
<hr />
<h2><a href="#aws-lambda-throttling-mechanisms" aria-hidden="true" class="anchor" id="aws-lambda-throttling-mechanisms"></a>AWS Lambda Throttling Mechanisms</h2>
<p>AWS enforces three key throttle limits to protect its infrastructure and ensure fair resource distribution:</p>
<h3><a href="#1-concurrency-limits" aria-hidden="true" class="anchor" id="1-concurrency-limits"></a>1. <strong>Concurrency Limits</strong></h3>
<p>Concurrency limits determine the number of in-flight Lambda executions allowed at a time. For example, with a concurrency limit of 1,000, up to 1,000 Lambda functions can execute simultaneously across all Lambdas in the account and region.</p>
<h3><a href="#2-tps-limits" aria-hidden="true" class="anchor" id="2-tps-limits"></a>2. <strong>TPS Limits</strong></h3>
<p>TPS is derived from concurrency and function duration. For instance:</p>
<ul>
<li>Function duration: 100 ms (equivalent to 100ms =100 × 10<sup>-3</sup> = 0.1s)</li>
<li>Concurrency: 1,000</li>
</ul>
<pre><code class="language-html">TPS = Concurrency / Function Duration = 10,000 TPS
</code></pre>
<p>If the function duration drops below 100 ms, TPS is capped at 10x the concurrency.</p>
<h3><a href="#3-burst-limits" aria-hidden="true" class="anchor" id="3-burst-limits"></a>3. <strong>Burst Limits</strong></h3>
<p>The burst limit ensures gradual scaling of concurrency, avoiding large spikes in cold starts. AWS uses the token bucket algorithm to enforce this:</p>
<ul>
<li>Each invocation consumes a token.</li>
<li>Tokens refill at a fixed rate (e.g., 500 tokens per minute).</li>
<li>The bucket has a maximum capacity (e.g., 1,000 tokens).</li>
</ul>
<p>For more details, refer to the <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html">AWS Lambda Burst Limits</a>.</p>
<hr />
<h2><a href="#mitigation-strategies" aria-hidden="true" class="anchor" id="mitigation-strategies"></a>Mitigation Strategies</h2>
<p>That being said, several approaches can be employed to mitigate the throttling scenarios observed in this case. These techniques aim to address the specific constraints and challenges imposed by the problem:</p>
<h3><a href="#1-limit-concurrency-using-semaphore" aria-hidden="true" class="anchor" id="1-limit-concurrency-using-semaphore"></a>1. <strong>Limit Concurrency Using Semaphore</strong></h3>
<p>Concurrency in Kotlin can be limited using the mapAsync function with a specified concurrency level:</p>
<pre><code class="language-kotlin">val results = records.mapAsync(concurrency = 100) { record -&gt;
    invokeLambda(record)
}
</code></pre>
<p>This implementation leverages coroutines in Kotlin to handle asynchronous operations efficiently. We don't want to deep dive here in how coroutines work, but think of it as a tool that allow lightweight threads to run without blocking, making it possible to manage multiple tasks concurrently without overwhelming system resources.</p>
<p>In the use case described, where the workload involves processing millions of records within 100 concurrent coroutines, the concurrency level of 100 was chosen as a reasonable limit. This decision balances the capacity of the node, configured with 4 vCPUs and 8 GB of RAM, against the resource requirements of each coroutine. For example, each coroutine processes records with a memory overhead of approximately 28.8 KB per record, plus 20 KB for runtime management. This setup ensures stability while maximizing throughput within the system’s constraints.</p>
<p>By introducing a Semaphore, the number of concurrent tasks can be restricted to this specified level. This prevents overloading the Lambda concurrency limits and reduces the risk of 429 Too Many Requests errors, ensuring that the system remains stable and performs reliably.</p>
<h4><a href="#estimated-time-to-process" aria-hidden="true" class="anchor" id="estimated-time-to-process"></a>Estimated Time to Process</h4>
<p>Using the following parameters:</p>
<ul>
<li><code>T</code>: Execution time for a single Lambda invocation.</li>
<li><code>n</code>: Number of concurrent Lambda invocations.</li>
<li><code>Total Records</code>: Total number of records to process.</li>
</ul>
<p>The total processing time can be calculated as:</p>
<pre><code class="language-html">Total Time = (Total Records / n) * T
</code></pre>
<h4><a href="#example-with-t--100-ms" aria-hidden="true" class="anchor" id="example-with-t--100-ms"></a>Example with <code>T = 100 ms</code></h4>
<p>Given:</p>
<ul>
<li><code>Total Records = 1,000,000</code></li>
<li><code>n = 100</code></li>
<li><code>T = 100 ms</code></li>
</ul>
<p>Substituting into the formula:</p>
<pre><code class="language-html">Total Time = (1,000,000 / 100) * 100 ms
</code></pre>
<p>Simplifying:</p>
<pre><code class="language-html">Total Time = 10,000 * 100 ms = 1,000,000 ms
</code></pre>
<p>Converting to seconds and minutes:</p>
<pre><code class="language-html">Total Time = 1,000,000 ms = 1,000 seconds = 16.67 minutes
</code></pre>
<h4><a href="#key-advantages" aria-hidden="true" class="anchor" id="key-advantages"></a>Key Advantages:</h4>
<ul>
<li><strong>Simple Implementation:</strong> Adding a Semaphore to the mapAsync function involves minimal changes to the codebase.</li>
<li><strong>Effective Throttling Control:</strong> The implementation ensures that the number of concurrent Lambda invocations does not exceed the predefined limit, maintaining system stability.</li>
</ul>
<h4><a href="#trade-offs" aria-hidden="true" class="anchor" id="trade-offs"></a>Trade-offs:</h4>
<ul>
<li><strong>Increased Processing Time:</strong> While throttling prevents errors, it may result in longer overall processing times due to the limitation on simultaneous executions.</li>
<li><strong>No Guarantee:</strong> While this approach prevents the majority of 429 Too Many Requests errors, it does not guarantee that such errors will not occur again. This is because, even when the number of concurrent Lambdas in execution is controlled, the system might still exceed burst limits, which are governed by the token bucket algorithm.</li>
<li><strong>Difficult to Manage in Distributed Systems:</strong> This approach is more practical in scenarios with a single node running the application. In distributed systems with multiple nodes running the same application (e.g., 10 instances), it becomes challenging to coordinate a distributed TPS control mechanism. Each node would need to communicate and share state to ensure the total TPS remains within AWS limits, which significantly increases complexity.</li>
</ul>
<h3><a href="#2-retry-with-exponential-backoff" aria-hidden="true" class="anchor" id="2-retry-with-exponential-backoff"></a>2. <strong>Retry with Exponential Backoff</strong></h3>
<p>Retries with exponential backoff handle throttled requests effectively by spreading out retry attempts over time. This reduces the chance of overwhelming the system further when transient issues or throttling limits occur. The exponential backoff algorithm increases the delay between retries after each failed attempt, making it particularly useful in high-concurrency systems and also in services/calls that might fail at times.</p>
<h4><a href="#how-it-works" aria-hidden="true" class="anchor" id="how-it-works"></a>How It Works:</h4>
<p>The implementation retries an AWS Lambda invocation up to a specified number of attempts, introducing exponentially increasing delays between retries. For example:</p>
<pre><code class="language-kotlin">suspend fun invokeWithRetry(record: Record, retries: Int = 3) {
    var attempts = 0
    while (attempts &lt; retries) {
        try {
            invokeLambda(record)
            break
        } catch (e: Exception) {
            if (++attempts == retries) throw e
            delay((2.0.pow(attempts) * 100).toLong())
        }
    }
}
</code></pre>
<h4><a href="#estimated-time-to-process-1" aria-hidden="true" class="anchor" id="estimated-time-to-process-1"></a>Estimated Time to Process</h4>
<p>Assume:</p>
<ul>
<li>Each retry introduces a delay that doubles after every attempt.</li>
<li><code>D</code>: Cumulative delay for retries.</li>
<li><code>r</code>: Number of retry attempts per record.</li>
</ul>
<p>Cumulative delay is given by:</p>
<pre><code class="language-html">D = Σ (2^i * T_retry) for i = 1 to r
</code></pre>
<p>Where:</p>
<ul>
<li><code>T_retry</code> = Base retry delay (e.g., 100 ms).</li>
</ul>
<p>Example with <code>T_retry = 100 ms</code> and <code>r = 3</code>:</p>
<pre><code class="language-html">D = (2^1 * 100 ms) + (2^2 * 100 ms) + (2^3 * 100 ms)
D = 200 ms + 400 ms + 800 ms = 1,400 ms
</code></pre>
<p>If 10% of records require retries, the retry time is:</p>
<pre><code class="language-html">Retry Time = (Total Records * 10%) * D / n
Retry Time = (1,000,000 * 0.1) * 1,400 ms / 100
Retry Time = 1,400,000 ms = 1,400 seconds = 23.33 minutes
</code></pre>
<p>Adding this to the initial processing time:</p>
<pre><code class="language-html">Total Time = Initial Time + Retry Time
Total Time = 16.67 minutes + 23.33 minutes = 40 minutes
</code></pre>
<hr />
<h4><a href="#pros" aria-hidden="true" class="anchor" id="pros"></a>Pros:</h4>
<ul>
<li><strong>Handles transient errors gracefully:</strong> Retries ensure that temporary issues, such as short-lived throttling or network disruptions, do not result in failed processing.</li>
<li><strong>Distributed systems friendly:</strong> Can be independently implemented in each node, avoiding the need for centralized control mechanisms.</li>
<li><strong>Reduces system load during failures:</strong> The increasing delay between retries prevents the system from being overwhelmed.</li>
</ul>
<h4><a href="#cons" aria-hidden="true" class="anchor" id="cons"></a>Cons:</h4>
<ul>
<li><strong>Adds latency:</strong> The exponential backoff mechanism inherently increases the time taken to complete processing, can take even BIGGER times when considering worst case scenarios(potentially 10x more the total time discussed).</li>
<li><strong>Increases code complexity and testability:</strong> Requires additional logic to manage retries and delays and testing those scenarios when only part of the requests fail.</li>
</ul>
<h3><a href="#3-use-sqs-for-decoupling" aria-hidden="true" class="anchor" id="3-use-sqs-for-decoupling"></a>3. <strong>Use SQS for Decoupling</strong></h3>
<p>Amazon Simple Queue Service (SQS) can act as a buffer between producers (e.g., the application processing records) and consumers (e.g., AWS Lambda), enabling controlled, asynchronous processing of requests. This approach decouples the producer and consumer, ensuring the workload is processed at a rate the system can handle.</p>
<h4><a href="#how-it-works-1" aria-hidden="true" class="anchor" id="how-it-works-1"></a>How It Works:</h4>
<ol>
<li>The application writes each record to an SQS queue instead of invoking AWS Lambda directly.</li>
<li>AWS Lambda is configured to process messages from the queue at a controlled rate, dictated by the batch size and concurrency settings.</li>
<li>This ensures that the rate of Lambda invocations remains within the account's concurrency and TPS limits.</li>
</ol>
<h4><a href="#additional-pattern-aws-serverless-land-example" aria-hidden="true" class="anchor" id="additional-pattern-aws-serverless-land-example"></a>Additional Pattern: AWS Serverless Land Example</h4>
<p>This approach aligns with a pattern presented on <a href="https://serverlessland.com/patterns/sqs-lambda-ddb-sam-java">AWS Serverless Land</a>: <strong>Create a Lambda function that batch writes to DynamoDB from SQS</strong>. This pattern deploys an SQS queue, a Lambda Function, and a DynamoDB table, allowing batch writes from SQS messages to DynamoDB. It demonstrates how to leverage a batch processing mechanism to handle high-throughput scenarios effectively.</p>
<p>The provided SAM template uses Java 11, SQS, Lambda, and DynamoDB to create a cost-effective, serverless architecture:</p>
<pre><code class="language-yaml">AWSTemplateFormatVersion: 2010-09-09
Transform: AWS::Serverless-2016-10-31
Description: sqs-lambda-dynamodb

Globals:
  Function:
    Runtime: java11
    MemorySize: 512
    Timeout: 25

Resources:
  OrderConsumer:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: OrderConsumer
      Handler: com.example.OrderConsumer::handleRequest
      CodeUri: target/sourceCode.zip
      Environment:
        Variables:
          QUEUE_URL: !Sub 'https://sqs.${AWS::Region}.amazonaws.com/${AWS::AccountId}/OrdersQueue'
          REGION: !Sub '${AWS::Region}'
          TABLE_NAME: !Ref OrdersTable
      Policies:
        - AWSLambdaSQSQueueExecutionRole
        - AmazonDynamoDBFullAccess

  OrdersQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: OrdersQueue

  OrdersTable:
    Type: 'AWS::DynamoDB::Table'
    Properties:
      TableName: OrdersTable
      AttributeDefinitions:
        - AttributeName: orderId
          AttributeType: S
      KeySchema:
        - AttributeName: orderId
          KeyType: HASH
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5
</code></pre>
<h4><a href="#estimated-time-to-process-2" aria-hidden="true" class="anchor" id="estimated-time-to-process-2"></a>Estimated Time to Process</h4>
<p>Assume:</p>
<ul>
<li><code>T_batch</code>: Execution time for processing a batch.</li>
<li><code>k</code>: Overhead due to batching.</li>
<li><code>b</code>: Number of messages per batch.</li>
<li><code>n</code>: Lambda concurrency.</li>
</ul>
<p>The total processing time is:</p>
<pre><code class="language-html">Total Time = (Total Records / (b * n)) * (T + k)
</code></pre>
<p>Example with:</p>
<ul>
<li><code>T = 100 ms</code></li>
<li><code>k = 20 ms</code></li>
<li><code>b = 10</code></li>
<li><code>n = 100</code></li>
<li><code>Total Records = 1,000,000</code></li>
</ul>
<p>Substitute into the formula:</p>
<pre><code class="language-html">Total Time = (1,000,000 / (10 * 100)) * (100 ms + 20 ms)
Total Time = (1,000,000 / 1,000) * 120 ms
Total Time = 1,000 * 120 ms = 120,000 ms
</code></pre>
<p>Convert to seconds and minutes:</p>
<pre><code class="language-html">Total Time = 120,000 ms = 120 seconds = 2 minutes
</code></pre>
<h4><a href="#the-importance-of-fifo-queues" aria-hidden="true" class="anchor" id="the-importance-of-fifo-queues"></a>The Importance of FIFO Queues</h4>
<p>To maintain consistency in DynamoDB, it is essential to configure the SQS queue as FIFO (First-In, First-Out) in this case. This ensures that messages are processed in the exact order they are received, which is critical in systems where the order of operations affects the final state of the database. For example:</p>
<ol>
<li>
<p><strong>Out-of-Order Processing Issues:</strong> If two updates to the same DynamoDB record are processed out of order (e.g., Update1 followed by Update2), but Update2 depends on Update1, the database could end up in an inconsistent state. FIFO queues prevent this by enforcing strict order. For our case, there was not duplicated entries on the file so FIFO was not in considerated despite being absolutely important for this usecase.</p>
</li>
<li>
<p><strong>Idempotency Challenges:</strong> Even when Lambda functions are designed to be idempotent, out-of-order processing can lead to unexpected behavior if operations rely on sequential execution. For instance, appending logs or incrementing counters requires a guarantee of order.</p>
</li>
<li>
<p><strong>Trade-offs with FIFO:</strong> While FIFO queues provide consistency, they come with some limitations:</p>
<ul>
<li><strong>Lower Throughput:</strong> FIFO queues have a maximum throughput of 300 transactions per second with batching (or 3,000 if using high-throughput mode).</li>
<li><strong>Increased Latency:</strong> Enforcing order may introduce slight delays in message processing.</li>
</ul>
</li>
</ol>
<hr />
<h4><a href="#pros-1" aria-hidden="true" class="anchor" id="pros-1"></a>Pros:</h4>
<ul>
<li><strong>Decouples producers and consumers:</strong> The producer can continue adding messages to the queue regardless of the Lambda processing speed.</li>
<li><strong>Prevents throttling:</strong> SQS regulates the rate at which messages are delivered to Lambda, avoiding sudden spikes that could exceed AWS limits.</li>
<li><strong>Distributed systems friendly:</strong> Works seamlessly in multi-node systems, as all nodes write to the same queue without requiring coordination.</li>
</ul>
<h4><a href="#cons-1" aria-hidden="true" class="anchor" id="cons-1"></a>Cons:</h4>
<ul>
<li><strong>Adds architectural complexity:</strong> Introducing SQS requires additional components and configuration.</li>
<li><strong>Adds code complexity:</strong> Introduce code complexity to the insertion lambda, so its responsible for managing sqs batch write operations, reading on SQS source and also being able to operate by asynchronous invocation for legacy systems.</li>
<li><strong>Introduces latency:</strong> Messages may wait in the queue before being processed, depending on the Lambda polling rate and queue depth. For example, a queue depth of 10,000 messages and a polling rate of 1,000 messages per second would result in a processing delay.</li>
</ul>
<hr />
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>AWS Lambda throttling issues, particularly for high-concurrency workloads, can be effectively managed using a combination of strategies such as concurrency control, retry mechanisms, and decoupling with SQS. Each of these approaches has its strengths and trade-offs:</p>
<ul>
<li>
<p><strong>Limit Concurrency Using Semaphore</strong>: A straightforward solution for single-node setups, providing reliable throttling control at the cost of slightly increased processing time. However, it requires additional considerations for distributed systems.</p>
</li>
<li>
<p><strong>Retry with Exponential Backoff</strong>: A robust technique for handling transient failures, distributing load over time and avoiding unnecessary retries. Yet, it can add significant latency in worst-case scenarios and increase implementation complexity.</p>
</li>
<li>
<p><strong>Use SQS for Decoupling</strong>: The most scalable and efficient approach when <code>T_batch = T + k</code>, with <code>k</code> being sufficiently small. While it introduces latency and complexity, its benefits make it the go-to solution for large-scale systems.</p>
</li>
</ul>
<p>As an ending insight, we can assure that for small workloads, async invocation can provide faster results, as it avoids the latency of queuing and batch processing. However, as the number of requests increases, direct invocation becomes inefficient and computationally expensive due to the high TPS demand and risk of breaching AWS limits. In contrast, decoupled architectures using SQS and batch processing scale more efficiently, ensuring stability and cost-effectiveness under heavy loads.</p>
<h3><a href="#next-steps-implementing-a-poc" aria-hidden="true" class="anchor" id="next-steps-implementing-a-poc"></a>Next Steps: Implementing a POC</h3>
<p>While this post has focused on explaining the challenges, strategies, and theoretical calculations for mitigation, an actual Proof of Concept (POC) would be very cool to validate and visualize these solutions in practice. A future post might explore how to design and execute a POC to measure the overall performance in a real-world scenario.</p>
<p>For more details on Lambda throttling, refer to the <a href="https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html">AWS Lambda Developer Guide</a> and the <a href="https://aws.amazon.com/blogs/compute/understanding-aws-lambdas-invoke-throttle-limits/">AWS Compute Blog</a>.</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Choosing a Garbage Collector for Your Java/Kotlin Application: Things I Wish I Knew Back Then</title><link>/garbage-collector.html</link><author>pedrohbl_</author><category>java</category><category>garbage-collector</category><category>kotlin</category><category>jvm</category><guid>/garbage-collector.html</guid><pubDate>Sun, 12 Jan 2025 00:00:00 GMT</pubDate><source url="">archive-2025</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#introduction" aria-hidden="true" class="anchor" id="introduction"></a>Introduction</h2>
<p>When I first started building Java and Kotlin applications, I didn’t really pay much attention to garbage collection. It was this magical process that &quot;just worked.&quot; But as I moved into more complex systems—batch processing, high-throughput APIs, and distributed architectures—I realized that choosing the right garbage collector could make or break my application’s performance, and also prevent some later production incidents.</p>
<p>Some of my early APIs even experienced breakdowns due to memory leaks, leading to unresponsive systems under heavy load. These episodes taught me the critical importance of understanding how GC works and how to configure it for specific workloads. Failing to consider GC for high-throughput APIs, for example, can lead to severe latency spikes, memory fragmentation, or outright crashes.</p>
<p>This article is a guide for those who, like me, wish they had a clearer understanding of JVM garbage collectors earlier. I will try to cover:</p>
<ol>
<li>How garbage collection works in the JVM.</li>
<li>The different types of GCs available.</li>
<li>Real-world use cases and configs for each GC.</li>
<li>Choosing the right garbage collector (references for informed decision-making).</li>
<li>Conclusion &amp; Exercises ;-).</li>
</ol>
<p>Let’s dive in and make garbage collection work <em>for</em> you, not against you.</p>
<hr />
<h2><a href="#how-garbage-collection-works-in-the-jvm" aria-hidden="true" class="anchor" id="how-garbage-collection-works-in-the-jvm"></a>How Garbage Collection Works in the JVM</h2>
<p>Garbage collection in the JVM is all about managing heap memory(imagine it's the playground where all your objects live). When objects are no longer referenced, they become eligible for garbage collection, freeing up memory for new allocations. But the process isn’t always seamless—GC pauses and overhead can significantly impact performance.</p>
<h3><a href="#key-concepts" aria-hidden="true" class="anchor" id="key-concepts"></a>Key Concepts</h3>
<h4><a href="#heap-memory" aria-hidden="true" class="anchor" id="heap-memory"></a>Heap Memory</h4>
<ol>
<li>
<p><strong>Eden Space (in the Young Generation):</strong></p>
<ul>
<li><strong>Purpose:</strong> This is where new objects are first allocated.</li>
<li><strong>Garbage Collection Behavior:</strong> Objects in Eden are short-lived and quickly collected during a minor GC cycle if they are no longer in use.</li>
<li><strong>Example:</strong> Suppose you’re creating multiple instances of a <code>Minion</code> class. And those minions are from <em>League of Legends</em> or <em>Despicable Me</em>—your choice:
<pre><code class="language-java">for (int i = 0; i &lt; 1000; i++) {
    Minion minion = new Minion(&quot;Minion &quot; + i);
}
</code></pre>
All these minions will initially be created in the Eden space. If they are not referenced anymore after their creation, they will be collected during the next minor GC.</li>
</ul>
</li>
<li>
<p><strong>Survivor Spaces (in the Young Generation):</strong></p>
<ul>
<li><strong>Purpose:</strong> Objects that survive one or more minor GC cycles in Eden are moved to Survivor spaces.</li>
<li><strong>Garbage Collection Behavior:</strong> Survivor spaces act as a staging area before objects are promoted to the Old Generation.</li>
<li><strong>Example:</strong> In a game application, temporary data like dead minions or player movement logs might survive for a short time in Survivor spaces before being discarded or promoted if reused frequently.</li>
</ul>
</li>
<li>
<p><strong>Old Generation:</strong></p>
<ul>
<li><strong>Purpose:</strong> Objects that have a long lifespan or survive multiple minor GC cycles are moved to the Old Generation.</li>
<li><strong>Garbage Collection Behavior:</strong> Garbage collection here is less frequent but more time-consuming.</li>
<li><strong>Example:</strong> Imagine you’re building a game where each <code>Player</code> represents a connected user on the match. These objects are long-lived compared to temporary data like minions or projectiles and may look like this:
<pre><code class="language-java">public class Player {
    private final String name;
    private final Inventory inventory;

    public Player(String name) {
        this.name = name;
        this.inventory = new Inventory();
    }
}
</code></pre>
A <code>Player</code> object, which holds data such as the player’s inventory and stats, will likely reside in the Old Generation as it persists for the entire application session.</li>
</ul>
</li>
<li>
<p><strong>Metaspace:</strong></p>
<ul>
<li><strong>Purpose:</strong> Think of Metaspace as the library(outside the heap) of your application—it keeps the blueprints (class metadata) for all the objects your application creates.</li>
<li><strong>Garbage Collection Behavior:</strong> Metaspace grows dynamically as new class loaders are introduced and is cleaned up when those class loaders are no longer needed. This ensures that unused blueprints don’t mess up your libraries.</li>
<li><strong>Example:</strong> Imagine you’re running a game that supports mods, and players can load new heroes dynamically. Each mod represents a new class dynamically loaded at runtime:
<pre><code class="language-java">Class&lt;?&gt; heroClass = Class.forName(&quot;com.game.dynamic.Hero&quot;);
Object hero = heroClass.getDeclaredConstructor().newInstance();
</code></pre>
The blueprint for the <code>Hero</code> class will be stored in Metaspace. When the mod is unloaded or the player exits the game, the class loader is no longer needed, and the JVM will clean up the associated Metaspace memory. This ensures that your application remains efficient, even with dynamic features.</li>
</ul>
</li>
</ol>
<h4><a href="#garbage-collector-phases" aria-hidden="true" class="anchor" id="garbage-collector-phases"></a>Garbage Collector Phases</h4>
<ol>
<li>
<p><strong>Mark:</strong></p>
<ul>
<li><strong>Purpose:</strong> Identify live objects by traversing references starting from the root set (e.g., static fields, local variables).</li>
<li><strong>Practical Example:</strong> Consider this code:
<pre><code class="language-java">Player player = new Player(&quot;Hero&quot;);
player.hitMinion();
</code></pre>
The <code>player</code> object is reachable because it’s referenced in the method. During the Mark phase, the GC identifies <code>player</code> and its dependencies as live objects.</li>
</ul>
</li>
<li>
<p><strong>Sweep:</strong></p>
<ul>
<li><strong>Purpose:</strong> Reclaim memory occupied by objects not marked as live.</li>
<li><strong>Practical Example:</strong> If the <code>player</code> reference is set to <code>null</code>:
<pre><code class="language-java">player = null;
</code></pre>
The next GC cycle’s Sweep phase will reclaim the memory occupied by the <code>player</code> object and its associated data.</li>
</ul>
</li>
<li>
<p><strong>Compact:</strong></p>
<ul>
<li><strong>Purpose:</strong> Reduce fragmentation by moving objects closer together in memory.</li>
<li><strong>Practical Example:</strong> After reclaiming memory, gaps may exist in the heap. Compacting ensures efficient allocation for future objects:
<pre><code class="language-java">// Before compaction: [Minion 1][   ][Minion 3][   ]
// After compaction:  [Minion 1][Minion 3][       ]
</code></pre>
This step is particularly important in systems with frequent allocations and deallocations(Related to CPU efficiency).</li>
</ul>
</li>
</ol>
<p>For a deep understanding, the JVM GC documentation provides wider insights (<a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/">source</a>).</p>
<hr />
<h2><a href="#types-of-jvm-garbage-collectors" aria-hidden="true" class="anchor" id="types-of-jvm-garbage-collectors"></a>Types of JVM Garbage Collectors</h2>
<h3><a href="#1-serial-garbage-collector-serial-gc" aria-hidden="true" class="anchor" id="1-serial-garbage-collector-serial-gc"></a>1. Serial Garbage Collector (Serial GC)</h3>
<h4><a href="#overview" aria-hidden="true" class="anchor" id="overview"></a>Overview:</h4>
<p>The Serial GC is single-threaded and optimized for simplicity. It processes the Young and Old Generations one at a time, pausing application threads during GC.</p>
<h4><a href="#when-to-use" aria-hidden="true" class="anchor" id="when-to-use"></a>When to Use:</h4>
<ul>
<li>VERY SMALL applications with SINGLE-THREAD workloads.</li>
<li>Low-memory environments (e.g., embedded systems).</li>
</ul>
<h4><a href="#limitations" aria-hidden="true" class="anchor" id="limitations"></a>Limitations:</h4>
<ul>
<li>
<p>Not suitable for high-concurrency, high-throughput systems.</p>
</li>
<li>
<p>Maximum throughput is low due to its single-threaded nature.</p>
</li>
</ul>
<h4><a href="#example" aria-hidden="true" class="anchor" id="example"></a>Example:</h4>
<p>Consider a system managing API calls for IoT devices that periodically send sensor data (e.g., room temperature). Each device sends minimal data in a predictable pattern, and the system handles only one request per thread. The Serial GC ensures predictable, low-overhead memory management, making it an ideal choice for such an environment.</p>
<h4><a href="#docker-example" aria-hidden="true" class="anchor" id="docker-example"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseSerialGC -Xmx512m -jar app.jar
</code></pre>
<hr />
<h3><a href="#2-parallel-garbage-collector-parallel-gc" aria-hidden="true" class="anchor" id="2-parallel-garbage-collector-parallel-gc"></a>2. Parallel Garbage Collector (Parallel GC)</h3>
<h4><a href="#overview-1" aria-hidden="true" class="anchor" id="overview-1"></a>Overview:</h4>
<p>Parallel GC, also known as the Throughput Collector, uses multiple threads to speed up garbage collection. It aims to maximize application throughput by minimizing the total GC time. You can check some crazy a** graphs and get better explanation at the official documentation <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/parallel.html#gen_arrangement_parallel">here</a>.</p>
<h4><a href="#when-to-use-1" aria-hidden="true" class="anchor" id="when-to-use-1"></a>When to Use:</h4>
<ul>
<li>Batch processing systems.</li>
<li>Applications prioritizing throughput over low latency.</li>
</ul>
<h4><a href="#example-1" aria-hidden="true" class="anchor" id="example-1"></a>Example:</h4>
<p>Imagine a financial service API that consolidates transactions into daily reports. Since the workload prioritizes throughput over latency, Parallel GC is ideal for processing large transaction sets efficiently.</p>
<h4><a href="#docker-example-1" aria-hidden="true" class="anchor" id="docker-example-1"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseParallelGC -Xmx2g -jar app.jar
</code></pre>
<hr />
<h3><a href="#3-g1-garbage-collector-g1gc" aria-hidden="true" class="anchor" id="3-g1-garbage-collector-g1gc"></a>3. G1 Garbage Collector (G1GC)</h3>
<h4><a href="#overview-2" aria-hidden="true" class="anchor" id="overview-2"></a>Overview:</h4>
<p>G1GC divides the heap into regions and collects garbage incrementally, making it a good balance between throughput and low latency.</p>
<h4><a href="#when-to-use-2" aria-hidden="true" class="anchor" id="when-to-use-2"></a>When to Use:</h4>
<ul>
<li>General-purpose applications.</li>
<li>Systems requiring predictable pause times.</li>
</ul>
<h4><a href="#example-2" aria-hidden="true" class="anchor" id="example-2"></a>Example:</h4>
<p>Any SaaS platform serving user requests in under 200ms with moderate traffic spikes.</p>
<h4><a href="#docker-example-2" aria-hidden="true" class="anchor" id="docker-example-2"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseG1GC -Xmx4g -XX:MaxGCPauseMillis=200 -jar app.jar
</code></pre>
<h4><a href="#important-considerations-about-g1gc" aria-hidden="true" class="anchor" id="important-considerations-about-g1gc"></a>Important considerations about G1GC:</h4>
<p>You might be wondering: &quot;If G1GC supports both good throughput and low latency, why not use it for every application? Sounds like a no-brainer...&quot;</p>
<p>But well, not quite. While G1GC is a fantastic general-purpose garbage collector, it’s not the universal solution for all workloads. Think of it as the &quot;jack of all trades&quot; of GCs—good at many things, but not necessarily the best at any one thing. <em>Poof!</em> Now that you’re out of the cave, let’s analyze:</p>
<ul>
<li>
<p><strong>Throughput-Focused Applications:</strong> If your application doesn’t care about pause times—for example, batch processing systems or data aggregation pipelines—why would you burden it with G1GC’s incremental collection overhead? Parallel GC is better suited here, offering raw performance without worrying about predictable pauses.</p>
</li>
<li>
<p><strong>Ultra-Low Latency Needs:</strong> If you’re building a real-time trading system or managing huge heaps (think terabytes), G1GC might struggle to meet your strict latency requirements. Collectors like ZGC or Shenandoah GC are designed specifically for these use cases, offering sub-10ms pause times.</p>
</li>
</ul>
<p>In short, G1GC is like that versatile tool in your toolbox—it works well for a variety of tasks, especially if you’re building the classic CRUD API (yes pretty much all of your messy simple Spring CRUDs). But if you’re running specialized workloads, you’ll want to pick a collector that’s optimized to your needs.</p>
<hr />
<h3><a href="#4-z-garbage-collector-zgc" aria-hidden="true" class="anchor" id="4-z-garbage-collector-zgc"></a>4. Z Garbage Collector (ZGC)</h3>
<h4><a href="#overview-3" aria-hidden="true" class="anchor" id="overview-3"></a>Overview:</h4>
<p>ZGC is designed for ultra-low-latency applications with large heaps (up to terabytes). Its pause times are typically under 10 milliseconds.</p>
<h4><a href="#when-to-use-3" aria-hidden="true" class="anchor" id="when-to-use-3"></a>When to Use:</h4>
<ul>
<li>Real-time systems.</li>
<li>Applications with very large heaps.</li>
</ul>
<h4><a href="#when-to-do-not-use" aria-hidden="true" class="anchor" id="when-to-do-not-use"></a>When to DO NOT use:</h4>
<ul>
<li>Imagine you have a batch processing system using ZGC. There is very high chance of facing inceased CPU utilization($$$) without any latency benefit. For example, a data ingestion pipeline optimized for high throughput but insensitive to pause times would waste resources managing unnecessary low-latency GC cycles.</li>
</ul>
<h4><a href="#example-3" aria-hidden="true" class="anchor" id="example-3"></a>Example:</h4>
<p>A trading system processing market data streams in real time.</p>
<h4><a href="#docker-example-3" aria-hidden="true" class="anchor" id="docker-example-3"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseZGC -Xmx16g -jar app.jar
</code></pre>
<hr />
<h3><a href="#5-shenandoah-garbage-collector" aria-hidden="true" class="anchor" id="5-shenandoah-garbage-collector"></a>5. Shenandoah Garbage Collector</h3>
<h4><a href="#overview-4" aria-hidden="true" class="anchor" id="overview-4"></a>Overview:</h4>
<p>Shenandoah GC minimizes pause times by performing concurrent compaction. It’s ideal for latency-sensitive applications.</p>
<h4><a href="#when-to-use-4" aria-hidden="true" class="anchor" id="when-to-use-4"></a>When to Use:</h4>
<ul>
<li><strong>Payment gateways</strong> with strict SLA requirements for latency.</li>
<li><strong>APIs with spiky traffic</strong> patterns, such as social media feeds or live voting systems.</li>
<li>Applications where reducing GC pause time is critical to user experience, such as gaming servers or interactive web applications.</li>
</ul>
<h4><a href="#when-to-do-not-use-1" aria-hidden="true" class="anchor" id="when-to-do-not-use-1"></a>When to DO NOT use:</h4>
<p>Using Shenandoah GC for batch processing systems or workloads optimized for high throughput over low latency (e.g., nightly data aggregation) may lead to inefficient CPU utilization. The additional overhead of concurrent compaction provides no benefits when predictable pauses are acceptable, reducing overall throughput compared to <strong>Parallel GC</strong>.</p>
<p>For exampe, a financial reconciliation batch process configured with Shenandoah might experience reduced throughput due to unnecessary focus on low pause times, delaying report generation.</p>
<h4><a href="#example-4" aria-hidden="true" class="anchor" id="example-4"></a>Example:</h4>
<p>A payment processing API handling high transaction volumes cannot afford GC-induced latency spikes during peak hours. Shenandoah’s low-pause nature ensures that transaction processing continues smoothly even under heavy load.</p>
<p>Another example is a real-time multiplayer gaming server, where latency spikes could lead to a poor player experience. Shenandoah ensures consistent frame updates and server responsiveness.</p>
<h4><a href="#docker-example-4" aria-hidden="true" class="anchor" id="docker-example-4"></a>Docker Example:</h4>
<pre><code class="language-dockerfile">FROM openjdk:17-jdk-slim
CMD java -XX:+UseShenandoahGC -Xmx8g -XX:+UnlockExperimentalVMOptions -jar app.jar
</code></pre>
<hr />
<h2><a href="#choosing-the-right-garbage-collector" aria-hidden="true" class="anchor" id="choosing-the-right-garbage-collector"></a>Choosing the Right Garbage Collector</h2>
<p>Here you can find a cheatsheet. But remember... you should always evaluate your own workload before choosing it's garbage collector.</p>
<table>
<thead>
<tr>
<th>Garbage Collector</th>
<th>Best For</th>
<th>JVM Version Support</th>
</tr>
</thead>
<tbody>
<tr>
<td>Serial GC</td>
<td>Small, single-threaded apps</td>
<td>All versions</td>
</tr>
<tr>
<td>Parallel GC</td>
<td>High-throughput batch systems</td>
<td>All versions</td>
</tr>
<tr>
<td>G1GC</td>
<td>General-purpose apps</td>
<td>Java 9+</td>
</tr>
<tr>
<td>ZGC</td>
<td>Real-time, large heap apps</td>
<td>Java 11+</td>
</tr>
<tr>
<td>Shenandoah GC</td>
<td>Low-latency apps</td>
<td>Java 11+</td>
</tr>
</tbody>
</table>
<hr />
<h2><a href="#conclusion" aria-hidden="true" class="anchor" id="conclusion"></a>Conclusion</h2>
<p>Choosing the right garbage collector for your application requires some knowledge over the tools I discussed in this post. But once you learn about it, you may have the power of taking decisions, and this is extremely valuable in Software Engineering field, also, by selecting the right GC you can significantly improve performance, stability and save some costs for your future applications based on JVM. Don’t let GC be a black box—embrace it, tune it, and let it work for you.</p>
<hr />
<h2><a href="#training-real-world-scenarios-and-solutions" aria-hidden="true" class="anchor" id="training-real-world-scenarios-and-solutions"></a>Training: Real-World Scenarios and Solutions</h2>
<h3><a href="#scenario-1-payment-gateway-latency" aria-hidden="true" class="anchor" id="scenario-1-payment-gateway-latency"></a>Scenario 1: Payment Gateway Latency</h3>
<p>You are building a payment gateway API that must process transactions in real-time with strict SLA requirements. The workload is spiky, with heavy traffic during sales events or specific times of the day. Which garbage collector would you choose to ensure low latency?</p>
<h3><a href="#scenario-2-batch-data-processing-system" aria-hidden="true" class="anchor" id="scenario-2-batch-data-processing-system"></a>Scenario 2: Batch Data Processing System</h3>
<p>Your application processes daily financial reconciliation batches, which involve large amounts of data. Latency is not a concern, but throughput must be maximized to complete processing as fast as possible. Which garbage collector fits this use case?</p>
<h3><a href="#scenario-3-real-time-multiplayer-game" aria-hidden="true" class="anchor" id="scenario-3-real-time-multiplayer-game"></a>Scenario 3: Real-Time Multiplayer Game</h3>
<p>You are designing a server for a real-time multiplayer game. The server must manage thousands of players, each generating events continuously. Latency spikes during garbage collection are unacceptable as they could lead to lag and a poor user experience. What GC configuration would you use?</p>
<hr />
<h2><a href="#solutions" aria-hidden="true" class="anchor" id="solutions"></a>Solutions</h2>
<h3><a href="#solution-1-payment-gateway-latency" aria-hidden="true" class="anchor" id="solution-1-payment-gateway-latency"></a>Solution 1: Payment Gateway Latency</h3>
<p>Use <strong>Shenandoah GC</strong> to ensure low latency and consistent response times. Its concurrent compaction minimizes pause times, making it ideal for latency-sensitive workloads.</p>
<h3><a href="#solution-2-batch-data-processing-system" aria-hidden="true" class="anchor" id="solution-2-batch-data-processing-system"></a>Solution 2: Batch Data Processing System</h3>
<p>Use <strong>Parallel GC</strong> to maximize throughput. Since latency isn’t a concern, the Parallel GC’s focus on high efficiency during garbage collection fits this workload.</p>
<h3><a href="#solution-3-real-time-multiplayer-game" aria-hidden="true" class="anchor" id="solution-3-real-time-multiplayer-game"></a>Solution 3: Real-Time Multiplayer Game</h3>
<p>Use <strong>ZGC</strong> to achieve ultra-low latency and scale with large heaps. It ensures that garbage collection does not interfere with real-time gameplay.</p>
<h3><a href="#references" aria-hidden="true" class="anchor" id="references"></a>References:</h3>
<ol>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/">Java Garbage Collection Basics - Oracle</a></li>
</ol>
<hr />
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item></channel></rss>